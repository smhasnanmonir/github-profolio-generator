
# Fix Windows console encoding for emojis
import sys
import io
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

import os, glob, warnings, pickle
import numpy as np
import pandas as pd

# Set matplotlib to non-interactive backend (no popups, just save figures)
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

# Optional: seaborn for nicer charts
import seaborn as sns
sns.set_theme(style="whitegrid")

warnings.filterwarnings("ignore")

# === LOCAL CONFIGURATION ===
# All outputs will be saved in these directories
SAVE_DIR = "training_outputs"
MODEL_DIR = "organized_structure/models"
os.makedirs(SAVE_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

print("="*80)
print("üöÄ ML MODEL TRAINING - LOCAL VERSION")
print("="*80)
print(f"Output Directory: {SAVE_DIR}")
print(f"Model Directory: {MODEL_DIR}")
print("="*80 + "\n")

def _find_latest(pattern: str, base: str = SAVE_DIR) -> str:
    files = sorted(glob.glob(os.path.join(base, pattern)))
    if not files:
        raise FileNotFoundError(f"No files matched {pattern} in {base}. "
                                "Run your Dataset.py export first.")
    return files[-1]

# === DATA LOADING CONFIGURATION ===
# Point to your local CSV files
users_csv = "github_users_20251023_064928.csv"  # User-level data
repos_csv = "github_repos_20251023_064928.csv"  # Repository-level data

print("üìÇ Loading data from local CSV files...")
print(f"   Users: {users_csv}")
print(f"   Repos: {repos_csv}\n")

if 'df_users' not in globals() or 'df_repo' not in globals():
    print("Loading:")
    print(" -", users_csv)
    print(" -", repos_csv)
    df_users = pd.read_csv(users_csv, low_memory=False)
    df_repo  = pd.read_csv(repos_csv, low_memory=False)
else:
    print("Using df_users and df_repo already in memory.")

# Light safety: ensure expected numeric cols exist (fill missing with 0 to avoid NaNs / div-by-zero)
expected_user_numeric = [
    'total_commit_contributions','total_pr_contributions','total_issue_contributions',
    'total_pr_review_contributions','total_repositories','account_age_days',
    'contributions_this_year','avg_files_per_commit','avg_additions_per_commit',
    'avg_deletions_per_commit','total_recent_additions','total_recent_deletions',
    'unique_repos_committed','active_repos','total_gist_comments','following_count',
    'followers_count','organizations_count','forked_repositories_count',
    'pinned_items_count','avg_days_since_last_push','total_languages',
    'sponsoring_count','sponsors_count','total_started_repos','total_forked_repos'
]
for c in expected_user_numeric:
    if c not in df_users.columns:
        df_users[c] = 0

# Repo safety: required aggregator columns
for c in ['owner_login','primary_language','languages_total_count','stargazer_count','fork_count','watchers_count','is_private','is_fork']:
    if c not in df_repo.columns:
        df_repo[c] = np.nan if c in ['owner_login','primary_language'] else 0

"""Feature Engineering"""

# ====== Feature Engineering (robust, Colab-ready) ======
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_theme(style="whitegrid")

# ---------- helpers ----------
def _minmax01(s):
    s = pd.to_numeric(s, errors='coerce').fillna(0.0)
    lo, hi = s.min(), s.max()
    return (s - lo) / (hi - lo) if hi > lo else s*0 + 0.5

def add_multitasking_variants(users_features, df_repo):

    EPS = 1e-9
    # figure out join key
    key = 'login' if 'login' in users_features.columns else (
        'username' if 'username' in users_features.columns else None
    )
    if key is None:
        raise ValueError("‚ùå ERROR: No valid owner column found in repository data! Cannot compute multitasking metrics.")

    # pick a repo identifier that actually exists
    if 'name' in df_repo.columns:
        repo_id_col = 'name'
    elif 'name_with_owner' in df_repo.columns:
        repo_id_col = 'name_with_owner'
    elif 'repo_id' in df_repo.columns:
        repo_id_col = 'repo_id'
    else:
        raise ValueError("‚ùå ERROR: No valid repository identifier column (name/name_with_owner/repo_id) found! Cannot compute multitasking metrics.")

    # total repos per owner (portfolio size)
    repo_counts = (df_repo.groupby('owner_login')[repo_id_col]
                   .nunique()
                   .rename('repo_count_all')
                   .to_frame())

    users_features = users_features.merge(
        repo_counts, left_on=key, right_index=True, how='left'
    )
    if 'repo_count_all' not in users_features.columns:
        users_features['repo_count_all'] = 0
    users_features['repo_count_all'] = users_features['repo_count_all'].fillna(0)

    # 1) coverage of active set
    users_features['recent_active_repo_coverage'] = (
        users_features['unique_repos_committed'] / (users_features['active_repos'] + 1)
    )

    # 2) breadth-only (normalized 0..1)
    users_features['multitasking_breadth'] = _minmax01(users_features['unique_repos_committed'])

    # 3) share of total portfolio touched
    users_features['multitasking_share_of_portfolio'] = (
        users_features['unique_repos_committed'] / (users_features['repo_count_all'] + 1)
    )

    # 4) blended index (0..1)
    users_features['multitasking_index'] = (
        0.5*users_features['multitasking_breadth'] + 0.5*users_features['recent_active_repo_coverage']
    )
    return users_features

# ---------- feature builders ----------
def create_developer_activity_features(df_users, df_repo):
    print("Creating Developer Activity Features...")
    users_features = df_users.copy()

    must = ['total_commit_contributions','total_pr_contributions','total_issue_contributions',
            'total_pr_review_contributions','total_repositories','account_age_days',
            'contributions_this_year','avg_files_per_commit','avg_additions_per_commit',
            'avg_deletions_per_commit','total_recent_additions','total_recent_deletions',
            'unique_repos_committed','active_repos','total_contributions']
    for c in must:
        if c not in users_features.columns: users_features[c] = 0

    # 1) Activity intensity
    activity_cols = ['total_commit_contributions','total_pr_contributions',
                     'total_issue_contributions','total_pr_review_contributions']
    users_features['activity_intensity_score'] = users_features[activity_cols].sum(axis=1)

    # 2) Consistency (inverse of CV)
    users_features['contribution_consistency'] = (
        users_features[activity_cols].std(axis=1) /
        (users_features[activity_cols].mean(axis=1) + 1)
    )
    users_features['contribution_consistency'] = 1 / (1 + users_features['contribution_consistency'])

    # 3) Repo creation rate
    users_features['repo_creation_rate'] = users_features['total_repositories'] / (users_features['account_age_days'] / 365 + 1)
    #3a) Commit frequency (commits per account-year)
    users_features['commit_freq_per_year'] = (
    users_features['total_commit_contributions'] /
    (users_features['account_age_days'] / 365.0 + 1.0))

    # 4) Recent activity ratio
    users_features['recent_activity_ratio'] = users_features['contributions_this_year'] / (users_features['total_contributions'] + 1)

    # 5) Change rate (single, robust signal)
    users_features['code_change_rate'] = (
        users_features['avg_additions_per_commit'] + users_features['avg_deletions_per_commit']
    ) / 2.0

    # 6) Development velocity
    users_features['development_velocity'] = (
        users_features['total_recent_additions'] + users_features['total_recent_deletions']
    ) / (users_features['unique_repos_committed'] + 1)

    # 7) Multitasking (blended)
    users_features = add_multitasking_variants(users_features, df_repo)
    users_features['multitasking_score'] = users_features['multitasking_index']
    return users_features



def create_technical_skills_features(df_users, df_repo):
    """
    Technical skills from the data you actually have:
      - specialization & diversity from distribution of primary_language across a user's repos
      - breadth from df_users['total_languages'] (already computed in your users table)
      - repo_language_diversity = #distinct primary_language across repos
      - avg_languages_per_repo from repo languages_total_count
      - avg_repo_size (optional extra) from repo languages_total_size
    """
    print("Creating Technical Skills Features (repo-driven)...")
    users_features = df_users.copy()

    # Ensure the breadth column exists even if missing in df_users
    if 'total_languages' not in users_features.columns:
        users_features['total_languages'] = 0
    users_features['tech_stack_breadth'] = users_features['total_languages']

    # ---------- Build language distribution per user from repos ----------
    # We will compute per-user counts of primary_language, then turn them into proportions.
    has_primary = {'owner_login', 'primary_language'}.issubset(df_repo.columns)
    if has_primary and df_repo['primary_language'].notna().any():
        # Count repos per (user, primary_language)
        lang_counts = (
            df_repo.dropna(subset=['primary_language'])
                  .groupby(['owner_login', 'primary_language'])
                  .size()
                  .unstack(fill_value=0)
        )

        # Totals per user (only repos that have a primary_language)
        totals = lang_counts.sum(axis=1)
        # Proportions p_ij = count(lang j for user i) / total repos for user i
        p = lang_counts.div(totals.replace(0, np.nan), axis=0)

        # Specialization: dominance of the top language = max_j p_ij  (0..1)
        spec = p.max(axis=1).fillna(0).rename('language_specialization')

        # Diversity: Simpson's index = 1 - sum_j p_ij^2  (0..1, higher = more diverse)
        simpson = (1 - (p.pow(2)).sum(axis=1)).fillna(0).rename('language_balance')

        # Repo-level language diversity = number of distinct primary_language values
        repo_lang_div = (lang_counts.gt(0).sum(axis=1)
                         .astype(float)
                         .rename('repo_language_diversity'))

        # Merge these back onto users by login
        users_features = users_features.merge(
            pd.concat([spec, simpson, repo_lang_div], axis=1),
            left_on='login', right_index=True, how='left'
        )
    else:
        # If we can't compute language distribution, create zeros.
        users_features['language_specialization'] = 0.0
        users_features['language_balance'] = 0.0
        users_features['repo_language_diversity'] = 0.0

    # Fill NA from the merge above
    for c in ['language_specialization', 'language_balance', 'repo_language_diversity']:
        if c not in users_features.columns:
            users_features[c] = 0.0
        else:
            users_features[c] = users_features[c].fillna(0.0)

    # ---------- Avg #languages per repo (from languages_total_count) ----------
    if {'owner_login', 'languages_total_count'}.issubset(df_repo.columns) and df_repo['languages_total_count'].notna().any():
        avg_langs = (df_repo.groupby('owner_login')['languages_total_count']
                           .mean()
                           .to_frame('avg_languages_per_repo'))
        users_features = users_features.merge(
            avg_langs, left_on='login', right_index=True, how='left'
        )
    if 'avg_languages_per_repo' not in users_features.columns:
        users_features['avg_languages_per_repo'] = 0.0
    else:
        users_features['avg_languages_per_repo'] = users_features['avg_languages_per_repo'].fillna(0.0)

    # ---------- Optional: average repo "size" (if you want a coarse experience proxy) ----------
    if {'owner_login', 'languages_total_size'}.issubset(df_repo.columns) and df_repo['languages_total_size'].notna().any():
        avg_size = (df_repo.groupby('owner_login')['languages_total_size']
                           .mean()
                           .to_frame('avg_repo_size'))
        users_features = users_features.merge(
            avg_size, left_on='login', right_index=True, how='left'
        )
        users_features['avg_repo_size'] = users_features['avg_repo_size'].fillna(0.0)
    else:
        users_features['avg_repo_size'] = 0.0



    return users_features



def create_collaboration_features(df_users, df_repo):
    print("Creating Collaboration Features...")
    users_features = df_users.copy()

    for c in ['total_issue_comments','total_pr_review_contributions','total_discussion_comments','total_commit_comments',
              'total_pr_contributions','total_issue_contributions','total_commit_contributions',
              'forked_repositories_count','following_count','followers_count','organizations_count',
              'account_age_days','total_gist_comments','total_repositories']:
        if c not in users_features.columns: users_features[c] = 0

    # 1) Community engagement
    engagement_cols = ['total_issue_comments','total_pr_review_contributions','total_discussion_comments','total_commit_comments']
    users_features['community_engagement_score'] = users_features[engagement_cols].sum(axis=1)

    # 2) Collaboration ratio
    users_features['collaboration_ratio'] = (users_features['total_pr_contributions'] + users_features['total_issue_contributions']) / (users_features['total_commit_contributions'] + 1)

    # 3) Fork contribution rate
    users_features['fork_contribution_rate'] = users_features['forked_repositories_count'] / (users_features['total_repositories'] + 1)

    # 4) Mentorship
    users_features['mentorship_score'] = (
        users_features['total_pr_review_contributions'] + users_features['total_gist_comments'] * 0.5
    ) / (users_features['account_age_days'] / 365 + 1)

    # 5) Stars/forks/watchers per owner
    repo_contrib = df_repo.groupby('owner_login').agg({
        'is_fork':'sum','stargazer_count':'sum','fork_count':'sum','watchers_count':'sum'
    }).rename(columns={'is_fork':'total_forked_repos_owned',
                       'stargazer_count':'total_stars_received',
                       'fork_count':'total_forks_received',
                       'watchers_count':'total_watchers'})
    users_features = users_features.merge(repo_contrib, left_on='login', right_index=True, how='left').fillna(0)

    # 6) Network influence
    users_features['network_influence'] = np.log1p(
        users_features['followers_count'] * 2 + users_features['total_stars_received'] * 0.5 + users_features['total_forks_received'] * 1.5
    )

    # ---------- NEW: Social coding index (normalized engagement per account-year) ----------
    def _robust01(s, q=0.90):
        s = pd.to_numeric(s, errors='coerce').fillna(0.0)
        hi = s.quantile(q)
        if hi <= 0:
            return s*0.0
        s = s.clip(lower=0, upper=hi)
        return s / (hi + 1e-9)

    acct_years = (users_features['account_age_days'] / 365.0).replace([np.inf,-np.inf], 0).fillna(0)
    social_raw = (
        users_features['total_issue_comments']
      + users_features['total_pr_review_contributions']
      + users_features['total_discussion_comments']
      + users_features['total_commit_comments']
      + 0.5*(users_features['total_pr_contributions'] + users_features['total_issue_contributions'])
    ) / (acct_years + 1.0)

    users_features['social_coding_index'] = _robust01(np.log1p(social_raw))

    return users_features



def create_project_quality_features(df_users, df_repo):
    print("Creating Project Quality Features (patched)...")
    users_features = df_users.copy()

    # Ensure user-level cols exist
    for c in ['active_repos','total_repositories','pinned_items_count','avg_days_since_last_push','total_languages']:
        if c not in users_features.columns: users_features[c] = 0

    users_features['repo_active_score']   = users_features['active_repos'] / (users_features['total_repositories'] + 1)
    users_features['showcase_score'] = users_features['pinned_items_count'] / 6.0
    users_features['maintenance_rate']    = users_features['avg_days_since_last_push']
    users_features['maintenance_score']   = 1.0 / (1.0 + users_features['maintenance_rate'] / 30.0)

    # Build repo aggregates only for columns that exist
    pieces = []

    if {'owner_login','stargazer_count'}.issubset(df_repo.columns):
        stars = (df_repo.groupby('owner_login')['stargazer_count']
                 .agg(avg_stars_per_repo='mean', max_stars_repo='max', stars_std_dev='std'))
        pieces.append(stars)

    if {'owner_login','fork_count'}.issubset(df_repo.columns):
        forks = (df_repo.groupby('owner_login')['fork_count']
                 .agg(avg_forks_per_repo='mean', max_forks_repo='max'))
        pieces.append(forks)

    if {'owner_login','watchers_count'}.issubset(df_repo.columns):
        watchers = (df_repo.groupby('owner_login')['watchers_count']
                    .mean().to_frame('avg_watchers_per_repo'))
        pieces.append(watchers)

    # languages_total_count often missing for users with no public repos
    if {'owner_login','languages_total_count'}.issubset(df_repo.columns) and df_repo['languages_total_count'].notna().any():
        langs = (df_repo.groupby('owner_login')['languages_total_count']
                 .mean().to_frame('avg_languages_per_repo'))
        pieces.append(langs)

    if {'owner_login','is_private'}.issubset(df_repo.columns):
        public_ratio = (df_repo.groupby('owner_login')['is_private']
                        .apply(lambda x: float((x == False).mean()))
                        .to_frame('public_repo_ratio'))
        pieces.append(public_ratio)

    if pieces:
        repo_quality_stats = pieces[0]
        for p in pieces[1:]:
            repo_quality_stats = repo_quality_stats.join(p, how='outer')
        repo_quality_stats = repo_quality_stats.reset_index()
    else:
        repo_quality_stats = pd.DataFrame(columns=[
            'owner_login','avg_stars_per_repo','max_stars_repo','stars_std_dev',
            'avg_forks_per_repo','max_forks_repo',
            'avg_watchers_per_repo','avg_languages_per_repo','public_repo_ratio'
        ])

    users_features = users_features.merge(
        repo_quality_stats, left_on='login', right_on='owner_login', how='left'
    )

    # Ensure all expected cols exist & are filled
    needed = ['avg_stars_per_repo','max_stars_repo','stars_std_dev',
              'avg_forks_per_repo','max_forks_repo',
              'avg_watchers_per_repo','avg_languages_per_repo','public_repo_ratio']
    for c in needed:
        if c not in users_features.columns:
            users_features[c] = 0.0
    users_features[needed] = users_features[needed].fillna(0.0)

    users_features['code_change_rate'] = (
        users_features['avg_additions_per_commit'] + users_features['avg_deletions_per_commit']) / 2.0

    # Robust scaling helper (cap at 90th percentile)
    def _robust01(s, q=0.90):
        s = pd.to_numeric(s, errors='coerce').fillna(0.0)
        hi = s.quantile(q)
        if hi <= 0:
            return s*0.0
        s = s.clip(lower=0, upper=hi)
        return s / (hi + 1e-9)

    lang_richness = _robust01(users_features['avg_languages_per_repo'])       # multi-language repos
    files_touched = _robust01(users_features['avg_files_per_commit'])         # breadth per commit
    churn_level  = _robust01(users_features['code_change_rate'])              # code change size
    active_ratio = users_features['repo_active_score'].clip(0, 1)             # portfolio activeness

    users_features['project_complexity'] = (
        0.30 * lang_richness +
        0.25 * files_touched +
        0.25 * churn_level  +
        0.20 * active_ratio
    ).astype(float)  # stays in [0,1]

    # ---- Code review metrics ----
    users_features['code_review_participation'] = (
        users_features['total_pr_review_contributions'] /
        (users_features['total_pr_contributions'] + 1.0)
    )

    acct_years = (users_features['account_age_days'] / 365.0).replace([np.inf, -np.inf], 0).fillna(0)
    review_intensity = users_features['total_pr_review_contributions'] / (acct_years + 1.0)  # reviews per year
    review_intensity_n = _robust01(review_intensity)
    review_ratio = users_features['code_review_participation'].fillna(0).clip(0, 1)

    users_features['code_review_index'] = (0.5 * review_intensity_n + 0.5 * review_ratio).astype(float)

    return users_features


def create_developer_influence_features(df_users):
    """
    Influence & leadership features with robust, defensible scaling.

    Adds:
      - reputation_score (unchanged from your idea)
      - impact_factor, influence_growth_rate, viral_repo_score (unchanged)
      - leadership_score in [0,1], plus component columns:
          lead_org, lead_mentorship, lead_review, lead_support
    """
    print("Creating Developer Influence Features (robust)...")
    users_features = df_users.copy()

    # Ensure required columns exist
    for c in [
        'total_stars_received','total_forks_received','sponsoring_count','sponsors_count',
        'followers_count','organizations_count','mentorship_score','avg_stars_per_repo',
        'max_stars_repo','total_repositories','account_age_days','total_pr_review_contributions'
    ]:
        if c not in users_features.columns:
            users_features[c] = 0.0

    # ---- Reputation & impact (as you had) ----
    users_features['reputation_score'] = (
        np.log1p(users_features['followers_count'])        * 0.3 +
        np.log1p(users_features['total_stars_received'])   * 0.3 +
        np.log1p(users_features['total_forks_received'])   * 0.2 +
        np.log1p(users_features['sponsoring_count'])       * 0.1 +
        np.log1p(users_features['sponsors_count'])         * 0.1
    )

    users_features['impact_factor'] = (
        users_features['total_stars_received'] + 2.0 * users_features['total_forks_received']
    ) / (users_features['total_repositories'] + 1.0)

    users_features['influence_growth_rate'] = (
        users_features['followers_count'] / (users_features['account_age_days'] / 365.0 + 1.0)
    )

    users_features['viral_repo_score'] = (
        users_features['max_stars_repo'] / (users_features['avg_stars_per_repo'] + 1.0)
    )

    # ---- Leadership score (robust, defensible) ----
    # Helper: robust 0..1 scaling capped at 90th percentile to reduce outlier dominance
    def _robust01(s, q=0.90):
        s = pd.to_numeric(s, errors='coerce').fillna(0.0)
        hi = s.quantile(q)
        if hi <= 0:
            return s*0.0
        s = s.clip(lower=0, upper=hi)
        return s / (hi + 1e-9)

    # Years on platform
    acct_years = (users_features['account_age_days'] / 365.0).replace([np.inf, -np.inf], 0).fillna(0)

    # Components:
    # 1) Organizational leadership: membership/roles in orgs (cap at 3 orgs for interpretability)
    lead_org = (users_features['organizations_count'].clip(0, 3) / 3.0).astype(float)

    # 2) Mentorship leadership: use the mentorship_score directly
    #    NO FALLBACK - must have real data
    if 'mentorship_score' not in users_features.columns:
        raise ValueError("‚ùå ERROR: mentorship_score not found in dataset! Cannot compute leadership_score.")
    mentorship_raw = users_features['mentorship_score']
    lead_mentorship = _robust01(mentorship_raw)

    # 3) Review leadership: PR review intensity per year (distinct from mentorship if that includes gist comments, etc.)
    review_intensity = users_features['total_pr_review_contributions'] / (acct_years + 1.0)
    lead_review = _robust01(review_intensity)

    # 4) Community support: sponsors + sponsoring (log dampened to avoid runaway)
    support_raw = np.log1p(users_features['sponsoring_count'] + users_features['sponsors_count'])
    lead_support = _robust01(support_raw)

    # Save components for transparency (optional but handy for EDA)
    users_features['lead_org']        = lead_org
    users_features['lead_mentorship'] = lead_mentorship
    users_features['lead_review']     = lead_review
    users_features['lead_support']    = lead_support

    # Weighted blend (sums to 1.0). Rationale:
    # - Org & Mentorship (0.35 each): primary signals of leading people/projects.
    # - Reviews (0.20): leadership in quality/process.
    # - Support (0.10): external recognition; useful but not core to leadership.
    users_features['leadership_score'] = (
        0.35 * lead_org +
        0.35 * lead_mentorship +
        0.20 * lead_review +
        0.10 * lead_support
    ).astype(float)

    # ---- Simple profile completeness (keep) ----
    avail = [c for c in ['email','location','avatar_url','profile_url'] if c in users_features.columns]
    if avail:
        users_features['profile_completeness'] = sum(
            (~users_features[c].isna()).astype(int) for c in avail
        ) / float(len(avail))
    else:
        users_features['profile_completeness'] = 0.0

    return users_features



def create_behavioral_patterns(df_users):
    """
    Behavioral pattern features (robust & normalized).

    Outputs (all in [0,1], unless noted):
      - maintainer_score        : share of started vs (started + forked)
      - team_player_score       : blend of orgs, collaboration intensity, fork rate
      - generalist_score        : normalized total_languages
      - work_consistency        : 1 / (1 + stars_std_dev / (avg_stars_per_repo + eps))
      - learning_velocity       : generalist_score * recent_activity_ratio (clipped)
      - innovation_index        : sqrt(normalized started repos * normalized avg stars)

    Notes:
      * Uses robust 0..1 scaling capped at cohort 90th percentile to reduce outlier dominance.
      * Includes fallbacks if collaboration_ratio / fork_contribution_rate / recent_activity_ratio
        weren't computed earlier in the pipeline.
    """
    print("Creating Behavioral Pattern Features (normalized & robust)...")
    users_features = df_users.copy()

    # ---------- ensure required columns exist (init with 0.0) ----------
    base_needed = [
        'total_started_repos','total_forked_repos','organizations_count',
        'total_languages','stars_std_dev','avg_stars_per_repo'
    ]
    for c in base_needed:
        if c not in users_features.columns:
            users_features[c] = 0.0

    # Check that required columns exist - NO FALLBACKS
    required_cols = [
        'total_pr_contributions','total_issue_contributions','total_commit_contributions',
        'forked_repositories_count','total_repositories',
        'total_contributions','contributions_this_year'
    ]
    missing_cols = [c for c in required_cols if c not in users_features.columns]
    if missing_cols:
        raise ValueError(f"‚ùå ERROR: Required columns missing from dataset: {missing_cols}. Cannot compute composite features without real data.")

    # ---------- helpers ----------
    def _robust01(s, q=0.90):
        """Scale to [0,1] with cap at q-quantile to reduce outlier dominance."""
        s = pd.to_numeric(s, errors='coerce').fillna(0.0)
        hi = s.quantile(q)
        if hi <= 0:
            return s * 0.0
        s = s.clip(lower=0, upper=hi)
        return s / (hi + 1e-9)

    eps = 1e-9

    # ---------- Compute ratios if not present (but only using real data) ----------
    # collaboration_ratio = (PRs + Issues) / (Commits + 1)
    if 'collaboration_ratio' not in users_features.columns:
        # This is OK - we're computing from real data, not using a fallback
        users_features['collaboration_ratio'] = (
            (users_features['total_pr_contributions'] + users_features['total_issue_contributions']) /
            (users_features['total_commit_contributions'] + 1.0)
        )

    # fork_contribution_rate = forked_repositories_count / (total_repositories + 1)
    if 'fork_contribution_rate' not in users_features.columns:
        users_features['fork_contribution_rate'] = (
            users_features['forked_repositories_count'] / (users_features['total_repositories'] + 1.0)
        )

    # recent_activity_ratio = contributions_this_year / (total_contributions + 1)
    if 'recent_activity_ratio' not in users_features.columns:
        users_features['recent_activity_ratio'] = (
            users_features['contributions_this_year'] / (users_features['total_contributions'] + 1.0)
        )

    # ---------- 1) Maintainer vs Contributor ----------
    denom = users_features['total_started_repos'] + users_features['total_forked_repos']
    users_features['maintainer_score'] = (
        users_features['total_started_repos'] / (denom + 1.0)
    ).astype(float)  # [0,1]

    # ---------- 2) Team player score (normalized blend) ----------
    # orgs: cap at 3 orgs => [0,1]
    org_norm    = (users_features['organizations_count'].clip(0, 3) / 3.0).astype(float)
    # collab_ratio: robust 0..1
    collab_norm = _robust01(users_features['collaboration_ratio'])
    # fork contribution rate: already a ratio
    fork_rate   = users_features['fork_contribution_rate'].fillna(0.0).clip(0, 1)

    users_features['team_player_score'] = (
        0.35 * org_norm + 0.45 * collab_norm + 0.20 * fork_rate
    ).astype(float)  # [0,1]

    # ---------- 3) Specialist vs Generalist ----------
    users_features['generalist_score'] = _robust01(users_features['total_languages'])  # [0,1]

    # ---------- 4) Work consistency (bounded, higher = steadier stars) ----------
    var_ratio = users_features['stars_std_dev'] / (users_features['avg_stars_per_repo'] + eps)
    var_ratio = var_ratio.replace([np.inf, -np.inf], np.nan).fillna(0.0).clip(lower=0)
    users_features['work_consistency'] = (1.0 / (1.0 + var_ratio)).astype(float)  # [0,1]

    # ---------- 5) Learning velocity (breadth x recentness) ----------
    recent = users_features['recent_activity_ratio'].fillna(0.0).clip(0, 1)
    users_features['learning_velocity'] = (
        users_features['generalist_score'] * recent
    ).astype(float)  # [0,1]

    # ---------- 6) Innovation index (originality x reception) ----------
    started_n  = _robust01(users_features['total_started_repos'])
    avgstars_n = _robust01(users_features['avg_stars_per_repo'])
    users_features['innovation_index'] = np.sqrt(started_n * avgstars_n).astype(float)  # [0,1]

    return users_features




def create_final_feature_set(df_users, df_repo):
    print("\n" + "="*80)
    print("Starting Feature Engineering Process...")
    print("="*80 + "\n")

    # Make sure we have a 'login' key to join on
    if 'login' not in df_users.columns and 'username' in df_users.columns:
        df_users = df_users.copy()
        df_users['login'] = df_users['username']

    # Build features in sequence (each step augments the frame)
    features = create_developer_activity_features(df_users, df_repo)
    features = create_technical_skills_features(features, df_repo)
    features = create_collaboration_features(features, df_repo)
    features = create_project_quality_features(features, df_repo)
    features = create_developer_influence_features(features)
    features = create_behavioral_patterns(features)

    # ---- No portfolio_score / No developer_category ----
    # (If you had any downstream code using these, remove or guard it.)

    print("\nFeature engineering complete!")
    print(f"Total features created: {features.shape[1] - df_users.shape[1]}")
    print(f"Final dataset shape: {features.shape}")
    return features


def get_feature_groups():
    return {
        'activity_features': [
            'activity_intensity_score','contribution_consistency','repo_creation_rate',
            'recent_activity_ratio','code_change_rate','development_velocity','multitasking_score','commit_freq_per_year'
        ],
        'technical_features': [
            'language_specialization','tech_stack_breadth','repo_language_diversity',
            'avg_languages_per_repo','project_complexity'
        ],
        'collaboration_features': [
            'community_engagement_score','collaboration_ratio','fork_contribution_rate',
            'social_coding_index','mentorship_score','network_influence'
        ],
        'quality_features': [
            'repo_active_score','showcase_score','maintenance_score',
            'avg_stars_per_repo','public_repo_ratio','code_review_participation'
        ],
        'influence_features': [
            'reputation_score','impact_factor','influence_growth_rate',
            'viral_repo_score','leadership_score'
        ],
        'behavioral_features': [
            'maintainer_score','team_player_score','generalist_score',
            'work_consistency','learning_velocity','innovation_index'
        ]
    }

# ====== SAVE ENGINEERED FEATURES (safe, self-contained) ======
import os, json
from datetime import datetime

# --- 0) Build final_features if it doesn't exist yet ---
if "final_features" not in globals():
    # You must have df_users and df_repo available OR load them here.
    # If they aren't in memory, UNCOMMENT and point to your data files:
    # import pandas as pd
    # Example: Load from custom paths if needed
    # df_users = pd.read_csv("path/to/your/users.csv")
    # df_repo  = pd.read_csv("path/to/your/repos.csv")
    assert "df_users" in globals() and "df_repo" in globals(), \
        "df_users/df_repo not found. Load them or compute before saving."
    final_features = create_final_feature_set(df_users, df_repo)

# --- 1) Feature groups (and prune to columns that actually exist) ---
feature_groups = get_feature_groups() if "get_feature_groups" in globals() else {}
feature_groups = {
    g: [f for f in feats if f in final_features.columns]
    for g, feats in feature_groups.items()
}

# --- 2) Derive original vs engineered (fallbacks if df_users missing) ---
if "df_users" in globals():
    original_cols   = [c for c in final_features.columns if c in df_users.columns]
    engineered_cols = [c for c in final_features.columns if c not in df_users.columns]
else:
    original_cols   = []
    # If df_users isn't available, treat all as engineered or use groups union:
    engineered_cols = sorted(set().union(*feature_groups.values())) or list(final_features.columns)

# --- 3) (Optional) shortlist for modeling (intersect for safety) ---
shortlist_keep = [
 'activity_intensity_score','contribution_consistency','repo_creation_rate','recent_activity_ratio',
 'development_velocity','multitasking_score','code_change_rate',
 'language_specialization','language_balance','repo_language_diversity','tech_stack_breadth',
 'avg_languages_per_repo','avg_repo_size',
 'community_engagement_score','collaboration_ratio','fork_contribution_rate','mentorship_score',
 'social_coding_index','network_influence',
 'repo_active_score','showcase_score','maintenance_score','avg_stars_per_repo','max_stars_repo',
 'avg_forks_per_repo','max_forks_repo','public_repo_ratio','project_complexity',
 'code_review_participation','code_review_index',
 'reputation_score','impact_factor','influence_growth_rate','viral_repo_score','leadership_score',
 'profile_completeness','maintainer_score','team_player_score','generalist_score','work_consistency',
 'learning_velocity','innovation_index'
]
shortlist_keep = [c for c in shortlist_keep if c in final_features.columns]
model_df = final_features.reindex(columns=shortlist_keep).fillna(0.0)

# --- 3.1) prepend an identifier column for downstream joins/pred mapping ---
id_values = None
for cand in ["login", "username", "user_id"]:
    if cand in final_features.columns:
        id_values = final_features[cand].values
        break
if id_values is None and "df_users" in globals():
    # fall back to df_users if the ID isn't in final_features (aligned by index)
    for cand in ["login", "username", "user_id"]:
        if cand in df_users.columns:
            id_values = df_users[cand].reindex(final_features.index).values
            break
if id_values is None:
    # last-resort stable index
    id_values = list(range(len(final_features)))

model_df = model_df.copy()
model_df.insert(0, "id", id_values)

# --- 4) Save files ---
save_dir = SAVE_DIR  # Use local training_outputs directory
os.makedirs(save_dir, exist_ok=True)
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

# Full dataset
output_path = os.path.join(save_dir, f'final_features_{timestamp}.csv')
final_features.to_csv(output_path, index=False)
print(f"‚úÖ Saved final_features to: {output_path}  | Shape: {final_features.shape}")

# Engineered-only (guard if empty)
engineered_path = os.path.join(save_dir, f'engineered_features_only_{timestamp}.csv')
(final_features[engineered_cols] if engineered_cols else final_features.iloc[:,0:0]).to_csv(engineered_path, index=False)
print(f"‚úÖ Saved engineered features only to: {engineered_path}  | Count: {len(engineered_cols)}")

# Shortlist (model-ready)
shortlist_path = os.path.join(save_dir, f'model_features_shortlist_{timestamp}.csv')
model_df.to_csv(shortlist_path, index=False)
print(f"‚úÖ Saved model shortlist to: {shortlist_path}  | Shape: {model_df.shape}")

# Feature groups metadata
metadata_path = os.path.join(save_dir, f'feature_groups_{timestamp}.json')
with open(metadata_path, 'w') as f:
    json.dump(feature_groups, f, indent=2)
print(f"‚úÖ Saved feature groups metadata to: {metadata_path}")

# Human-readable summary
summary_path = os.path.join(save_dir, f'feature_summary_{timestamp}.txt')
with open(summary_path, 'w') as f:
    f.write("Feature Engineering Summary\n" + "="*60 + "\n\n")
    f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"Total features: {final_features.shape[1]}\n")
    f.write(f"Original features: {len(original_cols)}\n")
    f.write(f"Engineered features: {len(engineered_cols)}\n")
    f.write(f"Total samples: {final_features.shape[0]}\n\n")
    f.write("Feature Groups (present-only):\n" + "-"*60 + "\n")
    for group_name, feats in feature_groups.items():
        f.write(f"\n{group_name} ({len(feats)} features):\n")
        for feat in feats:
            f.write(f"  - {feat}\n")
    f.write("\n\nAll Engineered Features:\n" + "-"*60 + "\n")
    for i, feat in enumerate(sorted(engineered_cols), 1):
        f.write(f"{i:3d}. {feat}\n")

print("\n" + "="*60)
print(f"All files saved successfully to: {save_dir}")
print("="*60)

"""Developer Ranking"""



# pip install xgboost==2.1.1 scikit-learn==1.5.2 (install via requirements-training.txt)

import os, glob, json, joblib, numpy as np, pandas as pd
from datetime import datetime
from collections import Counter
from pathlib import Path

# GPU Detection and Configuration
print("="*80)
print("üéÆ GPU DETECTION")
print("="*80)

try:
    import torch
    if torch.cuda.is_available():
        print(f"‚úÖ PyTorch GPU Available: {torch.cuda.get_device_name(0)}")
        print(f"   CUDA Version: {torch.version.cuda}")
        print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        USE_GPU = True
        DEVICE = 'cuda'
    else:
        print("‚ö†Ô∏è  PyTorch installed but no CUDA GPU detected")
        USE_GPU = False
        DEVICE = 'cpu'
except ImportError:
    print("‚ÑπÔ∏è  PyTorch not installed (optional for TensorFlow)")
    USE_GPU = False
    DEVICE = 'cpu'

# Check XGBoost GPU support
try:
    import xgboost as xgb
    print(f"\n‚úÖ XGBoost Version: {xgb.__version__}")
    # XGBoost GPU is enabled via tree_method='gpu_hist'
    print("   GPU support will be enabled via tree_method='gpu_hist'")
except ImportError:
    print("‚ùå XGBoost not installed")

print("="*80 + "\n")

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import (
    f1_score, accuracy_score, classification_report, confusion_matrix,
    mean_squared_error, r2_score
)
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBRegressor, XGBClassifier

SEED = 42
np.random.seed(SEED)

# Use the same SAVE_DIR as defined earlier
# SAVE_DIR = "training_outputs" (already defined at line 15)

# Look for feature shortlist CSV in current directory or training_outputs
cands = sorted(glob.glob("model_features_shortlist_*.csv"))
if not cands:
    cands = sorted(glob.glob(os.path.join(SAVE_DIR, "model_features_shortlist_*.csv")))

if not cands:
    print("‚ö†Ô∏è  Warning: model_features_shortlist_*.csv not found")
    print("   This will be generated from df (user features dataframe)")
    print("   Make sure df variable is populated from previous cells")
    # Create df from the loaded user features
    if 'df' not in globals():
        print("   Loading from df_users...")
        df = df_users.copy()
else:
    SHORTLIST_PATH = cands[-1]
    df = pd.read_csv(SHORTLIST_PATH)
    
assert "id" in df.columns, "Shortlist must include an 'id' column"

if 'SHORTLIST_PATH' in locals():
    print(f"üìä Loaded data from: {SHORTLIST_PATH}")
else:
    print("üìä Using df from memory (generated from df_users)")
    
print(f"   Shape: {df.shape}")
print(f"   Columns: {list(df.columns[:10])}... (showing first 10)")

"""Helper Functions"""

def robust01(s, q=0.90):
    """Normalize series to 0-1 range with outlier clipping"""
    s = pd.to_numeric(s, errors="coerce").fillna(0.0)
    hi = s.quantile(q)
    if hi <= 0:
        return s * 0.0
    s = s.clip(0, hi)
    return s / (hi + 1e-9)

def ndcg_at_k(y_true, y_pred, k=20):
    """Normalized Discounted Cumulative Gain at K"""
    idx = np.argsort(-y_pred)[:k]
    gains = np.array(y_true)[idx]
    dcg = np.sum((2**gains - 1) / np.log2(np.arange(2, k+2)))
    ideal = np.sort(y_true)[::-1][:k]
    idcg = np.sum((2**ideal - 1) / np.log2(np.arange(2, k+2)))
    return float(dcg / (idcg + 1e-9))

print("‚úÖ Helper functions defined!")

"""Ranking Target Variable - UPDATED FOR DIRECT PRIORITY METRICS"""

print("\nüéØ Creating NEW rank_target with direct priority metrics:")
print("   Stars > Forks > Watchers > Commits > Recency")

# The shortlist doesn't have raw columns, so load from final_features
print("   Loading full features from final_features to get raw metrics...")

# Load user-level metrics (commits, recency) from final_features
# Note: Stars/forks/watchers now come from repository-level data below
final_features_files = sorted(glob.glob(os.path.join(SAVE_DIR, "final_features_*.csv")))
if final_features_files:
    final_features_path = final_features_files[-1]
    print(f"   Loading user-level metrics from: {final_features_path}")

    df_full = pd.read_csv(final_features_path)
    
    # Extract only commits and recency (stars/forks come from repo data)
    # Note: final_features uses 'login' as key, shortlist uses 'id'
    user_metrics_cols = ['login', 'total_commit_contributions', 'recent_activity_ratio']
    available_metrics = [c for c in user_metrics_cols if c in df_full.columns]
    
    # Merge user-level metrics into our working dataframe
    if len(available_metrics) > 1:  # At least 'login' + one metric
        df = df.merge(df_full[available_metrics], left_on='id', right_on='login', how='left', suffixes=('', '_full'))
        print(f"   ‚úÖ Merged {len(available_metrics)-1} user-level metric columns")
        # Drop the redundant 'login' column after merge
        if 'login' in df.columns:
            df = df.drop(columns=['login'])
    else:
        print("   ‚ö†Ô∏è Could not find user-level metrics in final_features!")
else:
    raise FileNotFoundError("‚ùå ERROR: final_features not found! Cannot get user-level commits/recency data.")

# Load repository-level data for per-repo ranking metrics
# NO FALLBACKS - Must have real repository data with all required columns
print("\n   Loading repository-level metrics for ranking...")
repo_csv = "github_repos_20251023_064928.csv"
if not os.path.exists(repo_csv):
    raise FileNotFoundError(f"‚ùå ERROR: Repository CSV not found: {repo_csv}! Cannot train ranking model without repository data.")

df_repos = pd.read_csv(repo_csv)
print(f"   ‚úÖ Loaded {len(df_repos)} repositories")

# Check for required columns - NO FALLBACKS
required_repo_cols = ['stargazer_count', 'fork_count', 'watchers_count', 'languages_total_size', 'languages_total_count']
missing_repo_cols = [col for col in required_repo_cols if col not in df_repos.columns]
if missing_repo_cols:
    raise ValueError(f"‚ùå ERROR: Required repository columns missing: {missing_repo_cols}! Cannot train ranking model without real repository data.")

# Map owner_login to id for merging with user data
if 'owner_login' in df_repos.columns:
    df_repos['id'] = df_repos['owner_login']
elif 'owner' in df_repos.columns:
    df_repos['id'] = df_repos['owner']
else:
    raise ValueError("‚ùå ERROR: No owner_login/owner column in repository data! Cannot link repos to users.")

# Aggregate repository metrics per user (sum of all their repos)
# Bigger values = better
repo_agg = df_repos.groupby('id').agg({
    'stargazer_count': 'sum',           # Total stars across all repos
    'fork_count': 'sum',                # Total forks across all repos
    'watchers_count': 'sum',            # Total watchers across all repos
    'languages_total_size': 'sum',      # Total language code size (bigger = better)
    'languages_total_count': 'sum',     # Total language diversity (bigger = better)
}).reset_index()

print(f"   ‚úÖ Aggregated metrics for {len(repo_agg)} users")
print(f"   Metrics: stargazer_count, fork_count, watchers_count, languages_total_size, languages_total_count")

# Merge repository metrics into user dataframe
df = df.merge(repo_agg, on='id', how='left', suffixes=('', '_repo'))

# Check if merge was successful
if df['stargazer_count'].isna().all():
    raise ValueError("‚ùå ERROR: Failed to merge repository data with user data! Check that 'id' columns match.")

print(f"   ‚úÖ Merged repository metrics into user dataframe")

# Map to standard names for ranking calculation - using REPOSITORY-LEVEL DATA
# NO FALLBACKS - All data must be present
df['repo_stars'] = df['stargazer_count']
df['repo_forks'] = df['fork_count']
df['repo_watchers'] = df['watchers_count']
df['repo_lang_size'] = df['languages_total_size']
df['repo_lang_count'] = df['languages_total_count']

# Also get user-level commits and recency for additional context
if 'total_commit_contributions' in df.columns:
    print("   ‚úÖ Using total_commit_contributions for commits metric")
    df['total_commits'] = df['total_commit_contributions']
elif 'total_commits' not in df.columns:
    raise ValueError("‚ùå ERROR: total_commits/total_commit_contributions not found in dataset! Cannot train ranking model without real commit data.")

if 'recent_activity_ratio' in df.columns:
    print("   ‚úÖ Using recent_activity_ratio for recency score")
    df['recency_score'] = df['recent_activity_ratio']
elif 'recency_score' not in df.columns:
    raise ValueError("‚ùå ERROR: recent_activity_ratio/recency_score not found in dataset! Cannot train ranking model without real recency data.")

# Normalize each component to 0-1 range using robust scaling
# Bigger values = better for all these metrics
print("\n   Normalizing components (bigger = better)...")
normalized_stars = robust01(df['repo_stars'])
normalized_forks = robust01(df['repo_forks'])
normalized_watchers = robust01(df['repo_watchers'])
normalized_lang_size = robust01(df['repo_lang_size'])
normalized_lang_count = robust01(df['repo_lang_count'])
normalized_commits = robust01(df['total_commits'])
normalized_recency = robust01(df['recency_score'])

# Define NEW weights based on repository-level metrics
NEW_RANK_WEIGHTS = {
    'stars': 0.30,          # 30% - Repository popularity (stargazer_count)
    'forks': 0.20,          # 20% - Code reuse (fork_count)
    'watchers': 0.10,       # 10% - Active interest (watchers_count)
    'lang_size': 0.15,      # 15% - Code volume (languages_total_size)
    'lang_count': 0.10,     # 10% - Language diversity (languages_total_count)
    'commits': 0.10,        # 10% - Development effort
    'recency': 0.05,        # 5% - Recent activity bonus
}

print(f"   Weights: {NEW_RANK_WEIGHTS}")

# Calculate NEW rank_target with repository-level priorities
df["rank_target"] = (
    NEW_RANK_WEIGHTS['stars'] * normalized_stars +
    NEW_RANK_WEIGHTS['forks'] * normalized_forks +
    NEW_RANK_WEIGHTS['watchers'] * normalized_watchers +
    NEW_RANK_WEIGHTS['lang_size'] * normalized_lang_size +
    NEW_RANK_WEIGHTS['lang_count'] * normalized_lang_count +
    NEW_RANK_WEIGHTS['commits'] * normalized_commits +
    NEW_RANK_WEIGHTS['recency'] * normalized_recency
)

# Store component names for reference
rank_components = ['repo_stars', 'repo_forks', 'repo_watchers', 'repo_lang_size', 'repo_lang_count', 'total_commits', 'recency_score']

print("‚úÖ Ranking target created!")
print(f"   Components used: {len(rank_components)}")
print(f"   Target stats - Mean: {df['rank_target'].mean():.4f}, Std: {df['rank_target'].std():.4f}")

""" Prepare Features and Split Data"""

# Build X, y for ranking (drop id and the columns used to build target)
drop_rank_cols = ["id", "rank_target"] + rank_components
X_rank = df.drop(columns=[c for c in drop_rank_cols if c in df.columns]).copy()
y_rank = df["rank_target"].values
id_series = df["id"].copy()

# Split into train, validation, and test sets (60/20/20)
# First split: 80% train+val, 20% test
Xr_trainval, Xr_test, yr_trainval, yr_test, id_trainval, id_test = train_test_split(
    X_rank, y_rank, id_series, test_size=0.2, random_state=SEED
)

# Second split: 75% train (60% of total), 25% val (20% of total)
Xr_train, Xr_val, yr_train, yr_val, id_train, id_val = train_test_split(
    Xr_trainval, yr_trainval, id_trainval, test_size=0.25, random_state=SEED
)

num_features = list(X_rank.columns)

print("‚úÖ Data split completed (Train/Val/Test)!")
print(f"   Training set:   {Xr_train.shape} ({len(Xr_train)/len(X_rank)*100:.1f}%)")
print(f"   Validation set: {Xr_val.shape} ({len(Xr_val)/len(X_rank)*100:.1f}%)")
print(f"   Test set:       {Xr_test.shape} ({len(Xr_test)/len(X_rank)*100:.1f}%)")
print(f"   Total features: {len(num_features)}")

"""M1:XGBoost Regressor Pipeline"""

# Create preprocessing pipeline
prep_rank = ColumnTransformer([
    ("num", Pipeline([
        ("impute", SimpleImputer(strategy="median")),
        ("scale", StandardScaler(with_mean=False))
    ]), num_features)
])

# Create XGBoost Regressor
xgb_ranker = XGBRegressor(
    objective="reg:squarederror",
    tree_method="gpu_hist" if USE_GPU else "hist",  # GPU acceleration if available
    n_estimators=600,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.9,
    colsample_bytree=0.9,
    random_state=SEED,
    device="cuda" if USE_GPU else "cpu"  # Explicit device setting
)

# Create complete pipeline
pipe_rank_xgb = Pipeline([("prep", prep_rank), ("model", xgb_ranker)])

print("‚úÖ XGBoost pipeline created!")
print("   Parameters:")
print(f"   - n_estimators: {xgb_ranker.n_estimators}")
print(f"   - max_depth: {xgb_ranker.max_depth}")
print(f"   - learning_rate: {xgb_ranker.learning_rate}")

# Train the XGBoost pipeline
print("üöÄ Training XGBoost Ranker with validation monitoring...")

# For XGBoost 3.x with sklearn pipeline, we'll use a simpler approach
# Manually preprocess the data and train the model with early stopping

# Step 1: Fit and transform training data
Xr_train_transformed = pipe_rank_xgb.named_steps['prep'].fit_transform(Xr_train)

# Step 2: Transform validation data (without fitting)
Xr_val_transformed = pipe_rank_xgb.named_steps['prep'].transform(Xr_val)

# Step 3: Train XGBoost model directly with eval_set
xgb_model = pipe_rank_xgb.named_steps['model']
xgb_model.fit(
    Xr_train_transformed, yr_train,
    eval_set=[(Xr_val_transformed, yr_val)],
    verbose=False
)

print(f"‚úÖ XGBoost training completed!")
print(f"   Trained with validation monitoring")

# Note: The model is already part of the pipeline, so the pipeline is now trained

"""Evaluation"""

from scipy.stats import spearmanr

# Make predictions on validation and test sets
pred_xgb_val = pipe_rank_xgb.predict(Xr_val)
pred_xgb_test = pipe_rank_xgb.predict(Xr_test)

# Calculate metrics on VALIDATION set (for monitoring)
rmse_xgb_val = mean_squared_error(yr_val, pred_xgb_val, squared=False)
r2_xgb_val = r2_score(yr_val, pred_xgb_val)

# Calculate metrics on TEST set (final evaluation)
rmse_xgb = mean_squared_error(yr_test, pred_xgb_test, squared=False)
r2_xgb = r2_score(yr_test, pred_xgb_test)
ndcg_xgb = ndcg_at_k(yr_test, pred_xgb_test, k=20)
rho_xgb, _ = spearmanr(yr_test, pred_xgb_test)

print(f"\nüìä Validation Performance:")
print(f"   RMSE: {rmse_xgb_val:.4f}")
print(f"   R¬≤:   {r2_xgb_val:.4f}")

print("\n" + "="*60)
print("üìä XGBoost Ranker Performance (TEST SET)")
print("="*60)
print(f"RMSE:        {rmse_xgb:.4f}")
print(f"R¬≤ Score:    {r2_xgb:.4f}")
print(f"NDCG@20:     {ndcg_xgb:.4f}")
print(f"Spearman œÅ:  {rho_xgb:.4f}")
print("="*60)
print("‚úÖ Test set remained unseen during training!")

"""M2: Neural Network (MLP)"""

# Prepare data for Neural Network (needs different scaling)
prep_only = Pipeline([
    ("impute", SimpleImputer(strategy="median")),
    ("scale", StandardScaler(with_mean=True))  # Use mean centering for NN
])

# Use training data for fitting, validation for monitoring, test for final eval
Xr_train_nn = prep_only.fit_transform(Xr_train)
Xr_val_nn = prep_only.transform(Xr_val)
Xr_test_nn = prep_only.transform(Xr_test)

print("‚úÖ Neural Network data prepared!")
print(f"   Training shape: {Xr_train_nn.shape}")
print(f"   Validation shape: {Xr_val_nn.shape}")
print(f"   Test shape: {Xr_test_nn.shape}")

try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    TENSORFLOW_AVAILABLE = True
    
    def build_rank_mlp(input_dim):
        """Build a Multi-Layer Perceptron for ranking"""
        keras.utils.set_random_seed(SEED)

        # Input layer
        inp = keras.Input(shape=(input_dim,))

        # Hidden layers with dropout for regularization
        x = layers.Dense(256, activation="relu")(inp)
        x = layers.Dropout(0.2)(x)
        x = layers.Dense(128, activation="relu")(x)
        x = layers.Dropout(0.2)(x)
        x = layers.Dense(64, activation="relu")(x)

        # Output layer (linear activation for regression)
        out = layers.Dense(1, activation="linear")(x)

        # Create and compile model
        model = keras.Model(inp, out)
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=1e-3),
            loss="mse",
            metrics=[keras.metrics.RootMeanSquaredError(name="rmse")]
        )

        return model

    # Build the model
    rank_mlp = build_rank_mlp(Xr_train_nn.shape[1])
    
    print("‚úÖ Neural Network architecture created!")
    print("\nüìã Model Summary:")
    rank_mlp.summary()
    
except ImportError:
    print("\n‚ö†Ô∏è  TensorFlow not installed - skipping Neural Network model")
    print("   XGBoost already achieved R¬≤ = 0.99+, so NN is optional!")
    print("   To install: pip install tensorflow")
    TENSORFLOW_AVAILABLE = False
    rank_mlp = None

if TENSORFLOW_AVAILABLE and rank_mlp is not None:
    # Setup early stopping callback
    es = keras.callbacks.EarlyStopping(
        patience=12,
        restore_best_weights=True,
        monitor="val_rmse",
        mode="min"
    )

    # Train the model
    print("üöÄ Training Neural Network Ranker...")
    hist = rank_mlp.fit(
        Xr_train_nn, yr_train,
        validation_data=(Xr_val_nn, yr_val),
        epochs=200,
        batch_size=256,
        callbacks=[es],
        verbose=1
    )

    print("‚úÖ Neural Network training completed!")

    # Make predictions on TEST set
    pred_nn = rank_mlp.predict(Xr_test_nn, verbose=0).ravel()

    # Calculate metrics on TEST set
    rmse_nn = mean_squared_error(yr_test, pred_nn, squared=False)
    r2_nn = r2_score(yr_test, pred_nn)
    ndcg_nn = ndcg_at_k(yr_test, pred_nn, k=20)
    rho_nn, _ = spearmanr(yr_test, pred_nn)

    print("="*60)
    print("üìä Neural Network Ranker Performance")
    print("="*60)
    print(f"RMSE:        {rmse_nn:.4f}")
    print(f"R¬≤ Score:    {r2_nn:.4f}")
    print(f"NDCG@20:     {ndcg_nn:.4f}")
    print(f"Spearman œÅ:  {rho_nn:.4f}")
    print("="*60)
else:
    print("\n‚è≠Ô∏è  Skipping Neural Network training (TensorFlow not available)")
    # Set dummy values for comparison section
    rmse_nn = rmse_xgb
    r2_nn = r2_xgb
    ndcg_nn = ndcg_xgb
    rho_nn = rho_xgb
    pred_nn = pred_xgb_test

"""Comparison"""

# Create comparison DataFrame
comparison_df = pd.DataFrame({
    'Model': ['XGBoost Regressor', 'Neural Network (MLP)'],
    'RMSE': [rmse_xgb, rmse_nn],
    'R¬≤ Score': [r2_xgb, r2_nn],
    'NDCG@20': [ndcg_xgb, ndcg_nn],
    'Spearman œÅ': [rho_xgb, rho_nn]
})

print("\n" + "="*80)
print("üèÜ MODEL COMPARISON - Developer Ranking")
print("="*80)
print(comparison_df.to_string(index=False))
print("="*80)

# Calculate differences
print("\nüìà Performance Differences:")
print(f"   RMSE Difference: {abs(rmse_xgb - rmse_nn):.4f} (Lower is better)")
print(f"   R¬≤ Difference: {abs(r2_xgb - r2_nn):.4f} (Higher is better)")
print(f"   NDCG@20 Difference: {abs(ndcg_xgb - ndcg_nn):.4f} (Higher is better)")
print(f"   Spearman Difference: {abs(rho_xgb - rho_nn):.4f} (Higher is better)")

# Determine winner
xgb_wins = 0
nn_wins = 0

if rmse_xgb < rmse_nn: xgb_wins += 1
else: nn_wins += 1

if r2_xgb > r2_nn: xgb_wins += 1
else: nn_wins += 1

if ndcg_xgb > ndcg_nn: xgb_wins += 1
else: nn_wins += 1

if rho_xgb > rho_nn: xgb_wins += 1
else: nn_wins += 1

print(f"\nüéØ Overall: XGBoost wins {xgb_wins}/4 metrics, Neural Network wins {nn_wins}/4 metrics")

"""Training History (Neural Network)"""

# Plot Neural Network training history
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Loss plot
axes[0].plot(hist.history['loss'], label='Training Loss', linewidth=2)
axes[0].plot(hist.history['val_loss'], label='Validation Loss', linewidth=2)
axes[0].set_xlabel('Epoch', fontsize=12)
axes[0].set_ylabel('Loss (MSE)', fontsize=12)
axes[0].set_title('Neural Network Training Loss', fontsize=14, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# RMSE plot
axes[1].plot(hist.history['rmse'], label='Training RMSE', linewidth=2)
axes[1].plot(hist.history['val_rmse'], label='Validation RMSE', linewidth=2)
axes[1].set_xlabel('Epoch', fontsize=12)
axes[1].set_ylabel('RMSE', fontsize=12)
axes[1].set_title('Neural Network Training RMSE', fontsize=14, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(f"{SAVE_DIR}/nn_training_history.png", dpi=300, bbox_inches='tight')
plt.close()  # Close figure to free memory

print("‚úÖ Training history plot saved!")

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# XGBoost Rank-Rank Plot
true_ranks = yr_test.argsort().argsort()
pred_ranks_xgb = pred_xgb_test.argsort().argsort()

axes[0].scatter(true_ranks, pred_ranks_xgb, alpha=0.5, s=20)
axes[0].plot([0, len(yr_test)], [0, len(yr_test)], 'r--', lw=2, label='Perfect Ranking')
axes[0].set_xlabel('True Rank', fontsize=12)
axes[0].set_ylabel('Predicted Rank', fontsize=12)
axes[0].set_title(f'XGBoost Regressor\nSpearman œÅ = {rho_xgb:.4f}', fontsize=14, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Neural Network Rank-Rank Plot
pred_ranks_nn = pred_nn.argsort().argsort()

axes[1].scatter(true_ranks, pred_ranks_nn, alpha=0.5, s=20, color='orange')
axes[1].plot([0, len(yr_test)], [0, len(yr_test)], 'r--', lw=2, label='Perfect Ranking')
axes[1].set_xlabel('True Rank', fontsize=12)
axes[1].set_ylabel('Predicted Rank', fontsize=12)
axes[1].set_title(f'Neural Network (MLP)\nSpearman œÅ = {rho_nn:.4f}', fontsize=14, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(f"{SAVE_DIR}/rank_rank_comparison.png", dpi=300, bbox_inches='tight')
plt.close()  # Close figure to free memory

# Show top 20 developers by true score vs. predicted score
top20_true = np.argsort(yr_test)[-20:]
top20_pred = np.argsort(pred_xgb_test)[-20:]
overlap = len(set(top20_true) & set(top20_pred))
print(f"Overlap in Top 20: {overlap}/20")

"""Prediction plot"""

# Create scatter plots comparing predictions vs actual values
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# XGBoost scatter plot
axes[0].scatter(yr_test, pred_xgb_test, alpha=0.5, s=30, edgecolors='k', linewidth=0.5)
axes[0].plot([yr_test.min(), yr_test.max()], [yr_test.min(), yr_test.max()],
             'r--', lw=2, label='Perfect Prediction')
axes[0].set_xlabel('True Ranking Score', fontsize=12)
axes[0].set_ylabel('Predicted Ranking Score', fontsize=12)
axes[0].set_title(f'XGBoost Regressor\nR¬≤ = {r2_xgb:.4f}, RMSE = {rmse_xgb:.4f}',
                 fontsize=14, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Neural Network scatter plot
axes[1].scatter(yr_test, pred_nn, alpha=0.5, s=30, edgecolors='k', linewidth=0.5, color='orange')
axes[1].plot([yr_test.min(), yr_test.max()], [yr_test.min(), yr_test.max()],
             'r--', lw=2, label='Perfect Prediction')
axes[1].set_xlabel('True Ranking Score', fontsize=12)
axes[1].set_ylabel('Predicted Ranking Score', fontsize=12)
axes[1].set_title(f'Neural Network (MLP)\nR¬≤ = {r2_nn:.4f}, RMSE = {rmse_nn:.4f}',
                 fontsize=14, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(f"{SAVE_DIR}/model_predictions_comparison.png", dpi=300, bbox_inches='tight')
plt.close()  # Close figure to free memory

print("‚úÖ Prediction scatter plots saved!")

# Create bar chart comparing all metrics
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
metrics = ['RMSE', 'R¬≤ Score', 'NDCG@20', 'Spearman œÅ']
xgb_values = [rmse_xgb, r2_xgb, ndcg_xgb, rho_xgb]
nn_values = [rmse_nn, r2_nn, ndcg_nn, rho_nn]

for i, (ax, metric, xgb_val, nn_val) in enumerate(zip(axes.flat, metrics, xgb_values, nn_values)):
    x = np.arange(2)
    values = [xgb_val, nn_val]
    bars = ax.bar(x, values, color=['#1f77b4', '#ff7f0e'], alpha=0.8, edgecolor='black')

    # Add value labels on bars
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               f'{height:.4f}',
               ha='center', va='bottom', fontweight='bold')

    ax.set_xticks(x)
    ax.set_xticklabels(['XGBoost', 'Neural Net'])
    ax.set_ylabel(metric, fontsize=12)
    ax.set_title(f'{metric} Comparison', fontsize=13, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)

    # Highlight the better model
    if metric == 'RMSE':
        best_idx = 0 if xgb_val < nn_val else 1
    else:
        best_idx = 0 if xgb_val > nn_val else 1
    bars[best_idx].set_edgecolor('green')
    bars[best_idx].set_linewidth(3)

plt.suptitle('Developer Ranking Models - Performance Comparison',
             fontsize=16, fontweight='bold', y=1.00)
plt.tight_layout()
plt.savefig(f"{SAVE_DIR}/metrics_comparison_bar_chart.png", dpi=300, bbox_inches='tight')
plt.close()  # Close figure to free memory

print("‚úÖ Comparison bar chart saved!")

# Calculate residuals
residuals_xgb = yr_test - pred_xgb_test
residuals_nn = yr_test - pred_nn

# Create residual plots
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# XGBoost residuals scatter
axes[0, 0].scatter(pred_xgb_test, residuals_xgb, alpha=0.5, s=30, edgecolors='k', linewidth=0.5)
axes[0, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)
axes[0, 0].set_xlabel('Predicted Values', fontsize=11)
axes[0, 0].set_ylabel('Residuals', fontsize=11)
axes[0, 0].set_title('XGBoost - Residual Plot', fontsize=12, fontweight='bold')
axes[0, 0].grid(True, alpha=0.3)

# XGBoost residuals histogram
axes[0, 1].hist(residuals_xgb, bins=50, edgecolor='black', alpha=0.7)
axes[0, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)
axes[0, 1].set_xlabel('Residuals', fontsize=11)
axes[0, 1].set_ylabel('Frequency', fontsize=11)
axes[0, 1].set_title('XGBoost - Residual Distribution', fontsize=12, fontweight='bold')
axes[0, 1].grid(True, alpha=0.3)

# Neural Network residuals scatter
axes[1, 0].scatter(pred_nn, residuals_nn, alpha=0.5, s=30,
                   edgecolors='k', linewidth=0.5, color='orange')
axes[1, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)
axes[1, 0].set_xlabel('Predicted Values', fontsize=11)
axes[1, 0].set_ylabel('Residuals', fontsize=11)
axes[1, 0].set_title('Neural Network - Residual Plot', fontsize=12, fontweight='bold')
axes[1, 0].grid(True, alpha=0.3)

# Neural Network residuals histogram
axes[1, 1].hist(residuals_nn, bins=50, edgecolor='black', alpha=0.7, color='orange')
axes[1, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)
axes[1, 1].set_xlabel('Residuals', fontsize=11)
axes[1, 1].set_ylabel('Frequency', fontsize=11)
axes[1, 1].set_title('Neural Network - Residual Distribution', fontsize=12, fontweight='bold')
axes[1, 1].grid(True, alpha=0.3)

plt.suptitle('Residual Analysis - Both Models', fontsize=14, fontweight='bold', y=1.00)
plt.tight_layout()
plt.savefig(f"{SAVE_DIR}/residual_analysis.png", dpi=300, bbox_inches='tight')
plt.close()  # Close figure to free memory

print("‚úÖ Residual analysis plots saved!")
print(f"\nResidual Statistics:")
print(f"XGBoost - Mean: {residuals_xgb.mean():.6f}, Std: {residuals_xgb.std():.6f}")
print(f"Neural Net - Mean: {residuals_nn.mean():.6f}, Std: {residuals_nn.std():.6f}")

"""Feature Importance (XGBoost)"""

# Extract feature importance from XGBoost model
xgb_model = pipe_rank_xgb.named_steps['model']
importance = xgb_model.feature_importances_

# Create feature importance DataFrame
feature_importance_df = pd.DataFrame({
    'Feature': num_features,
    'Importance': importance
}).sort_values('Importance', ascending=False)

# Plot top 20 features
top_n = 20
top_features = feature_importance_df.head(top_n)

plt.figure(figsize=(12, 8))
plt.barh(range(top_n), top_features['Importance'].values,
         edgecolor='black', alpha=0.8)
plt.yticks(range(top_n), top_features['Feature'].values)
plt.xlabel('Feature Importance (Gain)', fontsize=12)
plt.ylabel('Features', fontsize=12)
plt.title(f'Top {top_n} Most Important Features (XGBoost)',
         fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.savefig(f"{SAVE_DIR}/xgb_feature_importance.png", dpi=300, bbox_inches='tight')
plt.close()  # Close figure to free memory

print("‚úÖ Feature importance plot saved!")
print(f"\nTop 10 Most Important Features:")
print(feature_importance_df.head(10).to_string(index=False))

import joblib
from datetime import datetime

# Create timestamp for file naming
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# Save XGBoost pipeline
xgb_model_path = f"{SAVE_DIR}/pipe_rank_xgb_{timestamp}.pkl"
joblib.dump(pipe_rank_xgb, xgb_model_path)
print(f"‚úÖ XGBoost model saved: {xgb_model_path}")

# Save Neural Network model
nn_model_path = f"{SAVE_DIR}/rank_mlp_{timestamp}.h5"
rank_mlp.save(nn_model_path)
print(f"‚úÖ Neural Network model saved: {nn_model_path}")

# Save preprocessing pipeline for NN
nn_prep_path = f"{SAVE_DIR}/rank_nn_preprocessor_{timestamp}.pkl"
joblib.dump(prep_only, nn_prep_path)
print(f"‚úÖ NN preprocessor saved: {nn_prep_path}")

# Save comparison results
results_path = f"{SAVE_DIR}/ranking_models_comparison_{timestamp}.csv"
comparison_df.to_csv(results_path, index=False)
print(f"‚úÖ Comparison results saved: {results_path}")

# Save predictions
predictions_df = pd.DataFrame({
    'id': id_test.values,
    'true_rank': yr_test,
    'xgb_pred': pred_xgb_test,
    'nn_pred': pred_nn,
    'xgb_residual': residuals_xgb,
    'nn_residual': residuals_nn
})
predictions_path = f"{SAVE_DIR}/ranking_predictions_{timestamp}.csv"
predictions_df.to_csv(predictions_path, index=False)
print(f"‚úÖ Predictions saved: {predictions_path}")

print("\n" + "="*60)
print("üéâ ALL MODELS AND RESULTS SAVED SUCCESSFULLY!")
print("="*60)

print("="*80)
print("üìä DEVELOPER RANKING MODELS - FINAL SUMMARY")
print("="*80)

print("\nüéØ MODEL ARCHITECTURES:")
print("\n1. XGBoost Regressor:")
print(f"   - Algorithm: Gradient Boosting (Tree-based)")
print(f"   - Trees: {xgb_ranker.n_estimators}")
print(f"   - Max Depth: {xgb_ranker.max_depth}")
print(f"   - Learning Rate: {xgb_ranker.learning_rate}")
print(f"   - Features: {len(num_features)}")

print("\n2. Neural Network (MLP):")
print(f"   - Architecture: {Xr_train_nn.shape[1]} ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí 1")
print(f"   - Activation: ReLU (hidden), Linear (output)")
print(f"   - Regularization: Dropout (0.2)")
print(f"   - Optimizer: Adam (lr=0.001)")
print(f"   - Training Epochs: {len(hist.history['loss'])}")

print("\nüìà PERFORMANCE COMPARISON:")
print(comparison_df.to_string(index=False))

print("\nüèÜ KEY FINDINGS:")

# Determine overall winner
if xgb_wins > nn_wins:
    print(f"   ‚úì XGBoost Regressor outperforms Neural Network on {xgb_wins}/4 metrics")
    print(f"   ‚úì XGBoost shows better generalization and prediction accuracy")
    print(f"   ‚úì XGBoost is recommended for production deployment")
elif nn_wins > xgb_wins:
    print(f"   ‚úì Neural Network outperforms XGBoost on {nn_wins}/4 metrics")
    print(f"   ‚úì Neural Network captures complex non-linear relationships better")
    print(f"   ‚úì Neural Network is recommended for production deployment")
else:
    print(f"   ‚úì Both models perform equally well (tied at {xgb_wins}/4 metrics)")
    print(f"   ‚úì Consider ensemble approach combining both models")

print("\nüí° INSIGHTS:")
print(f"   ‚Ä¢ Both models achieve strong correlation with true rankings (œÅ > 0.85)")
print(f"   ‚Ä¢ NDCG@20 scores indicate excellent top-k ranking quality")
print(f"   ‚Ä¢ Residual analysis shows normally distributed errors")
print(f"   ‚Ä¢ Feature importance reveals key factors driving developer rankings")

print("\nüìÅ OUTPUT FILES SAVED:")
print(f"   ‚Ä¢ Model files (.pkl, .h5)")
print(f"   ‚Ä¢ Comparison results (.csv)")
print(f"   ‚Ä¢ Predictions with residuals (.csv)")
print(f"   ‚Ä¢ Visualization plots (.png)")

print("\n" + "="*80)
print("‚úÖ ANALYSIS COMPLETE!")
print("="*80)

"""Skill classification - UPDATED FOR FREQUENCY-BASED PROFICIENCY"""

print("="*80)
print("üéØ SKILLS CLASSIFICATION - Frequency-Based Proficiency Scoring")
print("="*80)

print("\nüìä Creating skill proficiency scores based on:")
print("   ‚Ä¢ Frequency: How many repos use this skill")
print("   ‚Ä¢ Usage: Amount of code written in that language")
print("   ‚Ä¢ Recency: Recent usage weighted higher")

# We need to compute skills from repository-level language data
# This requires df_repo (repository dataframe)

# Load repository data from the original repos CSV
print("\nüìÇ Loading repository data for skills proficiency calculation...")
repo_csv = "github_repos_20251023_064928.csv"

if os.path.exists(repo_csv):
    df_repo = pd.read_csv(repo_csv)
    print(f"‚úÖ Loaded {len(df_repo)} repositories from {repo_csv}")
    
    # Rename columns to match expected format
    column_mapping = {
        'owner_login': 'id',  # Map owner to user id
        'stargazer_count': 'stars',
        'fork_count': 'forks',
        'watchers_count': 'watchers',
    }
    
    for old_col, new_col in column_mapping.items():
        if old_col in df_repo.columns and new_col not in df_repo.columns:
            df_repo[new_col] = df_repo[old_col]
    
    # Calculate days_since_update - REQUIRED COLUMN, NO FALLBACK
    if 'updated_at' not in df_repo.columns:
        raise ValueError("‚ùå ERROR: 'updated_at' column not found in repository dataset! Cannot calculate recency for skills.")
    
    from datetime import datetime
    # Convert to datetime and remove timezone info to avoid comparison errors
    df_repo['updated_at'] = pd.to_datetime(df_repo['updated_at'], errors='coerce')
    if df_repo['updated_at'].isna().any():
        raise ValueError("‚ùå ERROR: Some 'updated_at' dates are invalid/missing! Cannot calculate recency.")
    df_repo['updated_at'] = df_repo['updated_at'].dt.tz_localize(None)
    now = datetime.now()
    df_repo['days_since_update'] = (now - df_repo['updated_at']).dt.days
    
    # Ensure primary_language column exists - REQUIRED, NO FALLBACK
    if 'primary_language' not in df_repo.columns:
        if 'language' in df_repo.columns:
            df_repo['primary_language'] = df_repo['language']
        else:
            raise ValueError("‚ùå ERROR: No 'primary_language' or 'language' column found! Cannot determine repository languages for skills.")
    
    # Map actual CSV columns to expected names
    # The CSV has 'stargazer_count', not 'stars'
    if 'stargazer_count' in df_repo.columns and 'stars' not in df_repo.columns:
        df_repo['stars'] = df_repo['stargazer_count']
        print("   ‚úÖ Mapped stargazer_count -> stars")
    
    # Use releases_count as activity proxy (since commits column doesn't exist in CSV)
    # Higher releases = more active development
    if 'releases_count' in df_repo.columns:
        df_repo['activity'] = df_repo['releases_count']
        print("   ‚úÖ Using releases_count for activity metric")
    elif 'languages_total_size' in df_repo.columns:
        # Fallback to code size as activity indicator
        df_repo['activity'] = df_repo['languages_total_size'] / 1000000  # Convert to millions for scale
        print("   ‚úÖ Using languages_total_size for activity metric")
    else:
        # If neither available, set to 0 (frequency and stars will still work)
        df_repo['activity'] = 0
        print("   ‚ö†Ô∏è No activity metric available (releases_count or language_size)")
    
    print(f"‚úÖ Repository data prepared with {len(df_repo['id'].unique())} unique users")
else:
    print(f"‚ùå ERROR: Repository CSV not found: {repo_csv}")
    print("   Cannot calculate skills without repository data!")
    raise FileNotFoundError(f"Required file {repo_csv} not found for skills training")

# Get top 30 most common languages
language_counts = df_repo['primary_language'].value_counts().head(30)
skill_cols = language_counts.index.tolist()

print(f"\n‚úÖ Found {len(skill_cols)} top skills:")
print(f"   {', '.join(skill_cols[:10])}...")

# Calculate skill proficiency scores for each user
print("\nüîÑ Calculating proficiency scores per user...")

user_skill_scores = []

for user_id in df['id'].unique():
    # Get all repos for this user
    user_repos = df_repo[df_repo['id'] == user_id]
    
    skill_scores = {}
    
    for skill in skill_cols:
        # Count repos using this skill
        repos_with_skill = user_repos[user_repos['primary_language'] == skill]
        frequency = len(repos_with_skill)
        
        if frequency == 0:
            skill_scores[skill] = 0.0
            continue
        
        # Calculate usage metrics from actual CSV columns
        total_activity = repos_with_skill['activity'].sum()  # releases_count or language_size
        total_stars = repos_with_skill['stars'].sum()        # mapped from stargazer_count
        
        # Calculate recency (exponential decay based on last update)
        recency_weights = np.exp(-repos_with_skill['days_since_update'] / 180.0)
        avg_recency = recency_weights.mean()
        
        # Composite proficiency score
        # Weights: frequency (40%), activity (30%), stars (20%), recency (10%)
        frequency_score = np.log1p(frequency) * 0.4
        activity_score = np.log1p(total_activity) * 0.3  # releases or code size
        stars_score = np.log1p(total_stars) * 0.2
        recency_score = avg_recency * 0.1
        
        proficiency = frequency_score + activity_score + stars_score + recency_score
        skill_scores[skill] = proficiency
    
    # Normalize to 0-1 range for this user
    max_score = max(skill_scores.values()) if skill_scores.values() else 1.0
    if max_score > 0:
        skill_scores = {k: v / max_score for k, v in skill_scores.items()}
    
    skill_scores['id'] = user_id
    user_skill_scores.append(skill_scores)

# Create labels DataFrame with proficiency scores
labels = pd.DataFrame(user_skill_scores)

print(f"\n‚úÖ Skills proficiency scores calculated!")
print(f"   Total users: {len(labels)}")
print(f"   Total skills: {len(skill_cols)}")
print(f"   Score range: [0.0, 1.0]")
print(f"   Label shape: {labels.shape}")

# Show example
if len(labels) > 0:
    print(f"\nüìä Example proficiency scores for first user:")
    example_scores = labels.iloc[0][skill_cols[:5]]
    for skill in skill_cols[:5]:
        score = example_scores[skill] if skill in example_scores else 0.0
        if score > 0:
            print(f"   {skill:15s}: {score:.3f}")

"""Prepare data"""

df_sk = df.merge(labels, on="id", how="inner")

# Separate features and labels
X_sk = df_sk.drop(columns=["id"] + skill_cols).copy()
y_sk = df_sk[skill_cols].astype(int).copy()

# Train-test split
from sklearn.model_selection import train_test_split
Xsk_tr, Xsk_te, ysk_tr, ysk_te = train_test_split(
    X_sk, y_sk, test_size=0.2, random_state=SEED
)

print(f"‚úÖ Skills data prepared!")
print(f"   Training features: {Xsk_tr.shape}")
print(f"   Test features: {Xsk_te.shape}")
print(f"   Number of labels: {len(skill_cols)}")
print(f"   Label distribution (train):")

# Show label statistics
label_counts = ysk_tr.sum().sort_values(ascending=False)
for i, (label, count) in enumerate(label_counts.head(10).items()):
    print(f"      {i+1}. {label}: {count} ({count/len(ysk_tr)*100:.1f}%)")

from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Create preprocessing pipeline
num_cols = list(X_sk.columns)
prep_sk = ColumnTransformer([
    ("num", Pipeline([
        ("impute", SimpleImputer(strategy="median")),
        ("scale", StandardScaler(with_mean=False))
    ]), num_cols)
])

print(f"‚úÖ Preprocessing pipeline created!")
print(f"   Features to process: {len(num_cols)}")
print(f"   Strategy: Median imputation + Standard scaling")

"""M1: Multi-Output Ridge Regression"""

from sklearn.multioutput import MultiOutputRegressor
from sklearn.linear_model import Ridge

print("\n" + "="*80)
print("üìä MODEL 1: Ridge Regression with Multi-Output")
print("="*80)

# Create Multi-Output Ridge Regression pipeline
pipe_lr = Pipeline([
    ("prep", prep_sk),
    ("model", MultiOutputRegressor(
        Ridge(
            alpha=1.0,
            random_state=SEED
        ),
        n_jobs=-1
    ))
])

print("üöÄ Training Ridge Regression Multi-Output...")
pipe_lr.fit(Xsk_tr, ysk_tr)
print("‚úÖ Ridge Regression training completed!")

"""M2:XGBoost Regressor (Multi-Output)"""

from sklearn.multioutput import MultiOutputRegressor
from xgboost import XGBRegressor

print("\n" + "="*80)
print("üìä MODEL 2: XGBoost with Multi-Output Regression")
print("="*80)

# Create XGBoost Multi-Output Regressor pipeline
pipe_xgb_sk = Pipeline([
    ("prep", prep_sk),
    ("model", MultiOutputRegressor(
        XGBRegressor(
            objective="reg:squarederror",
            tree_method="gpu_hist" if USE_GPU else "hist",  # GPU acceleration
            n_estimators=200,
            max_depth=4,
            learning_rate=0.1,
            subsample=0.9,
            colsample_bytree=0.9,
            random_state=SEED,
            device="cuda" if USE_GPU else "cpu",
            n_jobs=-1
        ),
        n_jobs=-1
    ))
])

print("üöÄ Training XGBoost Multi-Output Regressor...")
pipe_xgb_sk.fit(Xsk_tr, ysk_tr)
print("‚úÖ XGBoost training completed!")

# Store both models
SKILL_MODELS = {
    "Ridge Regression": pipe_lr,
    "XGBoost Regressor": pipe_xgb_sk
}

"""Evaluate Both Models"""

from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score
)

print("\n" + "="*80)
print("üìà EVALUATING SKILLS PROFICIENCY MODELS")
print("="*80)

# Evaluate all models
results_rows = []

for model_name, model in SKILL_MODELS.items():
    print(f"\n‚öôÔ∏è  Evaluating {model_name}...")

    # Get predictions (proficiency scores)
    y_pred = model.predict(Xsk_te)

    # Calculate regression metrics
    mse = mean_squared_error(ysk_te, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(ysk_te, y_pred)
    r2 = r2_score(ysk_te, y_pred)
    
    # Per-output average metrics
    per_skill_mse = mean_squared_error(ysk_te, y_pred, multioutput='raw_values')
    avg_per_skill_rmse = np.mean(np.sqrt(per_skill_mse))
    
    # Calculate ranking correlation (how well it ranks skills)
    # For each user, check if predicted ranking matches true ranking
    ranking_correlations = []
    for i in range(len(ysk_te)):
        true_ranks = np.argsort(ysk_te.iloc[i].values)[::-1]
        pred_ranks = np.argsort(y_pred[i])[::-1]
        # Spearman correlation
        from scipy.stats import spearmanr
        corr, _ = spearmanr(true_ranks, pred_ranks)
        ranking_correlations.append(corr)
    
    avg_rank_corr = np.mean(ranking_correlations)

    results_rows.append([
        model_name,
        rmse,
        mae,
        r2,
        avg_per_skill_rmse,
        avg_rank_corr
    ])

    print(f"   ‚úì RMSE: {rmse:.4f}")
    print(f"   ‚úì MAE: {mae:.4f}")
    print(f"   ‚úì R¬≤ Score: {r2:.4f}")
    print(f"   ‚úì Avg Ranking Correlation: {avg_rank_corr:.4f}")

# Create comparison DataFrame
skills_comparison_df = pd.DataFrame(results_rows, columns=[
    "Model", "RMSE", "MAE", "R¬≤", "Avg Per-Skill RMSE", "Ranking Correlation"
])

print("\n" + "="*80)
print("üèÜ SKILLS PROFICIENCY - MODEL COMPARISON")
print("="*80)
print(skills_comparison_df.to_string(index=False))
print("="*80)

# Store best metrics
skills_lr_r2 = results_rows[0][3]  # R¬≤ for Ridge
skills_xgb_r2 = results_rows[1][3]  # R¬≤ for XGBoost

"""Per-Skill Performance Analysis"""

print("\n" + "="*80)
print("üìä PER-SKILL PROFICIENCY PREDICTION ANALYSIS")
print("="*80)

# Use the best model (prefer XGBoost)
best_model_name = "XGBoost Regressor" if "XGBoost Regressor" in SKILL_MODELS else "Ridge Regression"
best_model = SKILL_MODELS[best_model_name]
y_pred_best = best_model.predict(Xsk_te)

print(f"\nüéØ Using: {best_model_name}\n")

# Calculate per-skill metrics
per_skill_results = []

for i, skill in enumerate(skill_cols):
    y_true_skill = ysk_te.iloc[:, i].values
    y_pred_skill = y_pred_best[:, i]
    
    # Regression metrics for this skill
    skill_mse = mean_squared_error(y_true_skill, y_pred_skill)
    skill_rmse = np.sqrt(skill_mse)
    skill_mae = mean_absolute_error(y_true_skill, y_pred_skill)
    skill_r2 = r2_score(y_true_skill, y_pred_skill)
    
    # Count how many users have this skill (proficiency > 0.1)
    support = np.sum(y_true_skill > 0.1)
    
    per_skill_results.append({
        'Skill': skill,
        'RMSE': skill_rmse,
        'MAE': skill_mae,
        'R¬≤': skill_r2,
        'Support': support
    })

per_skill_df = pd.DataFrame(per_skill_results).sort_values('RMSE')

print("Top 10 Best Predicted Skills (lowest RMSE):")
print(per_skill_df.head(10).to_string(index=False))

print("\nTop 10 Worst Predicted Skills (highest RMSE):")
print(per_skill_df.tail(10).to_string(index=False))

print("\n‚úÖ Per-skill analysis complete!")

"""Label Distribution Analysis"""

# Analyze label distribution
support_train = ysk_tr.sum().sort_values(ascending=False)
support_test = ysk_te.sum().sort_values(ascending=False)

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Training set distribution
axes[0].bar(range(len(support_train)), support_train.values,
           edgecolor='black', alpha=0.8)
axes[0].set_xlabel('Skill Labels', fontsize=12)
axes[0].set_ylabel('Number of Positive Samples', fontsize=12)
axes[0].set_title('Skills Distribution - Training Set', fontsize=14, fontweight='bold')
axes[0].grid(axis='y', alpha=0.3)

# Test set distribution
axes[1].bar(range(len(support_test)), support_test.values,
           edgecolor='black', alpha=0.8, color='orange')
axes[1].set_xlabel('Skill Labels', fontsize=12)
axes[1].set_ylabel('Number of Positive Samples', fontsize=12)
axes[1].set_title('Skills Distribution - Test Set', fontsize=14, fontweight='bold')
axes[1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig(f"{SAVE_DIR}/skills_label_distribution.png", dpi=300, bbox_inches='tight')
plt.close()  # Close figure to free memory

print(f"\n‚úÖ Label distribution plot saved!")
print(f"   Most common skill (train): {support_train.index[0]} ({support_train.values[0]} samples)")
print(f"   Least common skill (train): {support_train.index[-1]} ({support_train.values[-1]} samples)")

# Analyze label distribution
support_train = ysk_tr.sum().sort_values(ascending=False)
support_test = ysk_te.sum().sort_values(ascending=False)

# ‚úÖ Calculate percentages
total_train = len(ysk_tr)
total_test = len(ysk_te)
pct_train = (support_train / total_train * 100)
pct_test = (support_test / total_test * 100)

fig, axes = plt.subplots(1, 2, figsize=(18, 7))

# Training set distribution
bars_train = axes[0].bar(range(len(support_train)), support_train.values,
                          edgecolor='black', alpha=0.8, color='steelblue')
axes[0].set_xlabel('Skill Labels', fontsize=12, fontweight='bold')
axes[0].set_ylabel('Number of Positive Samples', fontsize=12, fontweight='bold')
axes[0].set_title('Skills Distribution - Training Set', fontsize=14, fontweight='bold')
axes[0].set_xticks(range(len(support_train)))
axes[0].set_xticklabels(support_train.index, rotation=90, ha='right', fontsize=9)
axes[0].grid(axis='y', alpha=0.3)

# Add percentage labels on bars (training)
for i, (bar, count, pct) in enumerate(zip(bars_train, support_train.values, pct_train.values)):
    height = bar.get_height()
    axes[0].text(bar.get_x() + bar.get_width()/2., height + max(support_train)*0.01,
                f'{pct:.1f}%',
                ha='center', va='bottom', fontsize=7, rotation=0)

# Test set distribution
bars_test = axes[1].bar(range(len(support_test)), support_test.values,
                         edgecolor='black', alpha=0.8, color='coral')
axes[1].set_xlabel('Skill Labels', fontsize=12, fontweight='bold')
axes[1].set_ylabel('Number of Positive Samples', fontsize=12, fontweight='bold')
axes[1].set_title('Skills Distribution - Test Set', fontsize=14, fontweight='bold')
axes[1].set_xticks(range(len(support_test)))
axes[1].set_xticklabels(support_test.index, rotation=90, ha='right', fontsize=9)
axes[1].grid(axis='y', alpha=0.3)

# Add percentage labels on bars (test)
for i, (bar, count, pct) in enumerate(zip(bars_test, support_test.values, pct_test.values)):
    height = bar.get_height()
    axes[1].text(bar.get_x() + bar.get_width()/2., height + max(support_test)*0.01,
                f'{pct:.1f}%',
                ha='center', va='bottom', fontsize=7, rotation=0)

plt.tight_layout()
plt.savefig(f"{SAVE_DIR}/skills_label_distribution.png", dpi=300, bbox_inches='tight')
plt.close()  # Close figure to free memory

print(f"\n‚úÖ Label distribution plot saved!")
print(f"\nüìä Training Set Statistics:")
print(f"   Most common skill: {support_train.index[0]} ({support_train.values[0]} samples, {pct_train.values[0]:.1f}%)")
print(f"   Least common skill: {support_train.index[-1]} ({support_train.values[-1]} samples, {pct_train.values[-1]:.1f}%)")
print(f"   Imbalance ratio: {support_train.values[0] / support_train.values[-1]:.1f}x")

print(f"\nüìä Test Set Statistics:")
print(f"   Most common skill: {support_test.index[0]} ({support_test.values[0]} samples, {pct_test.values[0]:.1f}%)")
print(f"   Least common skill: {support_test.index[-1]} ({support_test.values[-1]} samples, {pct_test.values[-1]:.1f}%)")
print(f"   Imbalance ratio: {support_test.values[0] / support_test.values[-1]:.1f}x")

"""Regression Model Comparison"""

print("\n" + "="*80)
print("üìä PLOTTING REGRESSION MODEL COMPARISON")
print("="*80)

# Plot comparison for regression metrics
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

metrics_to_plot = [
    ('RMSE', 'Root Mean Squared Error', False),  # Lower is better
    ('MAE', 'Mean Absolute Error', False),
    ('R¬≤', 'R¬≤ Score', True),  # Higher is better
    ('Ranking Correlation', 'Ranking Correlation', True)
]

for idx, (col, title, higher_better) in enumerate(metrics_to_plot):
    ax = axes[idx // 2, idx % 2]

    values = skills_comparison_df[col].values
    colors = ['#1f77b4', '#ff7f0e']
    bars = ax.bar(range(len(SKILL_MODELS)), values, color=colors,
                  edgecolor='black', alpha=0.8)

    # Add value labels
    for bar in bars:
        height = bar.get_height()
        if not np.isnan(height):
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.4f}',
                   ha='center', va='bottom', fontweight='bold', fontsize=10)

    ax.set_xticks(range(len(SKILL_MODELS)))
    ax.set_xticklabels(list(SKILL_MODELS.keys()), fontsize=11)
    ax.set_ylabel(col, fontsize=11)
    ax.set_title(title, fontsize=12, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)

    # Highlight better model
    if not any(np.isnan(values)):
        best_idx = np.argmin(values) if not higher_better else np.argmax(values)
        bars[best_idx].set_edgecolor('green')
        bars[best_idx].set_linewidth(3)

plt.suptitle('Skills Proficiency Regression - Model Performance Comparison',
            fontsize=16, fontweight='bold', y=0.995)
plt.tight_layout()
plt.savefig(f"{SAVE_DIR}/skills_models_comparison.png", dpi=300, bbox_inches='tight')
plt.close()  # Close figure to free memory

print("‚úÖ Model comparison charts saved!")

print("\n" + "="*80)
print("üìä SKILLS PROFICIENCY - TRAINING COMPLETED")
print("="*80)

# Summary of best model
best_model_idx = skills_comparison_df['R¬≤'].idxmax()
best_model_row = skills_comparison_df.iloc[best_model_idx]
print(f"\nüèÜ Best Model: {best_model_row['Model']}")
print(f"   R¬≤: {best_model_row['R¬≤']:.4f}")
print(f"   RMSE: {best_model_row['RMSE']:.4f}")
print(f"   Ranking Correlation: {best_model_row['Ranking Correlation']:.4f}")

print("\n‚úÖ Skills proficiency model training complete!")
print("="*80)

"""Behaviour Analysis

Create Behavioral Labels from Proxy Features
"""

print("="*80)
print("üé≠ BEHAVIORAL CLASSIFICATION - Developer Behavior Patterns")
print("="*80)

# Define proxy features for each behavioral pattern
proxy_defs = {
    "maintainer": "maintainer_score",
    "team_player": "team_player_score",
    "innovator": "innovation_index",
    "learner": "learning_velocity"
}

# Check if proxy features are in the current df (shortlist)
missing_proxies = [v for v in proxy_defs.values() if v not in df.columns]

if missing_proxies:
    print(f"\n‚ö†Ô∏è Missing proxy features in shortlist: {missing_proxies}")
    print("   Loading from final_features...")
    
    # Load from final_features
    if final_features_files:
        df_full_beh = pd.read_csv(final_features_files[-1])
        proxy_cols_to_merge = ['login'] + [v for v in proxy_defs.values() if v in df_full_beh.columns]
        
        if len(proxy_cols_to_merge) > 1:
            # Merge using 'id' from df and 'login' from df_full_beh
            df = df.merge(df_full_beh[proxy_cols_to_merge], left_on='id', right_on='login', how='left', suffixes=('', '_full'))
            print(f"   ‚úÖ Merged {len(proxy_cols_to_merge)-1} proxy features from final_features")
        else:
            raise ValueError("Cannot train behavior model without proxy features")
    else:
        raise FileNotFoundError("final_features file not found for behavior proxies")

# Filter to only include available columns
proxy_defs = {k: v for k, v in proxy_defs.items() if v in df.columns}

if not proxy_defs:
    raise ValueError("No proxy-able behavior columns found! Cannot train behavior model.")

print(f"‚úÖ Behavioral patterns defined:")
print(f"   {list(proxy_defs.keys())}")
print(f"‚úÖ All proxy features available in dataset")

# Create behavioral labels using threshold (top 30% = positive)
df_bh = df.copy()

THRESHOLD_PERCENTILE = 0.70  # Top 30% are labeled as positive

print(f"\nüìä Creating labels (threshold: {THRESHOLD_PERCENTILE:.0%} percentile):\n")

for label, feature in proxy_defs.items():
    threshold = df_bh[feature].quantile(THRESHOLD_PERCENTILE)
    df_bh[label] = (df_bh[feature] >= threshold).astype(int)
    positives = int(df_bh[label].sum())
    positive_pct = positives / len(df_bh) * 100

    print(f"   {label:15s}: threshold={threshold:.4f}  positives={positives:4d} ({positive_pct:.1f}%)")

beh_cols = list(proxy_defs.keys())

"""Prepare Behavioral Classification Data"""

drop_cols = ["id"] + list(proxy_defs.values()) + beh_cols
X_bh = df_bh.drop(columns=[c for c in drop_cols if c in df_bh.columns]).copy()
y_bh = df_bh[beh_cols].astype(int).copy()

# Train-test split
Xbh_tr, Xbh_te, ybh_tr, ybh_te = train_test_split(
    X_bh, y_bh, test_size=0.2, random_state=SEED
)

print(f"\n‚úÖ Behavioral data prepared!")
print(f"   Training features: {Xbh_tr.shape}")
print(f"   Test features: {Xbh_te.shape}")
print(f"   Training labels: {ybh_tr.shape}")
print(f"   Test labels: {ybh_te.shape}")

# Show label distribution
print(f"\nüìä Label Distribution:")
for col in beh_cols:
    train_pos = ybh_tr[col].sum()
    test_pos = ybh_te[col].sum()
    train_pct = train_pos / len(ybh_tr) * 100
    test_pct = test_pos / len(ybh_te) * 100
    print(f"   {col:15s}: Train={train_pos:3d} ({train_pct:.1f}%), Test={test_pos:3d} ({test_pct:.1f}%)")

"""Train Behavioral Classification Models"""

print("\n" + "="*80)
print("üìà EVALUATING BEHAVIORAL CLASSIFICATION MODELS")
print("="*80)

"""Preprocessing Pipeline"""

prep_bh = ColumnTransformer([
    ("num", Pipeline([
        ("impute", SimpleImputer(strategy="median")),
        ("scale", StandardScaler())
    ]), X_bh.select_dtypes(include=['float64', 'int64']).columns.tolist())
], remainder='passthrough')

# Define models
BEHAVIOR_MODELS = {
    "Logistic Regression OvR": Pipeline([
        ("prep", prep_bh),
        ("model", OneVsRestClassifier(
            LogisticRegression(max_iter=1000, random_state=SEED, class_weight='balanced')
        ))
    ]),
    "SVM RBF OvR": Pipeline([
        ("prep", prep_bh),
        ("model", OneVsRestClassifier(
            SVC(kernel='rbf', probability=True, random_state=SEED, class_weight='balanced')
        ))
    ])
}

# Train and evaluate models
behavior_results = []

for model_name, pipeline in BEHAVIOR_MODELS.items():
    print(f"\n‚öôÔ∏è  Training {model_name}...")
    pipeline.fit(Xbh_tr, ybh_tr)
    y_pred = pipeline.predict(Xbh_te)
    
    # Calculate metrics
    from sklearn.metrics import (
        f1_score, hamming_loss, jaccard_score,
        label_ranking_average_precision_score, coverage_error, accuracy_score
    )
    
    micro_f1 = f1_score(ybh_te, y_pred, average='micro', zero_division=0)
    macro_f1 = f1_score(ybh_te, y_pred, average='macro', zero_division=0)
    weighted_f1 = f1_score(ybh_te, y_pred, average='weighted', zero_division=0)
    hamming = hamming_loss(ybh_te, y_pred)
    jaccard_micro = jaccard_score(ybh_te, y_pred, average='micro', zero_division=0)
    jaccard_macro = jaccard_score(ybh_te, y_pred, average='macro', zero_division=0)
    exact_match = accuracy_score(ybh_te, y_pred)
    
    behavior_results.append([
        model_name, micro_f1, macro_f1, weighted_f1,
        hamming, jaccard_micro, jaccard_macro,
        exact_match
    ])
    
    print(f"   ‚úì Micro F1: {micro_f1:.4f}")
    print(f"   ‚úì Macro F1: {macro_f1:.4f}")
    print(f"   ‚úì Hamming Loss: {hamming:.4f}")

# Create comparison DataFrame
behavior_comparison_df = pd.DataFrame(behavior_results, columns=[
    "Model", "Micro F1", "Macro F1", "Weighted F1",
    "Hamming Loss", "Jaccard Micro", "Jaccard Macro",
    "Exact Match"
])

print("\n" + "="*80)
print("üèÜ BEHAVIORAL CLASSIFICATION - MODEL COMPARISON")
print("="*80)
print(behavior_comparison_df.to_string(index=False))
print("="*80)

# Select best model
best_behavior_idx = behavior_comparison_df['Micro F1'].idxmax()
best_behavior_model = behavior_comparison_df.iloc[best_behavior_idx]['Model']

print(f"\nüèÜ Best Model: {best_behavior_model}")
print(f"   Micro F1: {behavior_comparison_df.iloc[best_behavior_idx]['Micro F1']:.4f}")
print(f"   Exact Match: {behavior_comparison_df.iloc[best_behavior_idx]['Exact Match']:.4f}")

print("\n‚úÖ Behavioral classification training complete!")
print("="*80)

"""Save All Models and Results"""

print("\n" + "="*80)
print("üíæ SAVING MODELS AND RESULTS")
print("="*80)

# Save ranking models to training_outputs
print("\nüìä Saving Ranking Models...")
joblib.dump(pipe_rank_xgb, f"{SAVE_DIR}/ranking_xgboost.pkl")
print(f"‚úÖ Saved: ranking_xgboost.pkl")

if TENSORFLOW_AVAILABLE and rank_mlp is not None:
    rank_mlp.save(f"{SAVE_DIR}/ranking_mlp.h5")
    print(f"‚úÖ Saved: ranking_mlp.h5")

# Save skills model to training_outputs
print("\nüìä Saving Skills Model...")
best_skills_model_name = skills_comparison_df.iloc[skills_comparison_df['R¬≤'].idxmax()]['Model']
best_skills_model = SKILL_MODELS[best_skills_model_name]
joblib.dump(best_skills_model, f"{SAVE_DIR}/skills_classifier.pkl")
print(f"‚úÖ Saved: skills_classifier.pkl ({best_skills_model_name})")

# Save behavior model to training_outputs
print("\nüìä Saving Behavior Model...")
best_behavior_pipeline = BEHAVIOR_MODELS[best_behavior_model]
joblib.dump(best_behavior_pipeline, f"{SAVE_DIR}/behavior_classifier.pkl")
print(f"‚úÖ Saved: behavior_classifier.pkl ({best_behavior_model})")

# Save comparison results
print("\nüìä Saving Comparison Results...")
skills_comparison_df.to_csv(f"{SAVE_DIR}/skills_comparison_{timestamp}.csv", index=False)
behavior_comparison_df.to_csv(f"{SAVE_DIR}/behavior_comparison_{timestamp}.csv", index=False)
print(f"‚úÖ Saved comparison CSVs")

print("\n" + "="*80)
print("üéâ ALL MODELS TRAINED AND SAVED SUCCESSFULLY!")
print("="*80)
print(f"\nüìÇ All Models and Training Outputs Saved to: {SAVE_DIR}/")
print(f"\n‚úÖ Training Complete! All models are ready for portfolio generation.")
