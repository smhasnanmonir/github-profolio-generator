# Repository-Level Ranking Metrics - Update

## ‚úÖ What Changed

The ranking model now uses **repository-level metrics** directly from `github_repos_20251023_064928.csv` instead of user-aggregated totals.

---

## üìä New Metrics Used

### From Repository CSV (`github_repos_20251023_064928.csv`)

**Required Columns (all NO FALLBACKS):**

1. **`stargazer_count`** - Total stars for each repository
   - Aggregated per user (sum across all their repos)
   - Bigger = Better ‚úÖ

2. **`fork_count`** - Total forks for each repository
   - Aggregated per user (sum across all their repos)
   - Bigger = Better ‚úÖ

3. **`watchers_count`** - Total watchers for each repository
   - Aggregated per user (sum across all their repos)
   - Bigger = Better ‚úÖ

4. **`languages_total_size`** - Total size of code in different languages
   - Aggregated per user (sum across all their repos)
   - Bigger = Better ‚úÖ (More code volume)

5. **`languages_total_count`** - Total number of different languages used
   - Aggregated per user (sum across all their repos)
   - Bigger = Better ‚úÖ (More language diversity)

### Additional Context (from user features)

6. **`total_commit_contributions`** - Total commits by the user
   - Bigger = Better ‚úÖ (Development effort)

7. **`recent_activity_ratio`** - Recent activity score
   - Bigger = Better ‚úÖ (Recent contributions)

---

## ‚öñÔ∏è New Ranking Weights

The ranking formula now uses **7 components** instead of 5:

```python
NEW_RANK_WEIGHTS = {
    'stars': 0.30,          # 30% - Repository popularity (stargazer_count)
    'forks': 0.20,          # 20% - Code reuse (fork_count)
    'watchers': 0.10,       # 10% - Active interest (watchers_count)
    'lang_size': 0.15,      # 15% - Code volume (languages_total_size)
    'lang_count': 0.10,     # 10% - Language diversity (languages_total_count)
    'commits': 0.10,        # 10% - Development effort
    'recency': 0.05,        # 5% - Recent activity bonus
}
```

**Total: 100%**

---

## üîÑ Before vs After

### Before (User-Aggregated Metrics)
```python
# Used pre-aggregated user totals from final_features.csv
total_stars_received      # 35%
total_forks_received      # 25%
total_watchers            # 15%
total_commit_contributions # 15%
recent_activity_ratio     # 10%
```

### After (Repository-Level Metrics)
```python
# Loads actual repository data and aggregates it
stargazer_count (sum)           # 30%
fork_count (sum)                # 20%
watchers_count (sum)            # 10%
languages_total_size (sum)      # 15% ‚≠ê NEW
languages_total_count (sum)     # 10% ‚≠ê NEW
total_commit_contributions      # 10%
recent_activity_ratio           # 5%
```

---

## üéØ Benefits

1. **More Granular Data**: Uses actual per-repo metrics instead of pre-aggregated totals
2. **Language Metrics**: Now considers code volume and language diversity
3. **Better Ranking**: Repositories with more code and diverse languages rank higher
4. **Bigger = Better**: All metrics are normalized where higher values = better ranking
5. **No Fallbacks**: Fails fast if repository CSV or required columns are missing

---

## üö® Error Handling

All new checks with NO FALLBACKS:

```python
# Check repository CSV exists
if not os.path.exists(repo_csv):
    raise FileNotFoundError("‚ùå ERROR: Repository CSV not found!")

# Check required columns exist
required_repo_cols = ['stargazer_count', 'fork_count', 'watchers_count', 
                      'languages_total_size', 'languages_total_count']
if missing_repo_cols:
    raise ValueError("‚ùå ERROR: Required repository columns missing!")

# Check merge was successful
if df['stargazer_count'].isna().all():
    raise ValueError("‚ùå ERROR: Failed to merge repository data!")
```

---

## üìà Expected Output

When training, you should see:

```
   Loading repository-level metrics for ranking...
   ‚úÖ Loaded 47224 repositories
   ‚úÖ Aggregated metrics for 6629 users
   Metrics: stargazer_count, fork_count, watchers_count, languages_total_size, languages_total_count
   ‚úÖ Merged repository metrics into user dataframe
   ‚úÖ Using total_commit_contributions for commits metric
   ‚úÖ Using recent_activity_ratio for recency score

   Normalizing components (bigger = better)...
   Weights: {'stars': 0.3, 'forks': 0.2, 'watchers': 0.1, 'lang_size': 0.15, 'lang_count': 0.1, 'commits': 0.1, 'recency': 0.05}

‚úÖ Ranking target created!
   Components used: 7
   Target stats - Mean: X.XXXX, Std: X.XXXX
```

---

## üîß Technical Details

### Data Flow

1. **Load**: `github_repos_20251023_064928.csv` ‚Üí `df_repos`
2. **Validate**: Check for required columns (`stargazer_count`, etc.)
3. **Map**: `owner_login` ‚Üí `id` for merging with user data
4. **Aggregate**: Sum all metrics per user (all their repos)
5. **Merge**: Join with user features dataframe
6. **Normalize**: Scale all components to 0-1 using `robust01()`
7. **Weight**: Apply weights to normalized components
8. **Calculate**: Sum weighted components to create `rank_target`

### Aggregation Method

```python
repo_agg = df_repos.groupby('id').agg({
    'stargazer_count': 'sum',           # SUM of all repo stars
    'fork_count': 'sum',                # SUM of all repo forks
    'watchers_count': 'sum',            # SUM of all repo watchers
    'languages_total_size': 'sum',      # SUM of all language sizes
    'languages_total_count': 'sum',     # SUM of all language counts
}).reset_index()
```

---

## ‚úÖ Summary

**Changes:**
- ‚úÖ Using repository-level metrics (per-repo ‚Üí aggregated per user)
- ‚úÖ Added `languages_total_size` (15% weight)
- ‚úÖ Added `languages_total_count` (10% weight)
- ‚úÖ Adjusted weights to total 100%
- ‚úÖ All metrics: bigger = better
- ‚úÖ No fallbacks - fails fast if data missing

**Result:**
- More accurate ranking based on actual repository data
- Rewards code volume and language diversity
- Better model performance expected with richer features

---

**Last Updated:** 2025-11-17
**File Modified:** `ml_model.py` (Lines 1012-1115)

