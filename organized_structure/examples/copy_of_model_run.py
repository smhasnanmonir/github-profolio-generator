# -*- coding: utf-8 -*-
"""Copy of model_run.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gFRJt4VRRDZVT81oAwHEfh1ODQlAdNyg
"""

from google.colab import drive

drive.mount('/content/drive')

import pandas as pd


df_users = pd.read_csv('/content/drive/MyDrive/Dataset/github_users_final.csv')
df_repos = pd.read_csv('/content/drive/MyDrive/Dataset/github_repos_final.csv')

print(f" Loaded user data: {df_users.shape}")
print(f" Loaded repo data: {df_repos.shape}")

# Import required libraries
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.metrics import silhouette_score, calinski_harabasz_score
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd


pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 20)

def load_processed_datasets(dataset_path="/content/drive/MyDrive/Dataset/"):
    try:
        # Load the processed datasets
        df_users = pd.read_csv(dataset_path + "github_users_final.csv")
        df_repos = pd.read_csv(dataset_path + "github_repos_final.csv")

        print(f" Loaded user data: {df_users.shape}")
        print(f"Loaded repo data: {df_repos.shape}")

        # Check data quality
        print(f"\n Data Overview:")
        print(f"   User features: {len(df_users.columns)}")
        print(f"   Repo features: {len(df_repos.columns)}")
        print(f"   Missing values (users): {df_users.isnull().sum().sum()}")
        print(f"   Missing values (repos): {df_repos.isnull().sum().sum()}")

        # Show available features
        print(f"\n Available user features:")
        feature_categories = {
            'Basic Info': [col for col in df_users.columns if col in ['login', 'created_at', 'followers', 'following']],
            'Activity': [col for col in df_users.columns if 'commits' in col or 'prs' in col or 'issues' in col],
            'Repository': [col for col in df_users.columns if 'repo' in col or 'stars' in col or 'forks' in col],
            'Ratios': [col for col in df_users.columns if 'ratio' in col or 'per_day' in col],
            'Log Features': [col for col in df_users.columns if col.endswith('_log')]
        }

        for category, features in feature_categories.items():
            if features:
                print(f"   {category}: {len(features)} features")
                print(f"      {', '.join(features[:3])}{'...' if len(features) > 3 else ''}")

        return df_users, df_repos

    except FileNotFoundError as e:
        print(f" File not found: {e}")
        print(f" Make sure you've run the preprocessing script and saved the CSV files!")
        print(f" Expected files:")
        print(f"    {dataset_path}github_users_final.csv")
        print(f"   {dataset_path}github_repos_final.csv")
        return None, None

    except Exception as e:
        print(f" Error loading datasets: {e}")
        return None, None

# Try to use existing df first, otherwise load from CSV
if 'df' in locals() and not df.empty:
    print(" Using existing processed data from memory")
    df_users = df.copy()
    df_repos = repo_df.copy() if 'repo_df' in locals() else pd.DataFrame()
else:
    print(" Loading data from saved CSV files...")
    df_users, df_repos = load_processed_datasets()

if df_users is not None:
    print(f"\n Data loaded successfully!")
    print(f" Ready for behavior analysis with {df_users.shape[0]} users")
else:
    print(f" Could not load data! Please run the preprocessing cells first.")

#
#  FEATURE ENGINEERING FOR BEHAVIOR ANALYSIS
#

print("ğŸ”§ Creating behavior-specific features for ML...")

# Create a copy for behavior analysis
df_behavior = df_users.copy()

# Check if required columns exist
required_cols = ['commits_per_day', 'prs_per_day', 'total_reviews', 'account_age_days',
                'followers', 'stars_total', 'total_prs', 'total_commits']

missing_cols = [col for col in required_cols if col not in df_behavior.columns]
if missing_cols:
    print(f" Missing columns: {missing_cols}")
    print(" Make sure you're using the fully processed dataset!")
else:
    print(" All required columns found!")

#composite behavior scores
df_behavior['activity_score'] = (
    df_behavior['commits_per_day'] * 0.3 +
    df_behavior['prs_per_day'] * 0.4 +
    df_behavior['total_reviews'] / (df_behavior['account_age_days'] + 1) * 0.3
)

df_behavior['popularity_score'] = (
    np.log1p(df_behavior['followers']) * 0.4 +
    np.log1p(df_behavior['stars_total']) * 0.6
)

df_behavior['collaboration_score'] = (
    df_behavior['total_prs'] / (df_behavior['total_commits'] + 1) * 0.5 +
    df_behavior['total_reviews'] / (df_behavior['total_prs'] + 1) * 0.5
)

# Handle potential missing columns
if 'avg_files_per_pr' in df_behavior.columns and 'pr_additions' in df_behavior.columns:
    df_behavior['code_quality_score'] = (
        df_behavior['avg_files_per_pr'] * 0.3 +
        (df_behavior['pr_additions'] / (df_behavior['pr_deletions'] + 1)) * 0.4 +
        df_behavior.get('loc_per_file_ratio', 0) * 0.3
    )
else:
    # Simplified version if some features are missing
    df_behavior['code_quality_score'] = df_behavior.get('avg_files_per_pr', 1.0)

print(f" Created 4 composite behavior scores")

# Feature selection for behavior analysis
behavior_features = [
    # Activity patterns
    'commits_per_day', 'prs_per_day', 'activity_score',

    # Social coding
    'followers_to_following_ratio', 'collaboration_score', 'popularity_score',

    # Code characteristics
    'avg_loc_per_pr', 'avg_files_per_pr', 'code_quality_score',
    'est_loc_per_commit', 'commit_to_pr_ratio',

    # Experience & scale
    'account_age_days', 'total_repos', 'avg_stars_repo',

    # Log-transformed features (better for clustering)
    'followers_log', 'stars_total_log', 'total_commits_log',
    'pr_additions_log', 'total_prs_log'
]

# Filter to existing columns
behavior_features = [f for f in behavior_features if f in df_behavior.columns]
X = df_behavior[behavior_features].copy()

# Handle missing values
X = X.fillna(X.median())

print(f"Selected {len(behavior_features)} features for behavior analysis")
print(f" Features: {behavior_features}")
print(f" Feature matrix shape: {X.shape}")
print(f" Missing values: {X.isnull().sum().sum()}")

# Display the new behavior scores
print("\n Behavior Scores Sample:")
display(df_behavior[['login', 'activity_score', 'popularity_score',
                     'collaboration_score', 'code_quality_score']].head())

#  FEATURE SELECTION AND DATA PREPARATION
#

print("Selecting features for behavior analysis...")

behavior_features = [
    # Activity patterns
    'commits_per_day', 'prs_per_day', 'activity_score',

    # Social coding
    'followers_to_following_ratio', 'collaboration_score', 'popularity_score',

    # Code characteristics
    'avg_loc_per_pr', 'avg_files_per_pr', 'code_quality_score',
    'est_loc_per_commit', 'commit_to_pr_ratio',

    # Experience & scale
    'account_age_days', 'total_repos', 'avg_stars_repo',

    # Log-transformed features (better for clustering)
    'followers_log', 'stars_total_log', 'total_commits_log',
    'pr_additions_log', 'total_prs_log'
]

# Filter to existing columns
behavior_features = [f for f in behavior_features if f in df_behavior.columns]
X = df_behavior[behavior_features].copy()

# Handle missing values
X = X.fillna(X.median())

print(f" Selected {len(behavior_features)} features:")
for i, feature in enumerate(behavior_features, 1):
    print(f"   {i:2d}. {feature}")

print(f"\n Feature matrix shape: {X.shape}")
print(f" Missing values: {X.isnull().sum().sum()}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  DATA SCALING AND DIMENSIONALITY REDUCTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print(" Scaling features...")
scaler = RobustScaler()  # Better for outliers than StandardScaler
X_scaled = scaler.fit_transform(X)
X_scaled_df = pd.DataFrame(X_scaled, columns=behavior_features, index=X.index)

print(" Applying dimensionality reduction...")

# PCA for feature understanding
pca = PCA(n_components=0.95)  # Keep 95% of variance
X_pca = pca.fit_transform(X_scaled)

# t-SNE for visualization
tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X)//4))
X_tsne = tsne.fit_transform(X_scaled)

print(f" PCA: {X_scaled.shape[1]} â†’ {X_pca.shape[1]} components")
print(f" t-SNE: 2D visualization ready")

# Show explained variance
print(f"\n PCA Explained Variance Ratio:")
for i, var in enumerate(pca.explained_variance_ratio_[:5], 1):
    print(f"   Component {i}: {var:.3f} ({var*100:.1f}%)")

# Visualize feature importance in first 3 PCA components
feature_importance = pd.DataFrame({
    'feature': behavior_features,
    'PC1': np.abs(pca.components_[0]),
    'PC2': np.abs(pca.components_[1]),
    'PC3': np.abs(pca.components_[2]) if pca.n_components_ > 2 else 0
})

print(f"\n Top features in Principal Components:")
display(feature_importance.nlargest(10, 'PC1')[['feature', 'PC1', 'PC2', 'PC3']])

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the CSV file
df_csv = pd.read_csv('/content/drive/MyDrive/Dataset/github_users_final.csv')

print("Head of the CSV DataFrame:")
print(df_csv.head())

print("\nInfo of the CSV DataFrame:")
print(df_csv.info())

# Define the specific 19 fields to be used for clustering
specified_numerical_cols_raw = [
    'commits_per_day',
    'prs_per_day',
    'activity_score',
    'followers_to_following_ratio',
    'collaboration_score',
    'popularity_score',
    'avg_loc_per_pr',
    'avg_files_per_pr',
    'code_quality_score',
    'est_loc_per_commit',
    'commit_to_pr_ratio',
    'account_age_days',
    'total_repos',
    'avg_stars_repo',
    'followers_log',
    'stars_total_log',
    'total_commits_log',
    'pr_additions_log',
    'total_prs_log'
]

actual_numerical_cols = []
missing_cols = []
available_cols_in_df = df_csv.columns.tolist()

# Iterate through the user's requested columns
for col in specified_numerical_cols_raw:
    if col in available_cols_in_df:
        actual_numerical_cols.append(col)
    elif col == 'popularity_score' and 'popularity_score_total' in available_cols_in_df:
        actual_numerical_cols.append('popularity_score_total')
        print(f"Note: Using 'popularity_score_total' as a proxy for '{col}'.")
    else:
        missing_cols.append(col)

if missing_cols:
    print(f"\nWarning: The following requested columns were not found in the CSV and will be skipped: {', '.join(set(missing_cols))}")

if not actual_numerical_cols:
    print("No numerical columns available for clustering after filtering. Exiting.")
    exit()

print(f"\nSelected Numerical Columns for Clustering: {actual_numerical_cols}")

# Handle missing values
for col in actual_numerical_cols:
    df_csv[col] = df_csv[col].fillna(0)

# Extract features
X = df_csv[actual_numerical_cols].copy()

# Scale the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# K-Means Clustering Analysis
print("\nK-Means Clustering Analysis...")

clustering_results = {}
silhouette_scores = []

# Define k range
max_k_possible = len(X) - 1
k_range_end = min(11, max_k_possible)

if k_range_end < 2:
    print("Not enough data to perform clustering with multiple clusters (minimum 2 samples required).")
else:
    k_range = range(2, k_range_end + 1)
    print(f"Testing K-Means with k = {list(k_range)}")

    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(X_scaled)

        if len(np.unique(clusters)) < 2:
            print(f"  K={k}: Only one cluster formed. Cannot calculate Silhouette Score.")
            score = -1.0
        else:
            score = silhouette_score(X_scaled, clusters)
            print(f"  K={k}: Silhouette = {score:.3f}")

        silhouette_scores.append(score)
        clustering_results[f'kmeans_{k}'] = {
            'labels': clusters,
            'model': kmeans,
            'silhouette': score,
            'method': 'K-Means',
            'n_clusters': k
        }

    valid_silhouette_scores = [s for s in silhouette_scores if s != -1.0]
    valid_k_range = [k for k, s in zip(k_range, silhouette_scores) if s != -1.0]

    if valid_silhouette_scores:
        optimal_k_index = np.argmax(valid_silhouette_scores)
        optimal_k = valid_k_range[optimal_k_index]
        print(f"\nOptimal K-Means clusters: {optimal_k} (silhouette: {max(valid_silhouette_scores):.3f})")

        plt.figure(figsize=(12, 4))

        plt.subplot(1, 2, 1)
        plt.plot(valid_k_range, valid_silhouette_scores, 'bo-')
        plt.xlabel('Number of Clusters (k)')
        plt.ylabel('Silhouette Score')
        plt.title('K-Means: Silhouette Score vs Number of Clusters')
        plt.axvline(x=optimal_k, color='red', linestyle='--', alpha=0.7)

        inertias = []
        for k in k_range:
            if k <= len(X):
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                kmeans.fit(X_scaled)
                inertias.append(kmeans.inertia_)
            else:
                inertias.append(np.nan)

        plt.subplot(1, 2, 2)
        inertias_filtered = [i for i in inertias if not np.isnan(i)]
        k_range_inertia_filtered = [k for k, i in zip(k_range, inertias) if not np.isnan(i)]
        plt.plot(k_range_inertia_filtered, inertias_filtered, 'ro-')
        plt.xlabel('Number of Clusters (k)')
        plt.ylabel('Inertia')
        plt.title('K-Means: Elbow Method')
        plt.axvline(x=optimal_k, color='red', linestyle='--', alpha=0.7)

        plt.tight_layout()
        plt.show()

        # Best Model Selection and Visualization
        best_model = max(clustering_results.items(), key=lambda x: x[1]['silhouette'] if x[1]['silhouette'] != -1.0 else -float('inf'))
        best_name, best_result = best_model

        print(f"\nBest clustering model: {best_result['method']} with {best_result['n_clusters']} clusters")
        print(f"  Silhouette score: {best_result['silhouette']:.3f}")

        df_csv['developer_type'] = best_result['labels']

        if len(X_scaled) > 1:
            tsne_perplexity = min(30, len(X_scaled) - 1)
            if tsne_perplexity < 1:
                print("\nNot enough data points for t-SNE visualization (need at least 2).")
                X_tsne = None
            else:
                tsne = TSNE(n_components=2, random_state=42, perplexity=tsne_perplexity)
                X_tsne = tsne.fit_transform(X_scaled)

            pca = PCA(n_components=2, random_state=42)
            X_pca = pca.fit_transform(X_scaled)

            plt.figure(figsize=(15, 5))

            if X_tsne is not None:
                plt.subplot(1, 3, 1)
                scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=best_result['labels'],
                                    cmap='viridis', alpha=0.6, s=50)
                plt.colorbar(scatter)
                plt.title(f'{best_result["method"]} Clusters\\n(t-SNE Visualization)')
                plt.xlabel('t-SNE Component 1')
                plt.ylabel('t-SNE Component 2')
            else:
                plt.subplot(1, 3, 1)
                plt.text(0.5, 0.5, "t-SNE plot not available\n(Not enough data points)",
                         horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)
                plt.title('t-SNE Visualization')

            plt.subplot(1, 3, 2)
            scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=best_result['labels'],
                                cmap='viridis', alpha=0.6, s=50)
            plt.colorbar(scatter)
            plt.title(f'{best_result["method"]} Clusters\\n(PCA Visualization)')
            plt.xlabel('First Principal Component')
            plt.ylabel('Second Principal Component')

            plt.subplot(1, 3, 3)
            cluster_counts = pd.Series(best_result['labels']).value_counts().sort_index()
            plt.bar(cluster_counts.index, cluster_counts.values, color='skyblue', alpha=0.7)
            plt.title('Cluster Size Distribution')
            plt.xlabel('Cluster ID')
            plt.ylabel('Number of Developers')

            for i, v in enumerate(cluster_counts.values):
                plt.text(cluster_counts.index[i], v + 1, str(v), ha='center', va='bottom')

            plt.tight_layout()
            plt.show()

            print(f"\nCluster Distribution:")
            for cluster_id, count in cluster_counts.items():
                percentage = count / len(best_result['labels']) * 100
                print(f"  Cluster {cluster_id}: {count:4d} developers ({percentage:5.1f}%)")
        else:
            print("\nNot enough data points for t-SNE or PCA visualization (need at least 2).")
    else:
        print("\nNo valid silhouette scores were calculated for any K. Cannot determine optimal K or visualize clusters.")

from google.colab import drive
drive.mount('/content/drive')

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BEST MODEL SELECTION AND VISUALIZATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Select best clustering model
best_model = max(clustering_results.items(), key=lambda x: x[1]['silhouette'])
best_name, best_result = best_model

print(f" Best clustering model: {best_result['method']} with {best_result['n_clusters']} clusters")
print(f"   Silhouette score: {best_result['silhouette']:.3f}")

# Add best clusters to dataframe
df_behavior['developer_type'] = best_result['labels']

# Visualize clusters using t-SNE
plt.figure(figsize=(15, 5))

# Plot 1: t-SNE visualization
plt.subplot(1, 3, 1)
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=best_result['labels'],
                     cmap='viridis', alpha=0.6, s=50)
plt.colorbar(scatter)
plt.title(f'{best_result["method"]} Clusters\n(t-SNE Visualization)')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')

# Plot 2: PCA visualization
plt.subplot(1, 3, 2)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=best_result['labels'],
                     cmap='viridis', alpha=0.6, s=50)
plt.colorbar(scatter)
plt.title(f'{best_result["method"]} Clusters\n(PCA Visualization)')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')

# Plot 3: Cluster size distribution
plt.subplot(1, 3, 3)
cluster_counts = pd.Series(best_result['labels']).value_counts().sort_index()
plt.bar(cluster_counts.index, cluster_counts.values, color='skyblue', alpha=0.7)
plt.title('Cluster Size Distribution')
plt.xlabel('Cluster ID')
plt.ylabel('Number of Developers')

# Add count labels on bars
for i, v in enumerate(cluster_counts.values):
    plt.text(cluster_counts.index[i], v + 1, str(v), ha='center', va='bottom')

plt.tight_layout()
plt.show()

print(f"\n Cluster Distribution:")
cluster_counts = pd.Series(best_result['labels']).value_counts().sort_index()
for cluster_id, count in cluster_counts.items():
    percentage = count / len(best_result['labels']) * 100
    print(f"   Cluster {cluster_id}: {count:4d} developers ({percentage:5.1f}%)")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  DEVELOPER PROFILE ANALYSIS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("Analyzing developer profiles by cluster...")

profiles = {}
for cluster_id in sorted(df_behavior['developer_type'].unique()):
    cluster_data = df_behavior[df_behavior['developer_type'] == cluster_id]

    profile = {
        'count': len(cluster_data),
        'percentage': len(cluster_data) / len(df_behavior) * 100,
        'characteristics': {}
    }

    # Key characteristics
    key_features = [    'commits_per_day',
    'prs_per_day',
    'activity_score',
    'followers_to_following_ratio',
    'collaboration_score',
    'popularity_score',
    'avg_loc_per_pr',
    'avg_files_per_pr',
    'code_quality_score',
    'est_loc_per_commit',
    'commit_to_pr_ratio',
    'account_age_days',
    'total_repos',
    'avg_stars_repo',
    'followers_log',
    'stars_total_log',
    'total_commits_log',
    'pr_additions_log',
    'total_prs_log']

    for feature in key_features:
        if feature in cluster_data.columns:
            profile['characteristics'][feature] = {
                'mean': cluster_data[feature].mean(),
                'median': cluster_data[feature].median(),
                'std': cluster_data[feature].std()
            }

    profiles[f'Cluster_{cluster_id}'] = profile

# Create comparison dataframe
comparison_data = []
for cluster_name, profile in profiles.items():
    row = {'cluster': cluster_name, 'count': profile['count'], 'percentage': profile['percentage']}

    for feature, stats in profile['characteristics'].items():
        row[f'{feature}_mean'] = stats['mean']

    comparison_data.append(row)

comparison_df = pd.DataFrame(comparison_data)

# Display key metrics comparison
key_metrics = ['activity_score_mean', 'popularity_score_mean', 'collaboration_score_mean',
               'code_quality_score_mean', 'commits_per_day_mean']

print("\nğŸ“‹ Cluster Comparison (Key Metrics):")
display(comparison_df[['cluster', 'count', 'percentage'] +
                     [col for col in key_metrics if col in comparison_df.columns]].round(3))

# Visualize cluster characteristics
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Plot 1: Activity vs Popularity
ax1 = axes[0, 0]
for cluster_id in sorted(df_behavior['developer_type'].unique()):
    cluster_data = df_behavior[df_behavior['developer_type'] == cluster_id]
    ax1.scatter(cluster_data['activity_score'], cluster_data['popularity_score'],
               label=f'Cluster {cluster_id}', alpha=0.6, s=30)
ax1.set_xlabel('Activity Score')
ax1.set_ylabel('Popularity Score')
ax1.set_title('Activity vs Popularity by Cluster')
ax1.legend()

# Plot 2: Collaboration vs Code Quality
ax2 = axes[0, 1]
for cluster_id in sorted(df_behavior['developer_type'].unique()):
    cluster_data = df_behavior[df_behavior['developer_type'] == cluster_id]
    ax2.scatter(cluster_data['collaboration_score'], cluster_data['code_quality_score'],
               label=f'Cluster {cluster_id}', alpha=0.6, s=30)
ax2.set_xlabel('Collaboration Score')
ax2.set_ylabel('Code Quality Score')
ax2.set_title('Collaboration vs Code Quality by Cluster')
ax2.legend()

# Plot 3: Feature importance heatmap
ax3 = axes[1, 0]
heatmap_data = comparison_df.set_index('cluster')[key_metrics].fillna(0)
sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='viridis', ax=ax3)
ax3.set_title('Cluster Characteristics Heatmap')

# Plot 4: Cluster sizes
ax4 = axes[1, 1]
cluster_counts = comparison_df.set_index('cluster')['count']
ax4.pie(cluster_counts.values, labels=cluster_counts.index, autopct='%1.1f%%')
ax4.set_title('Developer Distribution by Cluster')

plt.tight_layout()
plt.show()

from sklearn.cluster import DBSCAN

print("\nDBSCAN Clustering Analysis...")

# Apply DBSCAN
dbscan = DBSCAN(eps=1.5, min_samples=5)  # You can tune eps/min_samples
dbscan_labels = dbscan.fit_predict(X_scaled)

# Count unique clusters
unique_labels = np.unique(dbscan_labels)
n_clusters_dbscan = len(unique_labels[unique_labels != -1])  # exclude noise (-1)

print(f"DBSCAN found {n_clusters_dbscan} clusters (excluding noise)")

# Check if more than one cluster is found for Silhouette Score
if len(set(dbscan_labels)) > 1 and -1 in dbscan_labels:
    score = silhouette_score(X_scaled, dbscan_labels)
    print(f"  Silhouette Score (excluding noise): {score:.3f}")
else:
    score = -1.0
    print("  Not enough clusters to compute Silhouette Score.")

# Add to clustering results
clustering_results['dbscan'] = {
    'labels': dbscan_labels,
    'model': dbscan,
    'silhouette': score,
    'method': 'DBSCAN',
    'n_clusters': n_clusters_dbscan
}

# Add cluster labels to DataFrame
df_csv['dbscan_cluster'] = dbscan_labels

# Visualization (t-SNE, PCA, Cluster Size)
if len(X_scaled) > 1:
    tsne_perplexity = min(30, len(X_scaled) - 1)
    if tsne_perplexity < 1:
        X_tsne = None
        print("\nNot enough data points for t-SNE visualization.")
    else:
        tsne = TSNE(n_components=2, random_state=42, perplexity=tsne_perplexity)
        X_tsne = tsne.fit_transform(X_scaled)

    pca = PCA(n_components=2, random_state=42)
    X_pca = pca.fit_transform(X_scaled)

    plt.figure(figsize=(15, 5))

    # t-SNE plot
    if X_tsne is not None:
        plt.subplot(1, 3, 1)
        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=dbscan_labels,
                              cmap='tab10', alpha=0.6, s=50)
        plt.colorbar(scatter)
        plt.title(f'DBSCAN Clusters\n(t-SNE Visualization)')
        plt.xlabel('t-SNE Component 1')
        plt.ylabel('t-SNE Component 2')
    else:
        plt.subplot(1, 3, 1)
        plt.text(0.5, 0.5, "t-SNE plot not available\n(Not enough data points)",
                 horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)
        plt.title('t-SNE Visualization')

    # PCA plot
    plt.subplot(1, 3, 2)
    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=dbscan_labels,
                          cmap='tab10', alpha=0.6, s=50)
    plt.colorbar(scatter)
    plt.title(f'DBSCAN Clusters\n(PCA Visualization)')
    plt.xlabel('First Principal Component')
    plt.ylabel('Second Principal Component')

    # Cluster distribution bar chart
    plt.subplot(1, 3, 3)
    cluster_counts = pd.Series(dbscan_labels).value_counts().sort_index()
    bars = plt.bar(cluster_counts.index.astype(str), cluster_counts.values, color='lightgreen', alpha=0.7)
    plt.title('DBSCAN Cluster Size Distribution')
    plt.xlabel('Cluster ID')
    plt.ylabel('Number of Developers')

    for i, bar in enumerate(bars):
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2, yval + 1, int(yval), ha='center', va='bottom')

    plt.tight_layout()
    plt.show()

    print("\nDBSCAN Cluster Distribution:")
    for cluster_id, count in cluster_counts.items():
        percentage = count / len(dbscan_labels) * 100
        print(f"  Cluster {cluster_id:>3}: {count:4d} developers ({percentage:5.1f}%)")
else:
    print("Not enough data points for DBSCAN visualizations.")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA # For visualizing high-dimensional data
from sklearn.datasets import make_blobs # <--- ADD THIS LINE

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Â 1. Data Generation (for a complete, runnable example)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("Generating sample data...")
# Create a more complex dataset with multiple clusters for better demonstration
n_samples = 500
n_features = 10 # Example: High-dimensional data
random_state = 42

# Generate some distinct clusters
X, y_true = make_blobs(n_samples=n_samples, centers=4, n_features=n_features,
                       cluster_std=1.0, random_state=random_state)

# Add some noise to make it more realistic for DBSCAN
X = np.vstack([X, np.random.rand(50, n_features) * 10]) # Adding 50 noise points
y_true = np.append(y_true, [-1]*50) # Label noise points as -1 (optional, for tracking)

# Scale the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Determine an 'optimal_k' for K-Means and Hierarchical.
# In a real scenario, this would come from an elbow method or silhouette analysis.
# For this example, let's assume a good k is 4 (from make_blobs).
# If K-Means was run for optimal k, that value would be used here.
optimal_k = 4
if len(np.unique(y_true[y_true != -1])) > 1: # If there are actual clusters
    optimal_k = len(np.unique(y_true[y_true != -1])) # Use the true number of clusters if known

clustering_results = {} # Dictionary to store results from all methods

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Â 2. K-Means Clustering (as a common baseline)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\nPerforming K-Means Clustering...")
# Suppress KMeans warning for n_init by explicitly setting it
kmeans = KMeans(n_clusters=optimal_k, random_state=random_state, n_init=10)
kmeans_clusters = kmeans.fit_predict(X_scaled)
kmeans_score = silhouette_score(X_scaled, kmeans_clusters)

clustering_results['kmeans'] = {
    'labels': kmeans_clusters,
    'model': kmeans,
    'silhouette': kmeans_score,
    'method': 'K-Means',
    'n_clusters': optimal_k
}
print(f"  K-Means: {optimal_k} clusters (silhouette: {kmeans_score:.3f})")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Â 3. DBSCAN for outlier detection (from your provided code)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n Testing alternative clustering algorithms...")
print("\n DBSCAN Analysis:")
eps_values = [0.3, 0.5, 0.7, 1.0, 1.2, 1.5, 2.0] # More eps values for better exploration
best_dbscan = None
best_dbscan_score = -2 # Initialize with a very low score

# Store DBSCAN results for plotting parameter exploration
dbscan_param_results = {'eps': [], 'n_clusters': [], 'n_noise': [], 'silhouette': []}

for eps in eps_values:
    # min_samples calculation ensures it's at least 1, and grows with dataset size.
    # A common rule of thumb for min_samples is 2 * number of dimensions.
    # Let's use max(5, 2 * n_features) or len(X)//50, whichever is higher, for robustness.
    min_samples_val = max(5, 2 * X.shape[1], len(X)//50)
    dbscan = DBSCAN(eps=eps, min_samples=min_samples_val)
    clusters = dbscan.fit_predict(X_scaled)

    n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)
    n_noise = list(clusters).count(-1)

    print(f"   eps={eps:.1f}: {n_clusters} clusters, {n_noise} noise points")

    silhouette = -1 # Default if no valid clustering for silhouette
    if n_clusters > 1 and n_noise < len(X) * 0.9: # Ensure at least 2 clusters and not almost all noise
        try:
            silhouette = silhouette_score(X_scaled, clusters)
            print(f"           Silhouette: {silhouette:.3f}")
        except ValueError: # Handle cases where silhouette_score might fail (e.g., all points in one cluster after noise removal)
            silhouette = -1
            print("           Silhouette: N/A (not enough distinct labels or all noise)")

    dbscan_param_results['eps'].append(eps)
    dbscan_param_results['n_clusters'].append(n_clusters)
    dbscan_param_results['n_noise'].append(n_noise)
    dbscan_param_results['silhouette'].append(silhouette)

    # Consider a good DBSCAN result as having multiple clusters and a reasonable silhouette score,
    # or if it's the only one producing clusters (even with noise), prioritize it.
    if n_clusters > 1 and silhouette > best_dbscan_score:
        best_dbscan_score = silhouette
        best_dbscan = {
            'labels': clusters,
            'model': dbscan,
            'silhouette': silhouette,
            'method': 'DBSCAN',
            'n_clusters': n_clusters,
            'eps': eps
        }

if best_dbscan:
    clustering_results['dbscan'] = best_dbscan
    print(f"\n Best DBSCAN: {best_dbscan['n_clusters']} clusters (silhouette: {best_dbscan_score:.3f}, eps: {best_dbscan['eps']:.1f})")
else:
    print("\n No valid DBSCAN clustering found with current parameters.")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Â 4. Hierarchical clustering (from your provided code)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if len(X) <= 1000:  # Only for smaller datasets
    print(f"\n Hierarchical Clustering:")
    hier = AgglomerativeClustering(n_clusters=optimal_k)
    hier_clusters = hier.fit_predict(X_scaled)
    hier_score = silhouette_score(X_scaled, hier_clusters)

    clustering_results['hierarchical'] = {
        'labels': hier_clusters,
        'model': hier,
        'silhouette': hier_score,
        'method': 'Hierarchical',
        'n_clusters': optimal_k
    }
    print(f"    Hierarchical: {optimal_k} clusters (silhouette: {hier_score:.3f})")
else:
    print("\n Hierarchical Clustering skipped for large dataset ( > 1000 samples).")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Â 5. Compare all methods
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print(f"\n Clustering Method Comparison:")
for name, result in clustering_results.items():
    print(f"   {result['method']:15} | Clusters: {result['n_clusters']:2d} | Silhouette: {result['silhouette']:.3f}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Â 6. Plotting and Visualization
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# --- Chart 1: Silhouette Score Comparison Bar Chart ---
print("\nGenerating Silhouette Score Comparison Chart...")
if clustering_results:
    chart_data = pd.DataFrame(
        [
            {
                'Method': result['method'],
                'Clusters': result['n_clusters'],
                'Silhouette Score': result['silhouette']
            }
            for result in clustering_results.values()
        ]
    )

    chart_data = chart_data.sort_values(by='Silhouette Score', ascending=False)

    plt.figure(figsize=(10, 6))
    sns.barplot(x='Method', y='Silhouette Score', data=chart_data, palette='viridis')
    plt.xlabel('Clustering Method')
    plt.ylabel('Silhouette Score')
    plt.title('Clustering Method Comparison by Silhouette Score')
    plt.ylim(-0.2, 1) # Silhouette score ranges from -1 to 1
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.savefig('clustering_silhouette_comparison.png')
    plt.show()
else:
    print("No clustering results to compare for silhouette score.")

# --- Chart 2: Cluster Visualization (using PCA for high-dimensional data) ---
print("\nGenerating Cluster Visualization...")

# Reduce dimensions for visualization if n_features > 2
if X_scaled.shape[1] > 2:
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)
    plot_data = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
    title_suffix = " (PCA Reduced)"
else:
    plot_data = pd.DataFrame(X_scaled, columns=[f'Feature {i+1}' for i in range(X_scaled.shape[1])])
    title_suffix = ""

if 'kmeans' in clustering_results:
    plot_data['KMeans_Cluster'] = clustering_results['kmeans']['labels']
    plt.figure(figsize=(10, 8))
    sns.scatterplot(x=plot_data.columns[0], y=plot_data.columns[1], hue='KMeans_Cluster',
                    palette='tab10', legend='full', data=plot_data, s=50, alpha=0.7)
    plt.title(f'K-Means Clustering Visualization{title_suffix}')
    plt.xlabel(plot_data.columns[0])
    plt.ylabel(plot_data.columns[1])
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.savefig('kmeans_clusters_visualization.png')
    plt.show()

if 'dbscan' in clustering_results:
    plot_data['DBSCAN_Cluster'] = clustering_results['dbscan']['labels']
    # Map -1 (noise) to a distinct color (e.g., black or grey)
    unique_dbscan_labels = np.unique(plot_data['DBSCAN_Cluster'])
    colors = sns.color_palette('tab10', n_colors=len(unique_dbscan_labels) - (1 if -1 in unique_dbscan_labels else 0))
    if -1 in unique_dbscan_labels:
        # Check if the list is empty before inserting, to prevent IndexError if all are noise
        if len(colors) > 0:
            colors.insert(0, 'black') # Assign black to noise points (-1)
        else: # Handle case where only noise points are present (all -1)
            colors = ['black']
            unique_dbscan_labels = [-1] # Ensure palette mapping is correct if only noise

    dbscan_palette = dict(zip(unique_dbscan_labels, colors))

    plt.figure(figsize=(10, 8))
    sns.scatterplot(x=plot_data.columns[0], y=plot_data.columns[1], hue='DBSCAN_Cluster',
                    palette=dbscan_palette, legend='full', data=plot_data, s=50, alpha=0.7)
    plt.title(f'DBSCAN Clustering Visualization{title_suffix} (Noise in Black)')
    plt.xlabel(plot_data.columns[0])
    plt.ylabel(plot_data.columns[1])
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.savefig('dbscan_clusters_visualization.png')
    plt.show()

if 'hierarchical' in clustering_results:
    plot_data['Hierarchical_Cluster'] = clustering_results['hierarchical']['labels']
    plt.figure(figsize=(10, 8))
    sns.scatterplot(x=plot_data.columns[0], y=plot_data.columns[1], hue='Hierarchical_Cluster',
                    palette='tab10', legend='full', data=plot_data, s=50, alpha=0.7)
    plt.title(f'Hierarchical Clustering Visualization{title_suffix}')
    plt.xlabel(plot_data.columns[0])
    plt.ylabel(plot_data.columns[1])
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.savefig('hierarchical_clusters_visualization.png')
    plt.show()

# --- Chart 3: DBSCAN Parameter Exploration Plot ---
print("\nGenerating DBSCAN Parameter Exploration Chart...")
if dbscan_param_results['eps']:
    fig, axes = plt.subplots(3, 1, figsize=(12, 15), sharex=True)

    axes[0].plot(dbscan_param_results['eps'], dbscan_param_results['n_clusters'], marker='o', linestyle='-')
    axes[0].set_ylabel('Number of Clusters')
    axes[0].set_title('DBSCAN Parameter Exploration')
    axes[0].grid(True, linestyle='--', alpha=0.6)

    axes[1].plot(dbscan_param_results['eps'], dbscan_param_results['n_noise'], marker='o', linestyle='-', color='red')
    axes[1].set_ylabel('Number of Noise Points')
    axes[1].grid(True, linestyle='--', alpha=0.6)

    axes[2].plot(dbscan_param_results['eps'], dbscan_param_results['silhouette'], marker='o', linestyle='-', color='green')
    axes[2].set_xlabel('Epsilon (eps) Value')
    axes[2].set_ylabel('Silhouette Score')
    axes[2].grid(True, linestyle='--', alpha=0.6)
    # Only add best_dbscan_score line if best_dbscan exists and has a valid silhouette
    if best_dbscan and best_dbscan_score > -1:
        axes[2].axhline(y=best_dbscan_score, color='purple', linestyle=':', label=f'Best Silhouette ({best_dbscan_score:.2f})')
        axes[2].legend()

    plt.tight_layout()
    plt.savefig('dbscan_parameter_exploration.png')
    plt.show()
else:
    print("No DBSCAN parameter exploration data to plot.")

print("\nCharts and graphs generated successfully!")

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import numpy as np

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

tsne_perplexity = 30 # Use your chosen perplexity value
tsne = TSNE(n_components=2, random_state=42, perplexity=tsne_perplexity)
X_tsne = tsne.fit_transform(X_scaled)

# --- Gaussian Mixture Models (GMM) for Overlapping Clusters ---

print("Running Gaussian Mixture Models (GMM)...")
print("WHY GMM: Handles overlapping developer types (e.g., Star Contributors who are also Team Players)")

gmm_results = {}
n_components_range = range(2, 8)
gmm_scores = []

for n_comp in n_components_range:
    gmm = GaussianMixture(n_components=n_comp, random_state=42, covariance_type='full')
    gmm_labels = gmm.fit_predict(X_scaled)

    aic = gmm.aic(X_scaled)
    bic = gmm.bic(X_scaled)
    silhouette = silhouette_score(X_scaled, gmm_labels)

    gmm_results[f'gmm_{n_comp}'] = {
        'labels': gmm_labels,
        'model': gmm,
        'n_components': n_comp,
        'aic': aic,
        'bic': bic,
        'silhouette': silhouette,
        'method': 'GMM'
    }

    gmm_scores.append({'n_comp': n_comp, 'aic': aic, 'bic': bic, 'silhouette': silhouette})
    print(f"  Components: {n_comp} | AIC: {aic:.1f} | BIC: {bic:.1f} | Silhouette: {silhouette:.3f}")

optimal_gmm = min(gmm_scores, key=lambda x: x['bic'])
best_gmm = gmm_results[f"gmm_{optimal_gmm['n_comp']}"]

print(f"\nBest GMM: {optimal_gmm['n_comp']} components")
print(f"  BIC: {optimal_gmm['bic']:.1f} | Silhouette: {optimal_gmm['silhouette']:.3f}")

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
n_comps = [score['n_comp'] for score in gmm_scores]
aics = [score['aic'] for score in gmm_scores]
bics = [score['bic'] for score in gmm_scores]

plt.plot(n_comps, aics, 'b-o', label='AIC')
plt.plot(n_comps, bics, 'r-o', label='BIC')
plt.xlabel('Number of Components')
plt.ylabel('Information Criterion')
plt.title('GMM: Model Selection')
plt.legend()
plt.axvline(x=optimal_gmm['n_comp'], color='green', linestyle='--', alpha=0.7)

plt.subplot(1, 3, 2)
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=best_gmm['labels'],
                      cmap='viridis', alpha=0.6, s=50)
plt.colorbar(scatter)
plt.title(f'GMM Clusters (n={best_gmm["n_components"]})\n(t-SNE Visualization)')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')

plt.subplot(1, 3, 3)
probabilities = best_gmm['model'].predict_proba(X_scaled)
max_probs = np.max(probabilities, axis=1)
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=max_probs,
                      cmap='plasma', alpha=0.6, s=50)
plt.colorbar(scatter, label='Max Probability')
plt.title('GMM: Cluster Certainty\n(Higher = More Certain)')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')

plt.tight_layout()
plt.show()

# clustering_results['gmm'] = best_gmm

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Load the CSV file
df_csv = pd.read_csv('/content/drive/MyDrive/Dataset/github_users_final.csv')

# Define the specific 19 fields to be used for clustering
specified_numerical_cols_raw = [
    'commits_per_day',
    'prs_per_day',
    'activity_score',
    'followers_to_following_ratio',
    'collaboration_score',
    'popularity_score',
    'avg_loc_per_pr',
    'avg_files_per_pr',
    'code_quality_score',
    'est_loc_per_commit',
    'commit_to_pr_ratio',
    'account_age_days',
    'total_repos',
    'avg_stars_repo',
    'followers_log',
    'stars_total_log',
    'total_commits_log',
    'pr_additions_log',
    'total_prs_log'
]

actual_numerical_cols = []
missing_cols = []
available_cols_in_df = df_csv.columns.tolist()

# Iterate through the user's requested columns
for col in specified_numerical_cols_raw:
    if col in available_cols_in_df:
        actual_numerical_cols.append(col)
    elif col == 'popularity_score' and 'popularity_score_total' in available_cols_in_df:
        actual_numerical_cols.append('popularity_score_total')
    else:
        missing_cols.append(col)

# Handle missing values
for col in actual_numerical_cols:
    df_csv[col] = df_csv[col].fillna(0)

# Extract features
X = df_csv[actual_numerical_cols].copy()

# Scale the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Gaussian Mixture Model (GMM) Clustering Analysis
clustering_results = {}
silhouette_scores = []

# Define k range
max_k_possible = len(X) - 1
k_range_end = min(11, max_k_possible)

if k_range_end < 2:
    print("Not enough data to perform clustering with multiple clusters (minimum 2 samples required).")
else:
    k_range = range(2, k_range_end + 1)
    for k in k_range:
        gmm = GaussianMixture(n_components=k, random_state=42, n_init=10)
        clusters = gmm.fit_predict(X_scaled)

        if len(np.unique(clusters)) < 2:
            score = -1.0
        else:
            score = silhouette_score(X_scaled, clusters)

        silhouette_scores.append(score)
        clustering_results[f'gmm_{k}'] = {
            'labels': clusters,
            'model': gmm,
            'silhouette': score,
            'method': 'Gaussian Mixture',
            'n_clusters': k
        }

    valid_silhouette_scores = [s for s in silhouette_scores if s != -1.0]
    valid_k_range = [k for k, s in zip(k_range, silhouette_scores) if s != -1.0]

    if valid_silhouette_scores:
        optimal_k_index = np.argmax(valid_silhouette_scores)
        optimal_k = valid_k_range[optimal_k_index]

        plt.figure(figsize=(12, 4))

        plt.subplot(1, 2, 1)
        plt.plot(valid_k_range, valid_silhouette_scores, 'bo-')
        plt.xlabel('Number of Components (k)')
        plt.ylabel('Silhouette Score')
        plt.title('GMM: Silhouette Score vs Number of Components')
        plt.axvline(x=optimal_k, color='red', linestyle='--', alpha=0.7)

        log_likelihoods = []
        for k in k_range:
            if k <= len(X):
                gmm = GaussianMixture(n_components=k, random_state=42, n_init=10)
                gmm.fit(X_scaled)
                log_likelihoods.append(gmm.score(X_scaled) * len(X_scaled))
            else:
                log_likelihoods.append(np.nan)

        plt.subplot(1, 2, 2)
        log_likelihoods_filtered = [ll for ll in log_likelihoods if not np.isnan(ll)]
        k_range_ll_filtered = [k for k, ll in zip(k_range, log_likelihoods) if not np.isnan(ll)]
        plt.plot(k_range_ll_filtered, log_likelihoods_filtered, 'ro-')
        plt.xlabel('Number of Components (k)')
        plt.ylabel('Log-Likelihood')
        plt.title('GMM: Log-Likelihood vs Number of Components')
        plt.axvline(x=optimal_k, color='red', linestyle='--', alpha=0.7)

        plt.tight_layout()
        plt.show()

        # Best Model Selection and Visualization
        best_model = max(clustering_results.items(), key=lambda x: x[1]['silhouette'] if x[1]['silhouette'] != -1.0 else -float('inf'))
        best_name, best_result = best_model

        df_csv['developer_type'] = best_result['labels']

        if len(X_scaled) > 1:
            tsne_perplexity = min(30, len(X_scaled) - 1)
            if tsne_perplexity < 1:
                X_tsne = None
            else:
                tsne = TSNE(n_components=2, random_state=42, perplexity=tsne_perplexity)
                X_tsne = tsne.fit_transform(X_scaled)

            pca = PCA(n_components=2, random_state=42)
            X_pca = pca.fit_transform(X_scaled)

            plt.figure(figsize=(15, 5))

            if X_tsne is not None:
                plt.subplot(1, 3, 1)
                scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=best_result['labels'],
                                    cmap='viridis', alpha=0.6, s=50)
                plt.colorbar(scatter)
                plt.title(f'{best_result["method"]} Clusters\n(t-SNE Visualization)')
                plt.xlabel('t-SNE Component 1')
                plt.ylabel('t-SNE Component 2')
            else:
                plt.subplot(1, 3, 1)
                plt.text(0.5, 0.5, "t-SNE plot not available\n(Not enough data points)",
                         horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)
                plt.title('t-SNE Visualization')

            plt.subplot(1, 3, 2)
            scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=best_result['labels'],
                                cmap='viridis', alpha=0.6, s=50)
            plt.colorbar(scatter)
            plt.title(f'{best_result["method"]} Clusters\n(PCA Visualization)')
            plt.xlabel('First Principal Component')
            plt.ylabel('Second Principal Component')

            plt.subplot(1, 3, 3)
            cluster_counts = pd.Series(best_result['labels']).value_counts().sort_index()
            plt.bar(cluster_counts.index, cluster_counts.values, color='skyblue', alpha=0.7)
            plt.title('Cluster Size Distribution')
            plt.xlabel('Cluster ID')
            plt.ylabel('Number of Developers')

            for i, v in enumerate(cluster_counts.values):
                plt.text(cluster_counts.index[i], v + 1, str(v), ha='center', va='bottom')

            plt.tight_layout()
            plt.show()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SPECTRAL CLUSTERING FOR NON-CONVEX CLUSTERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from sklearn.cluster import SpectralClustering

print(" Running Spectral Clustering...")
# Finds complex, non-spherical developer behavior patterns")

# Test different numbers of clusters
spectral_results = {}
spectral_scores = []

# Only test if dataset is not too large (Spectral is computationally expensive)
if len(X_scaled) <= 2000:
    for n_clusters in range(2, 7):
        spectral = SpectralClustering(
            n_clusters=n_clusters,
            random_state=42,
            affinity='rbf',  # RBF kernel for non-linear patterns
            gamma=1.0
        )

        spectral_labels = spectral.fit_predict(X_scaled)
        silhouette = silhouette_score(X_scaled, spectral_labels)

        spectral_results[f'spectral_{n_clusters}'] = {
            'labels': spectral_labels,
            'model': spectral,
            'n_clusters': n_clusters,
            'silhouette': silhouette,
            'method': 'Spectral'
        }

        spectral_scores.append({'n_clusters': n_clusters, 'silhouette': silhouette})
        print(f"   Clusters: {n_clusters} | Silhouette: {silhouette:.3f}")

    # Find best spectral clustering
    best_spectral_config = max(spectral_scores, key=lambda x: x['silhouette'])
    best_spectral = spectral_results[f"spectral_{best_spectral_config['n_clusters']}"]

    print(f"\n Best Spectral: {best_spectral_config['n_clusters']} clusters")
    print(f"   Silhouette: {best_spectral_config['silhouette']:.3f}")

    # Add to clustering results
    clustering_results['spectral'] = best_spectral

    # Visualize
    plt.figure(figsize=(10, 4))

    plt.subplot(1, 2, 1)
    silhouettes = [score['silhouette'] for score in spectral_scores]
    n_clusters_list = [score['n_clusters'] for score in spectral_scores]
    plt.plot(n_clusters_list, silhouettes, 'go-')
    plt.xlabel('Number of Clusters')
    plt.ylabel('Silhouette Score')
    plt.title('Spectral Clustering: Performance')
    plt.axvline(x=best_spectral_config['n_clusters'], color='red', linestyle='--', alpha=0.7)

    plt.subplot(1, 2, 2)
    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=best_spectral['labels'],
                         cmap='viridis', alpha=0.6, s=50)
    plt.colorbar(scatter)
    plt.title(f'Spectral Clusters (n={best_spectral["n_clusters"]})')
    plt.xlabel('t-SNE Component 1')
    plt.ylabel('t-SNE Component 2')

    plt.tight_layout()
    plt.show()

    print(f" Spectral clustering complete!")

else:
    print(f"Dataset too large ({len(X_scaled)} samples) for Spectral Clustering")
    print(f" Spectral works best with <2000 samples due to computational complexity")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#CELL #5C: BIRCH CLUSTERING FOR LARGE DATASETS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from sklearn.cluster import Birch

# print(" Running BIRCH Clustering...")
# print(" WHY BIRCH: Memory-efficient for large developer datasets, handles noise well")

# Test different numbers of clusters
birch_results = {}
birch_scores = []

for n_clusters in range(2, 8):
    birch = Birch(
        n_clusters=n_clusters,
        threshold=0.5,  # Distance threshold for new subclusters
        branching_factor=50  # Max subclusters in each node
    )

    birch_labels = birch.fit_predict(X_scaled)
    silhouette = silhouette_score(X_scaled, birch_labels)

    birch_results[f'birch_{n_clusters}'] = {
        'labels': birch_labels,
        'model': birch,
        'n_clusters': n_clusters,
        'silhouette': silhouette,
        'method': 'BIRCH'
    }

    birch_scores.append({'n_clusters': n_clusters, 'silhouette': silhouette})
    print(f"   Clusters: {n_clusters} | Silhouette: {silhouette:.3f}")

# Find best BIRCH clustering
best_birch_config = max(birch_scores, key=lambda x: x['silhouette'])
best_birch = birch_results[f"birch_{best_birch_config['n_clusters']}"]

print(f"\ Best BIRCH: {best_birch_config['n_clusters']} clusters")
print(f"   Silhouette: {best_birch_config['silhouette']:.3f}")

# Add to clustering results
clustering_results['birch'] = best_birch

# Visualize BIRCH results
plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
silhouettes = [score['silhouette'] for score in birch_scores]
n_clusters_list = [score['n_clusters'] for score in birch_scores]
plt.plot(n_clusters_list, silhouettes, 'mo-')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.title('BIRCH Clustering: Performance')
plt.axvline(x=best_birch_config['n_clusters'], color='red', linestyle='--', alpha=0.7)

plt.subplot(1, 2, 2)
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=best_birch['labels'],
                     cmap='viridis', alpha=0.6, s=50)
plt.colorbar(scatter)
plt.title(f'BIRCH Clusters (n={best_birch["n_clusters"]})')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')

plt.tight_layout()
plt.show()

# print(f"âœ… BIRCH clustering complete!")
# print(f"ğŸ’¡ BIRCH Advantage: Fast, memory-efficient, good for incremental learning")

import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
import matplotlib.pyplot as plt

def align_data(X_scaled, X_tsne, df_behavior):
    min_length = min(len(X_scaled), len(X_tsne), len(df_behavior))

    # Truncate to smallest length
    X_scaled = X_scaled[:min_length]
    X_tsne = X_tsne[:min_length]
    df_behavior = df_behavior.iloc[:min_length].copy()

    # Verify alignment
    if not (len(X_scaled) == len(X_tsne) == len(df_behavior)):
        raise ValueError(f"Data alignment failed: X_scaled={len(X_scaled)}, "
                        f"X_tsne={len(X_tsne)}, df_behavior={len(df_behavior)}")

    return X_scaled, X_tsne, df_behavior

def run_anomaly_detection(X_scaled, X_tsne, df_behavior):
    # Align input data
    try:
        X_scaled, X_tsne, df_behavior = align_data(X_scaled, X_tsne, df_behavior)
    except ValueError as e:
        print(f"Error: {e}")
        return None

    # Isolation Forest
    isolation_forest = IsolationForest(
        contamination=0.1,  # Expect 10% anomalies
        random_state=42,
        n_estimators=100,
        n_jobs=-1
    )

    # Fit and predict anomalies
    anomaly_labels = isolation_forest.fit_predict(X_scaled)
    anomaly_scores = isolation_forest.decision_function(X_scaled)

    # Convert to binary (1 = normal, -1 = anomaly)
    normal_developers = (anomaly_labels == 1)
    anomalous_developers = (anomaly_labels == -1)

    # Local Outlier Factor
    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1, n_jobs=-1)
    lof_labels = lof.fit_predict(X_scaled)
    lof_scores = lof.negative_outlier_factor_

    # Create a copy of df_behavior
    df_result = df_behavior.copy()

    # Add anomaly information
    df_result['is_anomaly_isolation'] = (anomaly_labels == -1)
    df_result['anomaly_score'] = anomaly_scores
    df_result['is_anomaly_lof'] = (lof_labels == -1)
    df_result['lof_score'] = lof_scores

    # Print summary
    print("Anomaly Detection Results:")
    print(f"  Total developers: {len(df_result)}")
    print(f"  Normal developers: {normal_developers.sum()} "
          f"({normal_developers.sum()/len(anomaly_labels)*100:.1f}%)")
    print(f"  Anomalous developers: {anomalous_developers.sum()} "
          f"({anomalous_developers.sum()/len(anomaly_labels)*100:.1f}%)")

    # Visualize results
    plt.figure(figsize=(18, 5))

    # Plot 1: Isolation Forest t-SNE
    plt.subplot(1, 3, 1)
    plt.scatter(X_tsne[normal_developers, 0], X_tsne[normal_developers, 1],
                c='blue', alpha=0.6, s=50, label='Normal')
    plt.scatter(X_tsne[anomalous_developers, 0], X_tsne[anomalous_developers, 1],
               c='red', alpha=0.8, s=100, label='Anomaly', marker='x')
    plt.title('Isolation Forest: Anomaly Detection')
    plt.xlabel('t-SNE Component 1')
    plt.ylabel('t-SNE Component 2')
    plt.legend()

    # Plot 2: Anomaly scores distribution
    plt.subplot(1, 3, 2)
    plt.hist(anomaly_scores[normal_developers], bins=30, alpha=0.7,
             label='Normal', color='blue', density=True)
    plt.hist(anomaly_scores[anomalous_developers], bins=30, alpha=0.7,
             label='Anomaly', color='red', density=True)
    plt.xlabel('Anomaly Score')
    plt.ylabel('Density')
    plt.title('Anomaly Score Distribution')
    plt.legend()

    # Plot 3: Top anomalous developers
    plt.subplot(1, 3, 3)
    anomaly_indices = np.where(anomalous_developers)[0]
    top_anomalies = anomaly_indices[np.argsort(anomaly_scores[anomalous_developers])[:5]]

    activity_scores = df_result.iloc[top_anomalies]['activity_score'].values
    popularity_scores = df_result.iloc[top_anomalies]['popularity_score'].values

    plt.scatter(df_result['activity_score'], df_result['popularity_score'],
               c='blue', alpha=0.3, s=20, label='All Developers')
    plt.scatter(activity_scores, popularity_scores,
               c='red', s=100, alpha=0.8, label='Top Anomalies')
    plt.xlabel('Activity Score')
    plt.ylabel('Popularity Score')
    plt.title('Top 5 Anomalous Developers')
    plt.legend()

    for i, idx in enumerate(top_anomalies):
        plt.annotate(f'#{i+1}', (activity_scores[i], popularity_scores[i]),
                    xytext=(5, 5), textcoords='offset points')

    plt.tight_layout()
    plt.show()

    # Show top anomalous developers
    print("\nTop 5 Most Anomalous Developers:")
    for i, idx in enumerate(top_anomalies):
        dev_data = df_result.iloc[idx]
        print(f"  {i+1}. {dev_data['login']}")
        print(f"     Activity: {dev_data['activity_score']:.3f} | "
              f"Popularity: {dev_data['popularity_score']:.3f}")
        print(f"     Commits/day: {dev_data['commits_per_day']:.3f} | "
              f"Followers: {dev_data['followers']:.0f}")
        print(f"     Anomaly Score: {dev_data['anomaly_score']:.3f}")

    return df_result

# Example usage:
df_result = run_anomaly_detection(X_scaled, X_tsne, df_behavior)

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report

def align_data(X_scaled, df_behavior, behavior_features, y_labels=None):
    """Align all input data to have consistent sample counts and features."""
    # Ensure all features exist in df_behavior
    missing_features = [f for f in behavior_features if f not in df_behavior.columns]
    if missing_features:
        raise ValueError(f"Features not found in df_behavior: {missing_features}")

    # Determine minimum sample size
    sample_sizes = [len(X_scaled), len(df_behavior)]
    if y_labels is not None:
        sample_sizes.append(len(y_labels))
    min_samples = min(sample_sizes)

    # Truncate to smallest sample size
    X_scaled = X_scaled[:min_samples]
    df_behavior = df_behavior.iloc[:min_samples].copy()
    if y_labels is not None:
        y_labels = y_labels[:min_samples]

    # Create X_scaled from df_behavior to ensure feature alignment
    X_scaled_new = df_behavior[behavior_features].values

    # Validate shapes
    if X_scaled_new.shape[1] != len(behavior_features):
        raise ValueError(f"X_scaled has {X_scaled_new.shape[1]} features, expected {len(behavior_features)}")
    if X_scaled_new.shape[0] != len(df_behavior):
        raise ValueError(f"X_scaled has {X_scaled_new.shape[0]} samples, expected {len(df_behavior)}")
    if y_labels is not None and len(y_labels) != len(df_behavior):
        raise ValueError(f"Label length ({len(y_labels)}) does not match samples ({len(df_behavior)})")

    return X_scaled_new, df_behavior, y_labels

def run_random_forest(X_scaled, df_behavior, behavior_features, clustering_results=None):
    print("RUNNING RANDOM FOREST FEATURE IMPORTANCE ANALYSIS")
    print("=" * 60)

    # Create or validate labels
    if clustering_results:
        best_method = max(clustering_results.keys(), key=lambda k: clustering_results[k]['silhouette'])
        y_labels = clustering_results[best_method]['labels']
        print(f"Using {clustering_results[best_method]['method']} labels for Random Forest training")
        print(f"  Labels shape: {len(y_labels)} samples, {len(set(y_labels))} classes")
    else:
        if 'activity_score' in df_behavior.columns:
            median_activity = df_behavior['activity_score'].median()
            y_labels = (df_behavior['activity_score'] > median_activity).astype(int)
            print(f"Created binary labels based on activity score (median: {median_activity:.3f})")
        else:
            print("No clustering results or activity score available")
            print("  Creating random labels for demonstration...")
            y_labels = np.random.randint(0, 2, size=len(X_scaled))

    # Align data
    try:
        X_scaled, df_behavior, y_labels = align_data(X_scaled, df_behavior, behavior_features, y_labels)
    except ValueError as e:
        print(f"Error: {e}")
        return None

    # Train Random Forest
    print(f"\nTraining Random Forest...")
    print(f"  Features: {X_scaled.shape[1]}")
    print(f"  Samples: {X_scaled.shape[0]}")
    print(f"  Classes: {len(set(y_labels))}")

    rf_classifier = RandomForestClassifier(
        n_estimators=100,
        random_state=42,
        max_depth=10,
        min_samples_split=5,
        n_jobs=-1
    )

    rf_classifier.fit(X_scaled, y_labels)

    # Get feature importance
    feature_importance = pd.DataFrame({
        'feature': behavior_features,
        'importance': rf_classifier.feature_importances_
    }).sort_values('importance', ascending=False)

    print(f"\nFEATURE IMPORTANCE RESULTS:")
    print(f"Top 5 most important features:")
    for i, row in feature_importance.head().iterrows():
        print(f"  {i+1}. {row['feature']}: {row['importance']:.4f}")

    # Calculate model performance
    scores = cross_val_score(rf_classifier, X_scaled, y_labels, cv=5, scoring='accuracy')
    print(f"\nMODEL PERFORMANCE:")
    print(f"  Cross-validation accuracy: {scores.mean():.3f} (Â±{scores.std()*2:.3f})")

    # Generate classification report
    y_pred = rf_classifier.predict(X_scaled)
    print(f"\nCLASSIFICATION REPORT:")
    print(classification_report(y_labels, y_pred))

    # Store results
    ml_results = {
        'random_forest': {
            'model': rf_classifier,
            'feature_importance': feature_importance,
            'cv_scores': scores,
            'feature_names': behavior_features,
            'labels_used': y_labels,
            'classification_report': classification_report(y_labels, y_pred, output_dict=True)
        }
    }

    print(f"\nRandom Forest analysis complete!")
    print(f"Results stored in ml_results['random_forest']")

    return ml_results

# Define the 19 features
behavior_features = [
    'commits_per_day',
    'prs_per_day',
    'activity_score',
    'followers_to_following_ratio',
    'collaboration_score',
    'popularity_score',
    'avg_loc_per_pr',
    'avg_files_per_pr',
    'code_quality_score',
    'est_loc_per_commit',
    'commit_to_pr_ratio',
    'account_age_days',
    'total_repos',
    'avg_stars_repo',
    'followers_log',
    'stars_total_log',
    'total_commits_log',
    'pr_additions_log',
    'total_prs_log'
]

# Example usage:
ml_results = run_random_forest(X_scaled, df_behavior, behavior_features, clustering_results)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.cluster import KMeans

def align_data(X_scaled, df_behavior, behavior_features, target_labels):
    """Align all input data to have consistent sample counts and features."""
    # Ensure all features exist in df_behavior
    missing_features = [f for f in behavior_features if f not in df_behavior.columns]
    if missing_features:
        raise ValueError(f"Features not found in df_behavior: {missing_features}")

    # Determine minimum sample size
    min_samples = min(len(X_scaled), len(df_behavior), len(target_labels))

    # Truncate to smallest sample size
    X_scaled = X_scaled[:min_samples]
    df_behavior = df_behavior.iloc[:min_samples].copy()
    target_labels = target_labels[:min_samples]

    # Create X_scaled from df_behavior to ensure feature alignment
    X_scaled_new = df_behavior[behavior_features].values

    # Validate shapes
    if X_scaled_new.shape[1] != len(behavior_features):
        raise ValueError(f"X_scaled has {X_scaled_new.shape[1]} features, expected {len(behavior_features)}")
    if X_scaled_new.shape[0] != len(df_behavior):
        raise ValueError(f"X_scaled has {X_scaled_new.shape[0]} samples, expected {len(df_behavior)}")
    if len(target_labels) != len(df_behavior):
        raise ValueError(f"Label length ({len(target_labels)}) does not match samples ({len(df_behavior)})")

    return X_scaled_new, df_behavior, target_labels

def run_random_forest(X_scaled, df_behavior, behavior_features, best_result=None):
    print("Running Random Forest Feature Importance Analysis...")
    print("WHY Random Forest: Understand which features matter most for developer classification")
    print("=" * 60)

    # Create or validate labels
    if best_result:
        target_labels = best_result['labels']
    else:
        print("No best_result provided, using K-means with 4 clusters")
        kmeans = KMeans(n_clusters=4, random_state=42)
        target_labels = kmeans.fit_predict(X_scaled)

    # Align data
    try:
        X_scaled, df_behavior, target_labels = align_data(X_scaled, df_behavior, behavior_features, target_labels)
    except ValueError as e:
        print(f"Error: {e}")
        return None

    # Train Random Forest
    print(f"\nTraining Random Forest...")
    print(f"  Features: {X_scaled.shape[1]}")
    print(f"  Samples: {X_scaled.shape[0]}")
    print(f"  Classes: {len(set(target_labels))}")

    rf_classifier = RandomForestClassifier(
        n_estimators=100,
        random_state=42,
        max_depth=10,
        min_samples_split=5,
        n_jobs=-1
    )

    rf_classifier.fit(X_scaled, target_labels)

    # Get feature importance
    feature_importance = pd.DataFrame({
        'feature': behavior_features,
        'importance': rf_classifier.feature_importances_
    }).sort_values('importance', ascending=False)

    # Cross-validation score
    cv_scores = cross_val_score(rf_classifier, X_scaled, target_labels, cv=5, scoring='accuracy')
    print(f"\nRandom Forest Performance:")
    print(f"  Cross-validation accuracy: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")

    # Visualize feature importance
    plt.figure(figsize=(15, 10))

    # Plot 1: Feature importance
    plt.subplot(2, 2, 1)
    top_features = feature_importance.head(10)
    plt.barh(range(len(top_features)), top_features['importance'], color='skyblue')
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Feature Importance')
    plt.title('Top 10 Most Important Features\n(Random Forest)')
    plt.gca().invert_yaxis()

    # Plot 2: Cumulative importance
    plt.subplot(2, 2, 2)
    cumulative_importance = feature_importance['importance'].cumsum()
    plt.plot(range(1, len(cumulative_importance) + 1), cumulative_importance, 'b-o')
    plt.xlabel('Number of Features')
    plt.ylabel('Cumulative Importance')
    plt.title('Cumulative Feature Importance')
    plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='80% threshold')
    plt.axhline(y=0.9, color='orange', linestyle='--', alpha=0.7, label='90% threshold')
    plt.legend()

    # Plot 3: Feature importance by category
    plt.subplot(2, 2, 3)
    feature_categories = {
        'Activity': ['commits_per_day', 'prs_per_day', 'activity_score'],
        'Social': ['followers_to_following_ratio', 'collaboration_score', 'popularity_score'],
        'Code Quality': ['avg_loc_per_pr', 'avg_files_per_pr', 'code_quality_score'],
        'Experience': ['account_age_days', 'total_repos', 'avg_stars_repo'],
        'Log Features': [f for f in behavior_features if f.endswith('_log')]
    }

    category_importance = {}
    for category, features in feature_categories.items():
        category_features = [f for f in features if f in feature_importance['feature'].values]
        if category_features:
            importance_sum = feature_importance[feature_importance['feature'].isin(category_features)]['importance'].sum()
            category_importance[category] = importance_sum

    if category_importance:
        categories = list(category_importance.keys())
        importances = list(category_importance.values())
        plt.pie(importances, labels=categories, autopct='%1.1f%%', startangle=90)
        plt.title('Feature Importance by Category')

    # Plot 4: Prediction confidence
    plt.subplot(2, 2, 4)
    prediction_proba = rf_classifier.predict_proba(X_scaled)
    max_proba = np.max(prediction_proba, axis=1)
    plt.hist(max_proba, bins=30, alpha=0.7, color='green')
    plt.xlabel('Prediction Confidence')
    plt.ylabel('Frequency')
    plt.title('Random Forest Prediction Confidence')
    plt.axvline(x=max_proba.mean(), color='red', linestyle='--',
                label=f'Mean: {max_proba.mean():.3f}')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # Display feature importance table
    print(f"\nFeature Importance Rankings:")
    print(feature_importance.head(15))

    print(f"\nKey Insights:")
    print(f"  Most important feature: {feature_importance.iloc[0]['feature']} ({feature_importance.iloc[0]['importance']:.3f})")
    print(f"  Top 5 features explain {feature_importance.head(5)['importance'].sum():.1%} of variance")
    print(f"  Average prediction confidence: {max_proba.mean():.3f}")

    # Store results
    ml_results = {
        'random_forest': {
            'model': rf_classifier,
            'feature_importance': feature_importance,
            'cv_scores': cv_scores,
            'prediction_confidence': max_proba
        }
    }

    print(f"\nRandom Forest analysis complete!")
    print(f"Use this to focus on most important features for portfolio generation")

    return ml_results

# Define the 19 features
behavior_features = [
    'commits_per_day',
    'prs_per_day',
    'activity_score',
    'followers_to_following_ratio',
    'collaboration_score',
    'popularity_score',
    'avg_loc_per_pr',
    'avg_files_per_pr',
    'code_quality_score',
    'est_loc_per_commit',
    'commit_to_pr_ratio',
    'account_age_days',
    'total_repos',
    'avg_stars_repo',
    'followers_log',
    'stars_total_log',
    'total_commits_log',
    'pr_additions_log',
    'total_prs_log'
]

# Example usage:
ml_results = run_random_forest(X_scaled, df_behavior, behavior_features, best_result)



# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODEL PERFORMANCE COMPARISON
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

print(" Comparing Clustering Model Performance...")

performance_metrics = []

# Iterate through all clustering results stored
for name, result in clustering_results.items():
    model_name = result.get('method', name)
    n_clusters = result.get('n_clusters', 'N/A')
    labels = result.get('labels')

    if labels is not None and len(set(labels)) > 1 and len(set(labels)) < len(X_scaled):
        try:
            silhouette = silhouette_score(X_scaled, labels)
        except Exception as e:
            silhouette = None
            print(f" Could not calculate Silhouette for {model_name}: {e}")

        try:
            calinski_harabasz = calinski_harabasz_score(X_scaled, labels)
        except Exception as e:
             calinski_harabasz = None
             print(f" Could not calculate Calinski-Harabasz for {model_name}: {e}")

        try:
            davies_bouldin = davies_bouldin_score(X_scaled, labels)
        except Exception as e:
            davies_bouldin = None
            print(f" Could not calculate Davies-Bouldin for {model_name}: {e}")

        performance_metrics.append({
            'Model': model_name,
            'N_Clusters': n_clusters,
            'Silhouette Score': silhouette,
            'Calinski-Harabasz Score': calinski_harabasz,
            'Davies-Bouldin Index': davies_bouldin
        })
    else:
         print(f" Skipping {model_name} due to insufficient clusters or all points in one cluster/noise.")


performance_df = pd.DataFrame(performance_metrics)

print("\nğŸ“‹ Clustering Model Performance Table:")
display(performance_df.sort_values('Silhouette Score', ascending=False))

# Visualize performance metrics
plt.figure(figsize=(18, 6))

# Plot Silhouette Score
plt.subplot(1, 3, 1)
sns.barplot(x='Model', y='Silhouette Score', data=performance_df.sort_values('Silhouette Score', ascending=False))
plt.title('Clustering Model Performance: Silhouette Score')
plt.ylabel('Silhouette Score')
plt.xticks(rotation=45, ha='right')

# Plot Calinski-Harabasz Score
plt.subplot(1, 3, 2)
sns.barplot(x='Model', y='Calinski-Harabasz Score', data=performance_df.sort_values('Calinski-Harabasz Score', ascending=False))
plt.title('Clustering Model Performance: Calinski-Harabasz Score')
plt.ylabel('Calinski-Harabasz Score')
plt.xticks(rotation=45, ha='right')

# Plot Davies-Bouldin Index
plt.subplot(1, 3, 3)
sns.barplot(x='Model', y='Davies-Bouldin Index', data=performance_df.sort_values('Davies-Bouldin Index', ascending=True)) # Lower is better
plt.title('Clustering Model Performance: Davies-Bouldin Index')
plt.ylabel('Davies-Bouldin Index')
plt.xticks(rotation=45, ha='right')

plt.tight_layout()
plt.show()

print(f"\n Clustering Model Performance Comparison complete!")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODEL PERFORMANCE COMPARISON
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

print(" Comparing Clustering Model Performance...")

performance_metrics = []

# Iterate through all clustering results stored
for name, result in clustering_results.items():
    model_name = result.get('method', name)
    n_clusters = result.get('n_clusters', 'N/A')
    labels = result.get('labels')

    if labels is not None and len(set(labels)) > 1 and len(set(labels)) < len(X_scaled):
        try:
            silhouette = silhouette_score(X_scaled, labels)
        except Exception as e:
            silhouette = None
            print(f" Could not calculate Silhouette for {model_name}: {e}")

        try:
            calinski_harabasz = calinski_harabasz_score(X_scaled, labels)
        except Exception as e:
             calinski_harabasz = None
             print(f" Could not calculate Calinski-Harabasz for {model_name}: {e}")

        try:
            davies_bouldin = davies_bouldin_score(X_scaled, labels)
        except Exception as e:
            davies_bouldin = None
            print(f" Could not calculate Davies-Bouldin for {model_name}: {e}")

        performance_metrics.append({
            'Model': model_name,
            'N_Clusters': n_clusters,
            'Silhouette Score': silhouette,
            'Calinski-Harabasz Score': calinski_harabasz,
            'Davies-Bouldin Index': davies_bouldin
        })
    else:
         print(f" Skipping {model_name} due to insufficient clusters or all points in one cluster/noise.")


performance_df = pd.DataFrame(performance_metrics)

print("\nğŸ“‹ Clustering Model Performance Table:")
display(performance_df.sort_values('Silhouette Score', ascending=False))

# Visualize performance metrics
plt.figure(figsize=(18, 6))

# Plot Silhouette Score
plt.subplot(1, 3, 1)
sns.barplot(x='Model', y='Silhouette Score', data=performance_df.sort_values('Silhouette Score', ascending=False))
plt.title('Clustering Model Performance: Silhouette Score')
plt.ylabel('Silhouette Score')
plt.xticks(rotation=45, ha='right')

# Plot Calinski-Harabasz Score
plt.subplot(1, 3, 2)
sns.barplot(x='Model', y='Calinski-Harabasz Score', data=performance_df.sort_values('Calinski-Harabasz Score', ascending=False))
plt.title('Clustering Model Performance: Calinski-Harabasz Score')
plt.ylabel('Calinski-Harabasz Score')
plt.xticks(rotation=45, ha='right')

# Plot Davies-Bouldin Index
plt.subplot(1, 3, 3)
sns.barplot(x='Model', y='Davies-Bouldin Index', data=performance_df.sort_values('Davies-Bouldin Index', ascending=True)) # Lower is better
plt.title('Clustering Model Performance: Davies-Bouldin Index')
plt.ylabel('Davies-Bouldin Index')
plt.xticks(rotation=45, ha='right')

plt.tight_layout()
plt.show()

print(f"\n Clustering Model Performance Comparison complete!")

# MODEL COMPARISON SUMMARY
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from IPython.display import display

print("Generating Model Comparison Summary...")
print("=" * 60)

# --- 1. Clustering Model Comparison ---
print("\nğŸ“Š Clustering Model Performance Comparison:")
clustering_performance_metrics = []

# Iterate through all clustering results stored
for name, result in clustering_results.items():
    model_name = result.get('method', name)
    n_clusters = result.get('n_clusters', 'N/A')
    labels = result.get('labels')

    # Only compare clustering models with more than one cluster and not all points in noise
    if labels is not None and len(set(labels)) > 1 and len(set(labels)) < len(X_scaled):
        try:
            silhouette = silhouette_score(X_scaled, labels)
        except Exception:
            silhouette = np.nan  # Use NaN if calculation fails
        try:
            calinski_harabasz = calinski_harabasz_score(X_scaled, labels)
        except Exception:
            calinski_harabasz = np.nan
        try:
            davies_bouldin = davies_bouldin_score(X_scaled, labels)
        except Exception:
            davies_bouldin = np.nan

        clustering_performance_metrics.append({
            'Model': model_name,
            'N_Clusters': n_clusters,
            'Silhouette Score': silhouette,
            'Calinski-Harabasz Score': calinski_harabasz,
            'Davies-Bouldin Index': davies_bouldin
        })
    else:
        print(f"   Skipping {model_name} (Clustering) due to insufficient clusters or all points in one cluster/noise.")

clustering_performance_df = pd.DataFrame(clustering_performance_metrics)

# Display table, handling potential NaNs for sorting
print("\nğŸ“‹ Clustering Model Performance Table:")
display(clustering_performance_df.sort_values('Silhouette Score', ascending=False).fillna('N/A'))  # Fill NaN for display

# Visualize clustering performance metrics
if not clustering_performance_df.empty:
    plt.figure(figsize=(18, 6))
    # Plot Silhouette Score
    plt.subplot(1, 3, 1)
    sns.barplot(x='Model', y='Silhouette Score', data=clustering_performance_df.sort_values('Silhouette Score', ascending=False).dropna(subset=['Silhouette Score']))
    plt.title('Clustering Model Performance: Silhouette Score')
    plt.ylabel('Silhouette Score')
    plt.xticks(rotation=45, ha='right')

    # Plot Calinski-Harabasz Score
    plt.subplot(1, 3, 2)
    sns.barplot(x='Model', y='Calinski-Harabasz Score', data=clustering_performance_df.sort_values('Calinski-Harabasz Score', ascending=False).dropna(subset=['Calinski-Harabasz Score']))
    plt.title('Clustering Model Performance: Calinski-Harabasz Score')
    plt.ylabel('Calinski-Harabasz Score')
    plt.xticks(rotation=45, ha='right')

    # Plot Davies-Bouldin Index
    plt.subplot(1, 3, 3)
    sns.barplot(x='Model', y='Davies-Bouldin Index', data=clustering_performance_df.sort_values('Davies-Bouldin Index', ascending=True).dropna(subset=['Davies-Bouldin Index']))  # Lower is better
    plt.title('Clustering Model Performance: Davies-Bouldin Index')
    plt.ylabel('Davies-Bouldin Index')
    plt.xticks(rotation=45, ha='right')

    plt.tight_layout()
    plt.show()
else:
    print("   No valid clustering results to compare.")

# --- 2. Anomaly Detection Model Comparison ---
print("\nğŸ•µï¸ Anomaly Detection Results Summary:")
if 'df_behavior' in locals() and 'is_anomaly_isolation' in df_behavior.columns and 'is_anomaly_lof' in df_behavior.columns:
    print(f" Isolation Forest identified {df_behavior['is_anomaly_isolation'].sum()} anomalies ({df_behavior['is_anomaly_isolation'].mean()*100:.1f}%)")
    print(f" Local Outlier Factor identified {df_behavior['is_anomaly_lof'].sum()} anomalies ({df_behavior['is_anomaly_lof'].mean()*100:.1f}%)")

    # How many anomalies are identified by both?
    common_anomalies = df_behavior['is_anomaly_isolation'] & df_behavior['is_anomaly_lof']
    print(f" Anomalies identified by both IF and LOF: {common_anomalies.sum()} ({common_anomalies.mean()*100:.1f}%)")

    # Visualize anomaly overlap (requires t-SNE which should be available)
    if 'X_tsne' in locals():
        plt.figure(figsize=(12, 5))
        plt.subplot(1, 2, 1)
        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=df_behavior['is_anomaly_isolation'], cmap='coolwarm', alpha=0.6, s=50)
        plt.title('Isolation Forest Anomalies (1=Normal, 0=Anomaly)')
        plt.xlabel('t-SNE Component 1')
        plt.ylabel('t-SNE Component 2')

        # Create custom legend for boolean colors
        handles = [plt.Line2D([], [], marker='o', color=scatter.cmap(1.0), linestyle='None', label='Normal'),
                   plt.Line2D([], [], marker='o', color=scatter.cmap(0.0), linestyle='None', label='Anomaly')]
        plt.legend(handles=handles)

        plt.subplot(1, 2, 2)
        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=df_behavior['is_anomaly_lof'], cmap='coolwarm', alpha=0.6, s=50)
        plt.title('LOF Anomalies (1=Normal, 0=Anomaly)')
        plt.xlabel('t-SNE Component 1')
        plt.ylabel('t-SNE Component 2')

        # Create custom legend for boolean colors
        handles = [plt.Line2D([], [], marker='o', color=scatter.cmap(1.0), linestyle='None', label='Normal'),
                   plt.Line2D([], [], marker='o', color=scatter.cmap(0.0), linestyle='None', label='Anomaly')]
        plt.legend(handles=handles)

        plt.tight_layout()
        plt.show()
    else:
        print("   Cannot visualize anomaly overlap: t-SNE components not available.")
else:
    print("   Anomaly detection results (Isolation Forest or LOF) not found in df_behavior.")

# --- 3. Random Forest Feature Importance Summary ---
print("\nğŸŒ² Random Forest Feature Importance Summary:")
if 'ml_results' in locals() and 'random_forest' in ml_results and 'feature_importance' in ml_results['random_forest']:
    feature_importance_df = ml_results['random_forest']['feature_importance']
    print("\nğŸ“‹ Top 10 Most Important Features for Cluster Discrimination:")
    display(feature_importance_df.head(10))

    # Optional: Visualize feature importance
    plt.figure(figsize=(10, 6))
    sns.barplot(x='importance', y='feature', data=feature_importance_df.head(10), palette='viridis')
    plt.title('Top 10 Random Forest Feature Importance')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.tight_layout()
    plt.show()
else:
    print("   Random Forest feature importance results not found in ml_results.")

print("\n Overall Model Comparison Summary Complete!")

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score, calinski_harabasz_score
import matplotlib.pyplot as plt
import seaborn as sns

def compare_clustering_models(X, max_k=10, dbscan_eps=[0.5, 1.0, 1.5], dbscan_min_samples=[5, 10]):
    results = {
        'Model': [],
        'Parameters': [],
        'Silhouette_Score': [],
        'Calinski_Harabasz_Score': [],
        'N_Clusters': []
    }

    # Standardize features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # KMeans comparison
    for k in range(2, max_k + 1):
        kmeans = KMeans(n_clusters=k, random_state=42)
        labels = kmeans.fit_predict(X_scaled)

        if len(np.unique(labels)) > 1:
            sil_score = silhouette_score(X_scaled, labels)
            ch_score = calinski_harabasz_score(X_scaled, labels)

            results['Model'].append('KMeans')
            results['Parameters'].append(f'n_clusters={k}')
            results['Silhouette_Score'].append(sil_score)
            results['Calinski_Harabasz_Score'].append(ch_score)
            results['N_Clusters'].append(len(np.unique(labels)))

    # DBSCAN comparison
    for eps in dbscan_eps:
        for min_samples in dbscan_min_samples:
            dbscan = DBSCAN(eps=eps, min_samples=min_samples)
            labels = dbscan.fit_predict(X_scaled)

            if len(np.unique(labels)) > 1:  # Exclude cases with no clusters or all noise
                sil_score = silhouette_score(X_scaled, labels)
                ch_score = calinski_harabasz_score(X_scaled, labels)

                results['Model'].append('DBSCAN')
                results['Parameters'].append(f'eps={eps}, min_samples={min_samples}')
                results['Silhouette_Score'].append(sil_score)
                results['Calinski_Harabasz_Score'].append(ch_score)
                results['N_Clusters'].append(len(np.unique(labels)))

    # Agglomerative Clustering comparison
    for k in range(2, max_k + 1):
        agg = AgglomerativeClustering(n_clusters=k)
        labels = agg.fit_predict(X_scaled)

        sil_score = silhouette_score(X_scaled, labels)
        ch_score = calinski_harabasz_score(X_scaled, labels)

        results['Model'].append('Agglomerative')
        results['Parameters'].append(f'n_clusters={k}')
        results['Silhouette_Score'].append(sil_score)
        results['Calinski_Harabasz_Score'].append(ch_score)
        results['N_Clusters'].append(k)

    # Create results DataFrame
    results_df = pd.DataFrame(results)

    # Plotting
    plt.figure(figsize=(12, 6))

    # Silhouette Score Plot
    plt.subplot(1, 2, 1)
    sns.barplot(data=results_df, x='Model', y='Silhouette_Score', hue='Parameters')
    plt.title('Silhouette Score Comparison')
    plt.xticks(rotation=45)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

    # Calinski-Harabasz Score Plot
    plt.subplot(1, 2, 2)
    sns.barplot(data=results_df, x='Model', y='Calinski_Harabasz_Score', hue='Parameters')
    plt.title('Calinski-Harabasz Score Comparison')
    plt.xticks(rotation=45)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

    plt.tight_layout()
    plt.show()

    # Display results
    print("\nModel Comparison Results:")
    print(results_df.sort_values('Silhouette_Score', ascending=False))

    return results_df

# Assuming X is already defined from your feature engineering
# Call the comparison function
results_df = compare_clustering_models(X)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score, calinski_harabasz_score
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def create_clustering_pipeline(X, max_k=10, dbscan_eps=[0.5, 1.0, 1.5], dbscan_min_samples=[5, 10]):
    results = {
        'Model': [],
        'Parameters': [],
        'Silhouette_Score': [],
        'Calinski_Harabasz_Score': [],
        'N_Clusters': []
    }

    # Define pipelines for each clustering algorithm
    kmeans_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('kmeans', KMeans(random_state=42))
    ])

    dbscan_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('dbscan', DBSCAN())
    ])

    agg_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('agglomerative', AgglomerativeClustering())
    ])

    # Evaluate KMeans
    for k in range(2, max_k + 1):
        kmeans_pipeline.set_params(kmeans__n_clusters=k)
        labels = kmeans_pipeline.fit_predict(X)

        if len(np.unique(labels)) > 1:
            sil_score = silhouette_score(X, labels)
            ch_score = calinski_harabasz_score(X, labels)

            results['Model'].append('KMeans')
            results['Parameters'].append(f'n_clusters={k}')
            results['Silhouette_Score'].append(sil_score)
            results['Calinski_Harabasz_Score'].append(ch_score)
            results['N_Clusters'].append(len(np.unique(labels)))

    # Evaluate DBSCAN
    for eps in dbscan_eps:
        for min_samples in dbscan_min_samples:
            dbscan_pipeline.set_params(dbscan__eps=eps, dbscan__min_samples=min_samples)
            labels = dbscan_pipeline.fit_predict(X)

            if len(np.unique(labels)) > 1:
                sil_score = silhouette_score(X, labels)
                ch_score = calinski_harabasz_score(X, labels)

                results['Model'].append('DBSCAN')
                results['Parameters'].append(f'eps={eps}, min_samples={min_samples}')
                results['Silhouette_Score'].append(sil_score)
                results['Calinski_Harabasz_Score'].append(ch_score)
                results['N_Clusters'].append(len(np.unique(labels)))

    # Evaluate Agglomerative Clustering
    for k in range(2, max_k + 1):
        agg_pipeline.set_params(agglomerative__n_clusters=k)
        labels = agg_pipeline.fit_predict(X)

        sil_score = silhouette_score(X, labels)
        ch_score = calinski_harabasz_score(X, labels)

        results['Model'].append('Agglomerative')
        results['Parameters'].append(f'n_clusters={k}')
        results['Silhouette_Score'].append(sil_score)
        results['Calinski_Harabasz_Score'].append(ch_score)
        results['N_Clusters'].append(k)

    # Create results DataFrame
    results_df = pd.DataFrame(results)

    # Plotting
    plt.figure(figsize=(12, 6))

    # Silhouette Score Plot
    plt.subplot(1, 2, 1)
    sns.barplot(data=results_df, x='Model', y='Silhouette_Score', hue='Parameters')
    plt.title('Silhouette Score Comparison')
    plt.xticks(rotation=45)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

    # Calinski-Harabasz Score Plot
    plt.subplot(1, 2, 2)
    sns.barplot(data=results_df, x='Model', y='Calinski_Harabasz_Score', hue='Parameters')
    plt.title('Calinski-Harabasz Score Comparison')
    plt.xticks(rotation=45)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

    plt.tight_layout()
    plt.show()

    # Display results
    print("\nClustering Pipeline Results:")
    print(results_df.sort_values('Silhouette_Score', ascending=False))

    return results_df, kmeans_pipeline, dbscan_pipeline, agg_pipeline

# Assuming X is already defined from your feature engineering
# Call the pipeline function
results_df, kmeans_pipe, dbscan_pipe, agg_pipe = create_clustering_pipeline(X)

##Clustering model performance
from sklearn.model_selection import ParameterGrid
from sklearn.metrics import silhouette_score
import numpy as np

def grid_search_clustering(X, kmeans_params, dbscan_params):
    best_models = []
    X_scaled = StandardScaler().fit_transform(X)

    # KMeans Grid Search
    for params in ParameterGrid(kmeans_params):
        kmeans = KMeans(**params, random_state=42)
        labels = kmeans.fit_predict(X_scaled)
        if len(np.unique(labels)) > 1:
            score = silhouette_score(X_scaled, labels)
            best_models.append(('KMeans', params, score, len(np.unique(labels))))

    # DBSCAN Grid Search
    for params in ParameterGrid(dbscan_params):
        dbscan = DBSCAN(**params)
        labels = dbscan.fit_predict(X_scaled)
        if len(np.unique(labels)) > 1:
            score = silhouette_score(X_scaled, labels)
            best_models.append(('DBSCAN', params, score, len(np.unique(labels))))

    # Convert to DataFrame
    results = pd.DataFrame(best_models, columns=['Model', 'Parameters', 'Silhouette_Score', 'N_Clusters'])
    print("\nBest Models from Grid Search:")
    print(results.sort_values('Silhouette_Score', ascending=False))

    return results

# Define parameter grids
kmeans_params = {'n_clusters': range(2, 11), 'init': ['k-means++', 'random']}
dbscan_params = {'eps': [0.3, 0.5, 1.0, 1.5], 'min_samples': [5, 10, 15]}

# Run grid search
grid_search_results = grid_search_clustering(X, kmeans_params, dbscan_params)

#Visualize Clusters with Dimensionality Reduction

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_clusters(X, pipeline, model_name, params):
    # Fit pipeline and get labels
    pipeline.set_params(**params)
    labels = pipeline.fit_predict(X)

    # Reduce dimensions with PCA
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(StandardScaler().fit_transform(X))

    # Plot
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=labels, palette='deep')
    plt.title(f'{model_name} Clusters (PCA)')
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.show()

# Example: Visualize KMeans with best parameters (update with your best params)
best_kmeans_params = {'kmeans__n_clusters': 4}  # Example, replace with best from grid search
visualize_clusters(X, kmeans_pipe, 'KMeans', best_kmeans_params)

import pandas as pd
import matplotlib.pyplot as plt

def align_data(X, df_behavior):
    """Align X and df_behavior to have consistent sample counts."""
    min_samples = min(len(X), len(df_behavior))

    # Truncate to smallest sample size
    X = X[:min_samples]
    df_behavior = df_behavior.iloc[:min_samples].copy()

    # Validate shapes
    if len(X) != len(df_behavior):
        raise ValueError(f"Sample count mismatch: X has {len(X)} samples, df_behavior has {len(df_behavior)} samples")

    return X, df_behavior

def analyze_clusters(X, df_behavior, pipeline, params):
    # Align data
    try:
        X, df_behavior = align_data(X, df_behavior)
    except ValueError as e:
        print(f"Error: {e}")
        return None

    # Fit pipeline
    pipeline.set_params(**params)
    labels = pipeline.fit_predict(X)

    # Validate labels length
    if len(labels) != len(df_behavior):
        raise ValueError(f"Label length ({len(labels)}) does not match df_behavior ({len(df_behavior)})")

    # Add cluster labels to DataFrame
    df_result = df_behavior.copy()
    df_result['Cluster'] = labels

    # Group by cluster and compute mean for key features
    cluster_stats = df_result.groupby('Cluster')[
        ['activity_score', 'popularity_score', 'collaboration_score', 'code_quality_score']
    ].mean().reset_index()

    print("\nCluster Characteristics:")
    print(cluster_stats)

    # Visualize cluster profiles
    plt.figure(figsize=(10, 6))
    for column in cluster_stats.columns[1:]:
        plt.plot(cluster_stats['Cluster'], cluster_stats[column], marker='o', label=column)
    plt.title('Cluster Profiles')
    plt.xlabel('Cluster')
    plt.ylabel('Mean Score')
    plt.legend()
    plt.show()

    return df_result

# Example usage:
df_result = analyze_clusters(X, df_behavior, kmeans_pipe, {'kmeans__n_clusters': 4})

#Add More Clustering Algorithms

from sklearn.mixture import GaussianMixture
from sklearn.pipeline import Pipeline

def add_gmm_to_pipeline(X, max_k=10):
    gmm_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('gmm', GaussianMixture(random_state=42))
    ])

    results = []
    for k in range(2, max_k + 1):
        gmm_pipeline.set_params(gmm__n_components=k)
        labels = gmm_pipeline.fit_predict(X)
        if len(np.unique(labels)) > 1:
            sil_score = silhouette_score(X, labels)
            ch_score = calinski_harabasz_score(X, labels)
            results.append({
                'Model': 'GMM',
                'Parameters': f'n_components={k}',
                'Silhouette_Score': sil_score,
                'Calinski_Harabasz_Score': ch_score,
                'N_Clusters': len(np.unique(labels))
            })

    results_df = pd.DataFrame(results)
    print("\nGMM Results:")
    print(results_df)
    return gmm_pipeline, results_df

# Add GMM to analysis
gmm_pipe, gmm_results = add_gmm_to_pipeline(X)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier

def align_data(X, df_behavior, feature_names, labels=None):
    """Align X, df_behavior, and labels to have consistent sample counts and features."""
    # Ensure all features exist in df_behavior
    missing_features = [f for f in feature_names if f not in df_behavior.columns]
    if missing_features:
        raise ValueError(f"Features not found in df_behavior: {missing_features}")

    # Determine minimum sample size
    sample_sizes = [len(X), len(df_behavior)]
    if labels is not None:
        sample_sizes.append(len(labels))
    min_samples = min(sample_sizes)

    # Truncate to smallest sample size
    X = X[:min_samples]
    df_behavior = df_behavior.iloc[:min_samples].copy()
    if labels is not None:
        labels = labels[:min_samples]

    # Reconstruct X from df_behavior to ensure feature alignment
    X_new = df_behavior[feature_names].values

    # Validate shapes
    if X_new.shape[1] != len(feature_names):
        raise ValueError(f"X has {X_new.shape[1]} features, expected {len(feature_names)}")
    if X_new.shape[0] != len(df_behavior):
        raise ValueError(f"X has {X_new.shape[0]} samples, expected {len(df_behavior)}")
    if labels is not None and len(labels) != len(df_behavior):
        raise ValueError(f"Label length ({len(labels)}) does not match samples ({len(df_behavior)})")

    return X_new, df_behavior, labels

def feature_importance_analysis(X, labels, feature_names, df_behavior):
    # Align data
    try:
        X, df_behavior, labels = align_data(X, df_behavior, feature_names, labels)
    except ValueError as e:
        print(f"Error: {e}")
        return None

    # Train Random Forest
    rf = RandomForestClassifier(
        n_estimators=100,
        random_state=42,
        max_depth=10,
        min_samples_split=5,
        n_jobs=-1
    )
    rf.fit(X, labels)

    # Create feature importances DataFrame
    importances = pd.DataFrame({
        'Feature': feature_names,
        'Importance': rf.feature_importances_
    }).sort_values('Importance', ascending=False)

    # Plot feature importances
    plt.figure(figsize=(10, 6))
    sns.barplot(data=importances, x='Importance', y='Feature')
    plt.title('Feature Importance for Clustering')
    plt.tight_layout()
    plt.show()

    print("\nFeature Importances:")
    print(importances)

    return importances

# Define the 19 features
behavior_features = [
    'commits_per_day',
    'prs_per_day',
    'activity_score',
    'followers_to_following_ratio',
    'collaboration_score',
    'popularity_score',
    'avg_loc_per_pr',
    'avg_files_per_pr',
    'code_quality_score',
    'est_loc_per_commit',
    'commit_to_pr_ratio',
    'account_age_days',
    'total_repos',
    'avg_stars_repo',
    'followers_log',
    'stars_total_log',
    'total_commits_log',
    'pr_additions_log',
    'total_prs_log'
]

# Example usage:
best_kmeans = kmeans_pipe.set_params(kmeans__n_clusters=4)
labels = best_kmeans.fit_predict(X)
importances = feature_importance_analysis(X, labels, behavior_features, df_behavior)

#Validate Clusters with External Metrics

from sklearn.metrics import adjusted_rand_score

def validate_clusters(X, pipeline, params, true_labels):
    pipeline.set_params(**params)
    predicted_labels = pipeline.fit_predict(X)

    ari_score = adjusted_rand_score(true_labels, predicted_labels)
    print(f"\nAdjusted Rand Index for {params}: {ari_score}")
    return ari_score

# Example: Assuming you have true_labels (replace with actual labels if available)
# true_labels = df_behavior['some_label_column']
# validate_clusters(X, kmeans_pipe, {'kmeans__n_clusters': 4}, true_labels)

import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

def align_data(X, df_behavior):
    """Align X and df_behavior to have consistent sample counts."""
    min_samples = min(len(X), len(df_behavior))

    # Truncate to smallest sample size
    X = X[:min_samples]
    df_behavior = df_behavior.iloc[:min_samples].copy()

    # Validate shapes
    if len(X) != len(df_behavior):
        raise ValueError(f"Sample count mismatch: X has {len(X)} samples, df_behavior has {len(df_behavior)} samples")

    return X, df_behavior

def detect_anomalies(X, df_behavior):
    # Align data
    try:
        X, df_behavior = align_data(X, df_behavior)
    except ValueError as e:
        print(f"Error: {e}")
        return None

    # Standardize features
    X_scaled = StandardScaler().fit_transform(X)

    # Perform anomaly detection
    iso_forest = IsolationForest(
        contamination=0.1,
        random_state=42,
        n_jobs=-1  # Use all available cores
    )
    anomalies = iso_forest.fit_predict(X_scaled)

    # Add anomaly labels to DataFrame
    df_result = df_behavior.copy()
    df_result['Is_Anomaly'] = anomalies == -1

    # Print results
    print("\nAnomaly Detection Results:")
    print(df_result[df_result['Is_Anomaly']][['login', 'activity_score', 'popularity_score']])

    return df_result

# Example usage:
df_behavior = detect_anomalies(X, df_behavior)



import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.preprocessing import StandardScaler

def align_data(X, X_tsne, df_behavior, labels=None):
    """Align all inputs to have consistent sample counts."""
    sample_sizes = [len(X), len(X_tsne), len(df_behavior)]
    if labels is not None:
        sample_sizes.append(len(labels))
    min_samples = min(sample_sizes)

    # Truncate to smallest sample size
    X = X[:min_samples]
    X_tsne = X_tsne[:min_samples]
    df_behavior = df_behavior.iloc[:min_samples].copy()
    if labels is not None:
        labels = labels[:min_samples]

    # Validate shapes
    if not (len(X) == len(X_tsne) == len(df_behavior) and (labels is None or len(labels) == len(X))):
        raise ValueError(f"Sample count mismatch: X={len(X)}, X_tsne={len(X_tsne)}, "
                        f"df_behavior={len(df_behavior)}, labels={len(labels) if labels is not None else 'None'}")

    return X, X_tsne, df_behavior, labels

def perform_anomaly_detection(X, df_behavior):
    """Perform anomaly detection using Isolation Forest and LOF."""
    # Standardize features
    X_scaled = StandardScaler().fit_transform(X)

    # Isolation Forest
    iso_forest = IsolationForest(
        contamination=0.1,
        random_state=42,
        n_jobs=-1
    )
    iso_labels = iso_forest.fit_predict(X_scaled)
    iso_scores = iso_forest.decision_function(X_scaled)

    # Local Outlier Factor
    lof = LocalOutlierFactor(
        n_neighbors=20,
        contamination=0.1,
        n_jobs=-1
    )
    lof_labels = lof.fit_predict(X_scaled)
    lof_scores = lof.negative_outlier_factor_

    # Add results to DataFrame
    df_result = df_behavior.copy()
    df_result['is_anomaly_isolation'] = iso_labels == -1
    df_result['anomaly_score'] = iso_scores
    df_result['is_anomaly_lof'] = lof_labels == -1
    df_result['lof_score'] = lof_scores

    return df_result

def compare_clustering_anomaly_results(X, X_tsne, df_behavior, clustering_results):
    # Align data
    try:
        X, X_tsne, df_behavior, _ = align_data(X, X_tsne, df_behavior)
    except ValueError as e:
        print(f"Error: {e}")
        return None

    # Perform anomaly detection
    df_behavior = perform_anomaly_detection(X, df_behavior)

    # Compare clustering results
    print("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print("CLUSTERING MODEL COMPARISON")
    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    comparison_metrics = []
    for name, result in clustering_results.items():
        n_clusters_val = result.get('n_clusters', result.get('n_components', 'N/A'))
        comparison_metrics.append({
            'Method': result['method'],
            'n_Clusters': n_clusters_val,
            'Silhouette Score': result.get('silhouette', 'N/A'),
            'AIC': result.get('aic', 'N/A'),
            'BIC': result.get('bic', 'N/A')
        })

    comparison_df = pd.DataFrame(comparison_metrics)
    comparison_df['Silhouette Score Numeric'] = pd.to_numeric(comparison_df['Silhouette Score'], errors='coerce')
    print("\nClustering Comparison:")
    print(comparison_df.sort_values(by='Silhouette Score Numeric', ascending=False).drop(columns=['Silhouette Score Numeric']))

    # Visualize cluster assignments
    plt.figure(figsize=(20, 8))
    i = 1
    for name, result in clustering_results.items():
        if i <= 4:
            plt.subplot(1, 4, i)
            num_clusters_in_title = result.get('n_clusters', result.get('n_components', 'N/A'))
            scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=result['labels'],
                                 cmap='viridis', alpha=0.6, s=50)
            plt.colorbar(scatter)
            plt.title(f'{result["method"]} Clusters (n={num_clusters_in_title})')
            plt.xlabel('t-SNE Component 1')
            plt.ylabel('t-SNE Component 2')
            i += 1
    plt.tight_layout()
    plt.show()

    # Compare anomaly detection results
    print("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print("ANOMALY DETECTION COMPARISON")
    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    print(f"Isolation Forest identified {df_behavior['is_anomaly_isolation'].sum()} anomalies.")
    print(f"Local Outlier Factor identified {df_behavior['is_anomaly_lof'].sum()} anomalies.")

    # Visualize anomaly overlap
    plt.figure(figsize=(8, 6))
    overlap = df_behavior['is_anomaly_isolation'] & df_behavior['is_anomaly_lof']
    iso_only = df_behavior['is_anomaly_isolation'] & ~df_behavior['is_anomaly_lof']
    lof_only = ~df_behavior['is_anomaly_isolation'] & df_behavior['is_anomaly_lof']
    normal = ~df_behavior['is_anomaly_isolation'] & ~df_behavior['is_anomaly_lof']

    plt.scatter(X_tsne[normal, 0], X_tsne[normal, 1], c='blue', alpha=0.3, s=20, label='Normal')
    plt.scatter(X_tsne[iso_only, 0], X_tsne[iso_only, 1], c='red', alpha=0.6, s=50, label='IF Only')
    plt.scatter(X_tsne[lof_only, 0], X_tsne[lof_only, 1], c='orange', alpha=0.6, s=50, label='LOF Only')
    plt.scatter(X_tsne[overlap, 0], X_tsne[overlap, 1], c='purple', alpha=0.8, s=80, marker='x', label='Both')

    plt.title('Anomaly Overlap (Isolation Forest vs LOF)')
    plt.xlabel('t-SNE Component 1')
    plt.ylabel('t-SNE Component 2')
    plt.legend()
    plt.show()

    print("\nAnomalies identified by both Isolation Forest and LOF:")
    print(df_behavior[overlap][['login', 'activity_score', 'popularity_score', 'anomaly_score', 'lof_score']].head())

    return df_behavior

# Example usage:
df_behavior = compare_clustering_anomaly_results(X, X_tsne, df_behavior, clustering_results)

import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import RobustScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

def create_processing_pipeline(df_behavior, feature_names):
    print("Constructing data processing pipeline...")

    # Ensure all features exist in df_behavior
    missing_features = [f for f in feature_names if f not in df_behavior.columns]
    if missing_features:
        raise ValueError(f"Features not found in df_behavior: {missing_features}")

    # Construct X from df_behavior
    X = df_behavior[feature_names].values

    # Validate shapes
    if X.shape[0] != len(df_behavior):
        raise ValueError(f"X has {X.shape[0]} samples, expected {len(df_behavior)}")
    if X.shape[1] != len(feature_names):
        raise ValueError(f"X has {X.shape[1]} features, expected {len(feature_names)}")

    # Define pipeline steps
    pipeline_steps = [
        ('scaler', RobustScaler()),
        ('pca', PCA(n_components=0.95))
    ]

    # Create and fit pipeline
    data_processing_pipeline = Pipeline(pipeline_steps)
    X_processed = data_processing_pipeline.fit_transform(X)

    print(f"Data processing pipeline created with {len(pipeline_steps)} steps.")
    print(f"Processed data shape after pipeline: {X_processed.shape}")

    # Apply t-SNE for visualization
    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X_processed)//4))
    X_tsne = tsne.fit_transform(X_processed)
    print(f"t-SNE: 2D visualization ready")

    # Access fitted PCA
    fitted_pca = data_processing_pipeline.named_steps['pca']
    print(f"\nPCA Explained Variance Ratio:")
    for i, var in enumerate(fitted_pca.explained_variance_ratio_[:5], 1):
        print(f"  Component {i}: {var:.3f} ({var*100:.1f}%)")

    # Visualize feature importance in first 3 PCA components
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'PC1': np.abs(fitted_pca.components_[0]),
        'PC2': np.abs(fitted_pca.components_[1]),
        'PC3': np.abs(fitted_pca.components_[2]) if fitted_pca.n_components_ > 2 else np.zeros(len(feature_names))
    })

    print(f"\nTop features in Principal Components:")
    print(feature_importance.nlargest(10, 'PC1')[['feature', 'PC1', 'PC2', 'PC3']])

    return data_processing_pipeline, X_processed, X_tsne

# Define the 19 features
behavior_features = [
    'commits_per_day',
    'prs_per_day',
    'activity_score',
    'followers_to_following_ratio',
    'collaboration_score',
    'popularity_score',
    'avg_loc_per_pr',
    'avg_files_per_pr',
    'code_quality_score',
    'est_loc_per_commit',
    'commit_to_pr_ratio',
    'account_age_days',
    'total_repos',
    'avg_stars_repo',
    'followers_log',
    'stars_total_log',
    'total_commits_log',
    'pr_additions_log',
    'total_prs_log'
]

# Example usage:
data_processing_pipeline, X_processed, X_tsne = create_processing_pipeline(df_behavior, behavior_features)

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
import matplotlib.pyplot as plt

# ------------------- STEP 1: Ensure derived fields exist -------------------

# Compute missing fields if not already present
if 'activity_score' not in df_behavior.columns:
    df_behavior['activity_score'] = df_behavior['commits_per_day'] + df_behavior['prs_per_day']

if 'popularity_score' not in df_behavior.columns:
    df_behavior['popularity_score'] = df_behavior['followers_log'] + df_behavior['stars_total_log']

if 'collaboration_score' not in df_behavior.columns:
    df_behavior['collaboration_score'] = (
        df_behavior['commit_to_pr_ratio'] * df_behavior['avg_files_per_pr']
    )

if 'code_quality_score' not in df_behavior.columns:
    df_behavior['code_quality_score'] = (
        df_behavior['avg_loc_per_pr'] / (df_behavior['avg_files_per_pr'] + 1)
    )

# ------------------- STEP 2: Feature selection and scaling -------------------

feature_columns = [
    'commits_per_day', 'prs_per_day', 'activity_score',
    'followers_to_following_ratio', 'collaboration_score',
    'popularity_score', 'avg_loc_per_pr', 'avg_files_per_pr',
    'code_quality_score', 'est_loc_per_commit', 'commit_to_pr_ratio',
    'account_age_days', 'total_repos', 'avg_stars_repo',
    'followers_log', 'stars_total_log', 'total_commits_log',
    'pr_additions_log', 'total_prs_log'
]

# Keep only features that exist in DataFrame
existing_features = [f for f in feature_columns if f in df_behavior.columns]

X = df_behavior[existing_features].values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# t-SNE projection
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
X_tsne = tsne.fit_transform(X_scaled)

# ------------------- STEP 3: Alignment function -------------------

def align_data(X_scaled, X_tsne, df_behavior):
    min_length = min(len(X_scaled), len(X_tsne), len(df_behavior))
    X_scaled = X_scaled[:min_length]
    X_tsne = X_tsne[:min_length]
    df_behavior = df_behavior.iloc[:min_length].copy()
    return X_scaled, X_tsne, df_behavior

# ------------------- STEP 4: Anomaly Detection -------------------

def run_anomaly_detection(X_scaled, X_tsne, df_behavior):
    X_scaled, X_tsne, df_behavior = align_data(X_scaled, X_tsne, df_behavior)

    # Isolation Forest
    isolation_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=100, n_jobs=-1)
    anomaly_labels = isolation_forest.fit_predict(X_scaled)
    anomaly_scores = isolation_forest.decision_function(X_scaled)
    normal_devs = (anomaly_labels == 1)
    anomalous_devs = (anomaly_labels == -1)

    # Local Outlier Factor
    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1, n_jobs=-1)
    lof_labels = lof.fit_predict(X_scaled)
    lof_scores = lof.negative_outlier_factor_

    # Add results
    df_result = df_behavior.copy()
    df_result['is_anomaly_isolation'] = anomalous_devs
    df_result['anomaly_score'] = anomaly_scores
    df_result['is_anomaly_lof'] = (lof_labels == -1)
    df_result['lof_score'] = lof_scores

    # Print summary
    print("Anomaly Detection Results:")
    print(f"  Total developers: {len(df_result)}")
    print(f"  Normal developers: {normal_devs.sum()} ({normal_devs.sum()/len(df_result)*100:.1f}%)")
    print(f"  Anomalous developers: {anomalous_devs.sum()} ({anomalous_devs.sum()/len(df_result)*100:.1f}%)")

    # ------------------- Visualization -------------------
    plt.figure(figsize=(18, 5))

    # Plot 1: t-SNE with anomalies
    plt.subplot(1, 3, 1)
    plt.scatter(X_tsne[normal_devs, 0], X_tsne[normal_devs, 1], c='blue', s=50, alpha=0.5, label='Normal')
    plt.scatter(X_tsne[anomalous_devs, 0], X_tsne[anomalous_devs, 1], c='red', s=80, marker='x', alpha=0.8, label='Anomaly')
    plt.title('Isolation Forest on t-SNE')
    plt.xlabel('Component 1')
    plt.ylabel('Component 2')
    plt.legend()

    # Plot 2: Score distribution
    plt.subplot(1, 3, 2)
    plt.hist(anomaly_scores[normal_devs], bins=30, alpha=0.7, color='blue', label='Normal', density=True)
    plt.hist(anomaly_scores[anomalous_devs], bins=30, alpha=0.7, color='red', label='Anomaly', density=True)
    plt.title('Anomaly Score Distribution')
    plt.xlabel('Anomaly Score')
    plt.ylabel('Density')
    plt.legend()

    # Plot 3: Top 5 Anomalies in score space
    plt.subplot(1, 3, 3)
    top_indices = np.where(anomalous_devs)[0]
    top5_idx = top_indices[np.argsort(anomaly_scores[anomalous_devs])[:5]]

    plt.scatter(df_result['activity_score'], df_result['popularity_score'], c='gray', alpha=0.3, label='All')
    plt.scatter(df_result.iloc[top5_idx]['activity_score'], df_result.iloc[top5_idx]['popularity_score'],
                c='red', s=100, label='Top Anomalies')

    for i, idx in enumerate(top5_idx):
        plt.annotate(f'#{i+1}', (df_result.iloc[idx]['activity_score'], df_result.iloc[idx]['popularity_score']),
                     textcoords='offset points', xytext=(5, 5), ha='left')

    plt.title('Top 5 Anomalous Developers')
    plt.xlabel('Activity Score')
    plt.ylabel('Popularity Score')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # ------------------- Print Top Anomalies -------------------
    print("\nTop 5 Most Anomalous Developers:")
    for i, idx in enumerate(top5_idx):
        row = df_result.iloc[idx]
        print(f"{i+1}. {row.get('login', f'Index {idx}')}")
        print(f"   Activity: {row['activity_score']:.3f}, Popularity: {row['popularity_score']:.3f}")
        print(f"   Commits/day: {row['commits_per_day']:.3f}, Followers: {row.get('followers_log', 0):.0f}")
        print(f"   Anomaly Score: {row['anomaly_score']:.3f}")

    return df_result

# ------------------- Run everything -------------------
df_result = run_anomaly_detection(X_scaled, X_tsne, df_behavior)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

# 1. Feature Selection
feature_columns = [
    'commits_per_day', 'prs_per_day', 'activity_score',
    'followers_to_following_ratio', 'collaboration_score',
    'popularity_score', 'avg_loc_per_pr', 'avg_files_per_pr',
    'code_quality_score', 'est_loc_per_commit', 'commit_to_pr_ratio',
    'account_age_days', 'total_repos', 'avg_stars_repo',
    'followers_log', 'stars_total_log', 'total_commits_log',
    'pr_additions_log', 'total_prs_log'
]

# 2. Prepare Data
X = df_behavior[feature_columns].dropna()  # Drop rows with missing values
y = (df_behavior.loc[X.index, 'activity_score'] > df_behavior['activity_score'].median()).astype(int)  # Binary label

# 3. Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Scale Data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. Dimensionality Reduction for Visualization
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
X_tsne = tsne.fit_transform(np.vstack([X_train_scaled, X_test_scaled]))
X_tsne_train = X_tsne[:len(X_train)]
X_tsne_test = X_tsne[len(X_train):]

# 6. KNN Model
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)
y_pred_knn = knn.predict(X_test_scaled)

# 7. SVM Model
svm = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)
svm.fit(X_train_scaled, y_train)
y_pred_svm = svm.predict(X_test_scaled)

# 8. Evaluate
print("=== KNN Classification ===")
print(confusion_matrix(y_test, y_pred_knn))
print(classification_report(y_test, y_pred_knn))

print("\n=== SVM Classification ===")
print(confusion_matrix(y_test, y_pred_svm))
print(classification_report(y_test, y_pred_svm))

# 9. Visualization
plt.figure(figsize=(14, 6))

# Plot KNN
plt.subplot(1, 2, 1)
plt.title("KNN Prediction (t-SNE Projection)")
plt.scatter(X_tsne_test[:, 0], X_tsne_test[:, 1], c=y_pred_knn, cmap='coolwarm', edgecolor='k', s=60, label='Predicted')
plt.scatter(X_tsne_train[:, 0], X_tsne_train[:, 1], c=y_train, cmap='coolwarm', alpha=0.2, s=30, label='Train')
plt.xlabel("t-SNE 1"); plt.ylabel("t-SNE 2"); plt.legend()

# Plot SVM
plt.subplot(1, 2, 2)
plt.title("SVM Prediction (t-SNE Projection)")
plt.scatter(X_tsne_test[:, 0], X_tsne_test[:, 1], c=y_pred_svm, cmap='coolwarm', edgecolor='k', s=60, label='Predicted')
plt.scatter(X_tsne_train[:, 0], X_tsne_train[:, 1], c=y_train, cmap='coolwarm', alpha=0.2, s=30, label='Train')
plt.xlabel("t-SNE 1"); plt.ylabel("t-SNE 2"); plt.legend()

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from sklearn.datasets import load_iris  # replace with your own data
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Evaluation Metrics
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, silhouette_score
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, davies_bouldin_score

# Models
from sklearn.ensemble import RandomForestClassifier
from sklearn.cluster import KMeans, DBSCAN
from sklearn.mixture import GaussianMixture

# Load example data (replace this with your dataset)
data = load_iris()
X, y = data.data, data.target

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train/test split for classifier
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# -------------------------------
# 1. Random Forest Classifier
# -------------------------------
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print("ğŸ” Random Forest Classifier Evaluation")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# -------------------------------
# 2. K-Means Clustering
# -------------------------------
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans_labels = kmeans.fit_predict(X_scaled)

print("\nğŸ” K-Means Clustering Evaluation")
print("Adjusted Rand Index:", adjusted_rand_score(y, kmeans_labels))
print("Normalized Mutual Info:", normalized_mutual_info_score(y, kmeans_labels))
print("Silhouette Score:", silhouette_score(X_scaled, kmeans_labels))
print("Davies-Bouldin Score:", davies_bouldin_score(X_scaled, kmeans_labels))

# -------------------------------
# 3. DBSCAN Clustering
# -------------------------------
dbscan = DBSCAN(eps=0.7, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_scaled)

# Filter out noise points (-1)
core_mask = dbscan_labels != -1

print("\nğŸ” DBSCAN Clustering Evaluation")
print("Adjusted Rand Index:", adjusted_rand_score(y[core_mask], dbscan_labels[core_mask]))
print("Normalized Mutual Info:", normalized_mutual_info_score(y[core_mask], dbscan_labels[core_mask]))
print("Silhouette Score (core only):", silhouette_score(X_scaled[core_mask], dbscan_labels[core_mask]))
print("Davies-Bouldin Score (core only):", davies_bouldin_score(X_scaled[core_mask], dbscan_labels[core_mask]))

# -------------------------------
# 4. Gaussian Mixture Model (GMM)
# -------------------------------
gmm = GaussianMixture(n_components=3, random_state=42)
gmm_labels = gmm.fit_predict(X_scaled)

print("\nğŸ” Gaussian Mixture Model Evaluation")
print("Adjusted Rand Index:", adjusted_rand_score(y, gmm_labels))
print("Normalized Mutual Info:", normalized_mutual_info_score(y, gmm_labels))
print("Silhouette Score:", silhouette_score(X_scaled, gmm_labels))
print("Davies-Bouldin Score:", davies_bouldin_score(X_scaled, gmm_labels))